![[강화학습_4-2_(250325)_page-0001.jpg]]

![[강화학습_4-2_(250325)_page-0002.jpg]]
- 현실적으로 agent는 trans prob을 모른다고 봐야한다.
- 그렇다면 벨만 방정식을 못 쓴다.
- 결국 MDP를 모른다는 결론이 난다.

- 또한 agent는 reward도 모른다.
- reward를 모르면 계산이 안된다.

- Policy와 Action은 Agent에게 귀속되는 것이다.
	- 그러므로 Policy와 Action을 Agent가 안다, 모른다 할 게 아니라 그냥 본인의 것이다.
	- 하지만 trans prob, reward를 모른다는 것은 현실적으로 가능하고, 보통 모른다.
	- 또한 State는 주로 안다고 가정한다.

- MDP를 모른다는 것의 해석
	- agent가 눈이 있어서 state는 일종의 껍데기, 눈에 보인다.
	- 하지만 state에 갔을 때 그 안에 뭐가 있을지는 모른다.(reward를 모른다.)
	- 어떤 state를 갈 확률도 모른다.(trans prob을 모른다.)

- 경험을 통해 reward를 배우는 것 아닌가?
- 코딩할 때 reward를 세팅하지 않는가?
- 그런데 reward를 모른다는 것을 해석하자면, agent는 어떤 action을 하는 것에는 어떤 reward를 계산에 반영하지 않는다.
- 단지 reward는 어떤 action에 대한 결과에 포함되는 것이다.
- 그래서 agent는 reward를 모른다고 볼 수 있는 것이다.

- 결론: agent는 Planning을 못 한다.
	- agent가 움직이기 전에 env를 쭉 파악하고 계획하고 이동하는 것이 불가능하다.
	- agent는 경험을 통해 reward와 trans prob을 파악할 수 있다.
	- 이것이 Model-free Reinforcement Learning이다.


### Monte-Carlo RL(MC)
![[강화학습_4-2_(250325)_page-0003.jpg]]
- 지금까지 모든 수식이 State value function($v(s)$)의 수식을 벗어난 적이 없다.
- 원래 수학적으로 벨만 방정식에 따라 업데이트를 반복하는 계획을 세웠음
- 몬테까를로는 그냥 주사위를 엄청 많이 굴려서 대수의 법칙을 따라 경험을 한다.
![[강화학습_4-2_(250325)_page-0004.jpg]]
- 몬테 까를로의 핵심은 수학적인 계산을 통해 Planning을 하는 것이 아니라 경험을 통한다는 것이다.

![[강화학습_4-2_(250325)_page-0005.jpg]]
- Policy가 정해져있을 때 그 Policy를 측정하는 방법을 배우고 있는 것이다.
- State 첫 방문에만 카운트를 올린다.

![[강화학습_4-2_(250325)_page-0006.jpg]]
- 어떤 State를 방문할 때마다 카운트를 올린다.

![[강화학습_4-2_(250325)_page-0007.jpg]]

![[강화학습_4-2_(250325)_page-0008.jpg]]
- 몬테 카를로 방식은 대수의 법칙을 따른다. 횟수가 늘어날수록 안정적으로 파악이 된다.


![[강화학습_4-2_(250325)_page-0009.jpg]]
- 이전 평균을 구해놓은 상태라면 다음 평균을 구할 때는, 이전 평균값과 현재 값을 사용하면 쉽게 구할 수 있다.
- 이 수식을 통해 State value값을 구한다.

![[강화학습_4-2_(250325)_page-0010.jpg]]
- $\frac{1}{N(S_t)}$는 매우 작은 수가 된다.(대수의 법칙을 따르며 시행 횟수(카운터)가 매우 높아지기 때문이다.)
	- 그래서 그냥 작은 상수로 설정한다.(예를 들어 $\alpha = 0.001$)

### Temporal-Difference Learning(TD)
![[강화학습_4-2_(250325)_page-0011.jpg]]

![[강화학습_4-2_(250325)_page-0012.jpg]]
- 아래 부분의 수식이 TD이다.
- 수식은 간단하지만, 담겨있는 아이디어는 단순하지 않다.
	- ![[Pasted image 20250407110411.png|200]]이를 타겟이라고 한다.
		- ==왜 이를 타겟이라고 부르냐면 ????==
	- ![[Pasted image 20250407110443.png|150]]이는 TD error이다.

- TD를 만든 이유(15페이지 참고)
	- MC 수식을 보면 한계가 보인다
		- 업데이트하려면 리턴값($G_t$)을 알아야한다.
		- 하지만 리턴 값을 얻으려면 수많은 시행횟수를 모두 시행해야만 한다.
		- 수행이 끝나기 전까지는 업데이트가 불가능하고 Pending 된다. -> 실시간성이 없다.
	- 실시간성을 확보하기 위해서 TD를 만들었다.
		- TD를 통해 한 스텝마다 업데이트가 가능해진다.
- MC: offline- learning
- TD: online-learning
![[강화학습_4-2_(250325)_page-0013.jpg]]

![[강화학습_4-2_(250325)_page-0014.jpg]]

![[강화학습_4-2_(250325)_page-0015.jpg]]

![[강화학습_4-2_(250325)_page-0016.jpg]]
- Bias는 '내가 추론에 의해 가고있는게 맞는지'에 대한 이야기이다.(편향성이 있느냐에 대한 이야기)
	- 에피소드를 끝까지 갔는데 이것이 optimun policy가 아닐 수도 있다.
![[강화학습_4-2_(250325)_page-0017.jpg]]

실시간성 -> TD 승
Bias -> MC 승