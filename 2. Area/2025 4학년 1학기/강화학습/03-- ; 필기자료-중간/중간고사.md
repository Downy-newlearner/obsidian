14일에 backward view까지 끝날 것 같음. 시험범위도 여기까지일듯

## 중간고사 리뷰
1. Let's assume we are devising an optimized stock investment algorithm that fits into the MDP framework. Identify its states, actions, and rewards
	- States: stock price, Action: Buy/sell, Reward: 차액

2. Write all the Bellman Optimality Equations in recursive form for both state value and action-state value functions.

3. Suppose you treated pole-balancing as an episodic task but also used discounting $\gamma$, with alll rewards zero except for -1 upon failure. What then would the return be at each time t?
	- 터미널 스테이트로 갈 때 reward가 -1 인 상황이다.
		![[Pasted image 20250422131158.png]]


![[요담 한솔고 올림푸스 - 5.jpg]]
$\gamma$가 0일 때는 reward 계산 뒷 부분이 사라지므로 무조건 left가 유리하다.
$\gamma=0.9$일 때는 계속 계산할 수록 right가 유리하다.
![[Pasted image 20250422131709.png|300]]
$\gamma=0.5$일 때는 두 상황이 같다.

6. Consider an MDP with a single nonterminal state and a single action that transitions back to the nonterminal state with probability p and transitions to the terminal state with probability $1-p$. Let the reward be $+1$ on all transitions and let $\gamma=1$. Suppose you observe one episode that lasts 10 transitions. What is the every-visit estimators (every-visit monte-carlo evaluation) of the value of the nonterminal state? Assume we initialize $G=0$.
	- ![[Pasted image 20250422132251.png]]