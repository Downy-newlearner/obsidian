

![[강화학습_3-2_(250318)_page-0001.jpg]]

![[강화학습_3-2_(250318)_page-0002.jpg]]
- Planning
	- 말그대로 계획 및 설계
	- 설계 후 시뮬레이션을 통해 State value를 계산하는 것이 우리가 원하는 것이다.
	- state value가 계산되면 정책 평가가 가능하고
	- 요약해서 planning은 경험을 하지 않아도 계산기를 두드려서 각 state의 state value값을 알 수 있어서 미리 계산한 값을 따라 action을 하는 것이다.


![[강화학습_3-2_(250318)_page-0003.jpg]]

![[강화학습_3-2_(250318)_page-0004.jpg]]
- 재귀적으로 최적화하는 것을 Dynamic Programming이라고 한다.
- subprogram을 통해 전체 program을 최적화한다.

![[강화학습_3-2_(250318)_page-0005.jpg]]
- DP의 중요한 뼈대
	- Optimal substructure
		- 전체를 최적화하기 위해서는 하위구조가 최적화가 되어있어야한다.
	- Overlapping subproblems
		- 이미 구한 값은 재귀적으로 이후에 다시 사용한다.

![[강화학습_3-2_(250318)_page-0006.jpg]]

![[강화학습_3-2_(250318)_page-0007.jpg]]

![[강화학습_3-2_(250318)_page-0008.jpg]]

![[강화학습_3-2_(250318)_page-0009.jpg]]


![[강화학습_3-2_(250318)_page-0010.jpg]]

![[강화학습_3-2_(250318)_page-0011.jpg]]

![[강화학습_3-2_(250318)_page-0012.jpg]]

![[강화학습_3-2_(250318)_page-0013.jpg]]

![[강화학습_3-2_(250318)_page-0014.jpg]]

![[강화학습_3-2_(250318)_page-0015.jpg]]

![[강화학습_3-2_(250318)_page-0016.jpg]]

![[강화학습_3-2_(250318)_page-0017.jpg]]
- Early stopping을 해도 Greedy한 choice에 있어서 거의 유사한 선택을 하게 된다.
- 분명 차이점이 있을 수 있지만 improvement를 거듭할 수록 차이가 줄어든다.


![[강화학습_3-2_(250318)_page-0018.jpg]]
- Greedy Choice에 대한 합당성을 논의하는 슬라이드

- 전제: Deterministic policy

- q func은 state action이 있는데 두 번째줄 수식에서 a 대신에 $\pi'(s)$가 존재한다.
	- ??

- 항상 monotonic improvement를 하는 이유
	- ![[Pasted image 20250325132728.png]]
	- 여기서 항상 $q_4^\pi$가 제일 크다고 하면 나머지의 transmission prob을 0으로 만든다.
	- 그렇다면
		- $v_\pi(s) = \frac{1}{4}(q_1^\pi + q_2^\pi +q_3^\pi + q_4^\pi)$에서
		- $v_\pi'(s) = 1 * q_4^\pi$가 되므로 이전보다 항상 크다.


![[강화학습_3-2_(250318)_page-0019.jpg]]


### Value Iteration
![[강화학습_3-2_(250318)_page-0020.jpg]]
- 
![[강화학습_3-2_(250318)_page-0021.jpg]]
- 상황
	- 모든 액션에 대한 immediately reward 값은 -1
	- 감마는 1
	- 모든 trans prob은 1
	- 다음 state에 대한 value function의 값은 항상 0
![[강화학습_3-2_(250318)_page-0022.jpg]]
- eval -> imporv 하지않고 (기댓값을 계산해서 가치 함수값을 수정하지 않고) 그냥 max값을 찾아 넣어 훨씬 빠르게 improv할 수 있다. 이것이 value iteration이다.
- 시뮬레이션할 때 value iteration을 자주 사용한다.

- 이 구조가 중요한 이유는 MDP가 굉장히 복잡해보이지만, 막상 해보면 코딩이 굉장히 쉬워진다.

![[강화학습_3-2_(250318)_page-0023.jpg]]