![[강화학습_6-1_(250408)_page-0001.jpg]]

![[강화학습_6-1_(250408)_page-0002.jpg]]
- ==시험 언급하심==
- 한 번 synchrous update를 하면 0, 0, 0, 0, 0.5가 된다.
- 0, 0, 0, 0.25, (0.5보다 큰 값, 내가 놓쳐서 못 들음)
- 그러다 E는 0.8정도 되는 값이 된다.

![[강화학습_6-1_(250408)_page-0003.jpg]]

![[강화학습_6-1_(250408)_page-0004.jpg]]
- Learning rate에 따른 step이 진행할 때 RMS의 변화를 나타낸 그래프이다.
- 횟수가 더 진행되면 MC가 훨씬 true value에 가까워져 그래프가 TD에 비해 더 아래에 있을 것이다.

![[강화학습_6-1_(250408)_page-0005.jpg]]
- 에피소드들과 각 에피소드에 포함된 리턴값을 모아서 한 번에 처리하는 것이 Batch MC이다.
- Batch TD는 다른 깊은 내용이 있으니까 일단 넘어감

![[강화학습_6-1_(250408)_page-0006.jpg]]

![[강화학습_6-1_(250408)_page-0007.jpg]]

![[강화학습_6-1_(250408)_page-0008.jpg]]

![[강화학습_6-1_(250408)_page-0009.jpg]]

![[강화학습_6-1_(250408)_page-0010.jpg]]
- MC는 조금 더 통계를 따른다.
- not markov 환경이란?
	- 
![[강화학습_6-1_(250408)_page-0011.jpg]]

![[강화학습_6-1_(250408)_page-0012.jpg]]

![[강화학습_6-1_(250408)_page-0013.jpg]]

![[강화학습_6-1_(250408)_page-0014.jpg]]
- 샘플링은 selection 하는 것이다.
	- 샘플들을 통해 평균값을 추정해나간다.
	- MC, TD 모두 샘플링을 한다.
	- DP는 샘플링이 아님
		- 가지고 있는 정보를 통해 계산을 할 뿐이다.
- Bootstrapping은 '외부의 도움 없이 스스로, 점진적으로 발전해나간다'는 의미이다.
	- 신발 끈은 내가 잡아당겨서 내가 신어서 단어를 이렇게 사용하게 된 것 같다.
![[강화학습_6-1_(250408)_page-0015.jpg]]


![[강화학습_6-1_(250408)_page-0016.jpg]]

![[강화학습_6-1_(250408)_page-0017.jpg]]

![[강화학습_6-1_(250408)_page-0018.jpg]]

![[강화학습_6-1_(250408)_page-0019.jpg]]

![[강화학습_6-1_(250408)_page-0020.jpg]]

![[강화학습_6-1_(250408)_page-0021.jpg]]
- 등비수열의 합
- 시험 공부할 때는 수식 한 번씩 써보기

![[강화학습_6-1_(250408)_page-0022.jpg]]

![[강화학습_6-1_(250408)_page-0023.jpg]]
- ---때문에 TD의 온라인 훈련에 대한 장점을 잃는다 그래서 나온게 백워드 뷰이다.(다다음페이지)

![[강화학습_6-1_(250408)_page-0024.jpg]]
lambda가 0일 때가 오리지널 TD이다.

- 성능이 개선되는 이유:
	1. **샘플 효율성**이 높기 때문
		- 제한된 에피소드 수 내에서 빠르게 좋은 성능을 낼 수 있다.
	- TD는 현재 state에서 수집한 transition을 이용해 바로 업데이트할 수 있다.
	- 따라서 자주 방문하는 state에 대해 빠르게 학습이 가능하다.
	- 반면 MC는 전체 에피소드를 마친 뒤 모든 state에 대해 업데이트하기 때문에, 초기에는 학습 속도가 느릴 수 있다.
		- 결과적으로 TD는 일부 state에 대한 정보만으로도 빠르게 성능 향상이 가능하다.

- **요약:** 동일한 에피소드에서 TD와 MC가 추출하고 활용하는 정보의 양과 방식이 다르다.
![[강화학습_6-1_(250408)_page-0025.jpg]]


![[강화학습_6-1_(250408)_page-0026.jpg]]
- $E(s)$는 eligibility traces이다.
- 


![[강화학습_6-1_(250408)_page-0027.jpg]]

![[강화학습_6-1_(250408)_page-0028.jpg]]

![[강화학습_6-1_(250408)_page-0029.jpg]]

![[강화학습_6-1_(250408)_page-0030.jpg]]

![[강화학습_6-1_(250408)_page-0031.jpg]]

![[강화학습_6-1_(250408)_page-0032.jpg]]
- 왼쪽이 많이 사용된다.
	- 구현 쉽고, 응용도 많이 됨
- 하지만 backward view는 알아둘 만한 가치가 있다.


![[강화학습_6-1_(250408)_page-0033.jpg]]

![[강화학습_6-1_(250408)_page-0034.jpg]]

![[강화학습_6-1_(250408)_page-0035.jpg]]
- **Monte Carlo(MC)** 방식과 **Temporal Difference(TD)** 방식이 **서로 어떤 관계**를 가지는지 수학적으로 보여주는 식이다.


![[강화학습_6-1_(250408)_page-0036.jpg]]

![[강화학습_6-1_(250408)_page-0037.jpg]]
![[Pasted image 20250414112443.png|400]]
- Eligibility trace는 한 state에 대해 정의된 것이다.
	- $S_1$에서 $S_1$에 대한 $E_t$
	- $S_1$에서 $S_2$에 대한 $E_{t+1}$
	- ...


![[강화학습_6-1_(250408)_page-0038.jpg]]

![[강화학습_6-1_(250408)_page-0039.jpg]]

![[강화학습_6-1_(250408)_page-0040.jpg]]
![[Pasted image 20250414113210.png|200]] == ![[Pasted image 20250414113228.png|300]]

![[Pasted image 20250414113738.png|500]]
==online update를 유지한채로 TD, ---?? 녹음 29분대 다시 듣기==


![[강화학습_6-1_(250408)_page-0041.jpg]]

![[강화학습_6-1_(250408)_page-0042.jpg]]

![[강화학습_6-1_(250408)_page-0043.jpg]]

![[강화학습_6-1_(250408)_page-0044.jpg]]