![[강화학습_8-1_(250422)_page-0001.jpg]]

![[강화학습_8-1_(250422)_page-0002.jpg]]

![[강화학습_8-1_(250422)_page-0003.jpg]]
- 지금까진 정책 평가에 대해서 공부했고, 이제부터 정책 개선에 대해 공부한다.
- MC의 정책 개선: Monte Carlo control
- TD의 정책 개선: SARSA, Q-learning

![[강화학습_8-1_(250422)_page-0004.jpg]]

![[강화학습_8-1_(250422)_page-0005.jpg]]
- 지금까지 배운게 on-policy learning
- 남의 정보를 가져와 배운 것이 off-policy learning
	- 내 스스로 연습하면서 배우면 on-policy learning(직접 연습)
	- 페이커 플레이 보면서 배우면 off-policy learning(보고 배움)

![[강화학습_8-1_(250422)_page-0006.jpg]]

![[강화학습_8-1_(250422)_page-0007.jpg]]

![[강화학습_8-1_(250422)_page-0008.jpg]]

![[강화학습_8-1_(250422)_page-0009.jpg]]
- Q값을 이용해서 정책을 개선한다.
- Q값은 특정 상태(state) s에서 특정 행동(action) a를 했을 때, 앞으로 받을 보상의 총합의 기대값을 의미한다.

![[강화학습_8-1_(250422)_page-0010.jpg]]

![[강화학습_8-1_(250422)_page-0011.jpg]]

![[강화학습_8-1_(250422)_page-0012.jpg]]
- Exploitation & Exploration
	- ![[Pasted image 20250422134620.png|400]]
	- a만 하다보면 b는 업데이트를 못 하는 상황이 발생한다.
	- 그러므로 b도 랜덤으로 수행해 업데이트가 되도록하는 기법을 E&E라고 한다.
![[강화학습_8-1_(250422)_page-0013.jpg]]

![[강화학습_8-1_(250422)_page-0014.jpg]]



![[강화학습_8-1_(250422)_page-0015.jpg]]
- $\epsilon$-Greedy Exploration은 E&E를 수행하기 위한 기법이다.


![[강화학습_8-1_(250422)_page-0016.jpg]]

![[강화학습_8-1_(250422)_page-0017.jpg]]
![[Pasted image 20250422135501.png|400]]
![[강화학습_8-1_(250422)_page-0018.jpg]]
- 입실론 그리디는 policy improvement 방법(정책 계선 전략이 입실론 그리디)

![[강화학습_8-1_(250422)_page-0019.jpg]]
- 각 Episode마다 policy improvement를 한다.
- MC control은 정확히 온라인 학습은 아니지만, 온라인 like 학습이다.

![[강화학습_8-1_(250422)_page-0020.jpg]]
![[Pasted image 20250428105731.png|400]]
- 원래 이 이미지와 같게 greedy

![[강화학습_8-1_(250422)_page-0021.jpg]]

![[강화학습_8-1_(250422)_page-0022.jpg]]
- 초기화
- 주어져있는 policy를 통해 에피소드를 진행 - 데이터를 뽑아냄
- 리턴값 계산
- 에피소드에서 첫 state에서 터미널 state로 갈 동안 튜플(state, action)들을 토대로 각 state의 카운터를 측정하고 업데이트 공식을 사용한다.

![[강화학습_8-1_(250422)_page-0023.jpg]]

![[강화학습_8-1_(250422)_page-0024.jpg]]

![[강화학습_8-1_(250422)_page-0025.jpg]]

![[강화학습_8-1_(250422)_page-0026.jpg]]

![[강화학습_8-1_(250422)_page-0027.jpg]]