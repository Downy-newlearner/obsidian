![[강화학습_9-1_(250428)_page-0001.jpg]]

![[강화학습_9-1_(250428)_page-0002.jpg]]

![[강화학습_9-1_(250428)_page-0003.jpg]]

![[강화학습_9-1_(250428)_page-0004.jpg]]
- s, a, r, s, a가 필요하다는 의미로 알고리즘 이름이 Sarsa이다.
- ![[Pasted image 20250428113729.png|400]]

![[강화학습_9-1_(250428)_page-0005.jpg]]

![[강화학습_9-1_(250428)_page-0006.jpg]]
- 순수히 본인의 정책 안에서 개선을 한다.(On-Policy)
- 다른 정책을 참조한다면 Off-Policy가 된다.
![[강화학습_9-1_(250428)_page-0007.jpg]]
- 8-1 20페이지 적분 이야기와 같은 맥락이다.
- 본인의 합은 무한대, 제곱의 합은 무한대보다 작아야한다.
- (step size는 1보다 작은 값이다.)
![[강화학습_9-1_(250428)_page-0008.jpg]]

![[강화학습_9-1_(250428)_page-0009.jpg]]

![[강화학습_9-1_(250428)_page-0010.jpg]]

![[강화학습_9-1_(250428)_page-0011.jpg]]

![[강화학습_9-1_(250428)_page-0012.jpg]]

![[강화학습_9-1_(250428)_page-0013.jpg]]

![[강화학습_9-1_(250428)_page-0014.jpg]]

![[강화학습_9-1_(250428)_page-0015.jpg]]

![[강화학습_9-1_(250428)_page-0016.jpg]]
- importance는 weight 정도로 생각하면 된다.
- importance sampling은 weighted sampling이라고 생각하면 된다.

- 수식: P에 따른 분포 -> Q에 따른 분포
- 분포를 바꿀 때 Importance sampling 기법을 사용한다.

![[강화학습_9-1_(250428)_page-0017.jpg]]
- 지금 정책이 아니라 다른 정책을 따랐을 때 Return(G)값이 어떻게 되었을까? 를 추측할 수 있다.

![[강화학습_9-1_(250428)_page-0018.jpg]]


## Q-Learning
![[강화학습_9-1_(250428)_page-0019.jpg]]

![[강화학습_9-1_(250428)_page-0020.jpg]]

![[강화학습_9-1_(250428)_page-0021.jpg]]

![[강화학습_9-1_(250428)_page-0022.jpg]]
- Sarsa도 off-policy로 바꿀 수 있다.
- Q-learning의 경우는 importance sampling을 생략해도 같은 결과가 나온다. 
	- 그래서 구현이 용의하다.
	- 다만 반드시 max 값을 사용해야한다. ![[Pasted image 20250429133104.png|300]]
	- 안 쓰면 그냥 off policy sarsa이다.

![[Pasted image 20250429133323.png|400]]



## Sarsa와 Q-learning의 동작 차이는 어떤 것이 있을까?
![[강화학습_9-1_(250428)_page-0023.jpg]]
![[Pasted image 20250429133437.png|400]]





![[강화학습_9-1_(250428)_page-0024.jpg]]
- 많은 Episode를 이용할 수 있는 환경이라면 Sarsa가 적합하다.(Sarsa가 더 stable하다.)
- 


![[강화학습_9-1_(250428)_page-0025.jpg]]

![[강화학습_9-1_(250428)_page-0026.jpg]]