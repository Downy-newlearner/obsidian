![[강화학습_9-1_(250428)_page-0001.jpg]]

![[강화학습_9-1_(250428)_page-0002.jpg]]

![[강화학습_9-1_(250428)_page-0003.jpg]]

![[강화학습_9-1_(250428)_page-0004.jpg]]
- s, a, r, s, a가 필요하다는 의미로 알고리즘 이름이 Sarsa이다.
- ![[Pasted image 20250428113729.png|400]]

![[강화학습_9-1_(250428)_page-0005.jpg]]

![[강화학습_9-1_(250428)_page-0006.jpg]]
- 순수히 본인의 정책 안에서 개선을 한다.(On-Policy)
- 다른 정책을 참조한다면 Off-Policy가 된다.
![[강화학습_9-1_(250428)_page-0007.jpg]]
- 8-1 20페이지 적분 이야기와 같은 맥락이다.
- 본인의 합은 무한대, 제곱의 합은 무한대보다 작아야한다.
- (step size는 1보다 작은 값이다.)
![[강화학습_9-1_(250428)_page-0008.jpg]]

![[강화학습_9-1_(250428)_page-0009.jpg]]

![[강화학습_9-1_(250428)_page-0010.jpg]]

![[강화학습_9-1_(250428)_page-0011.jpg]]

![[강화학습_9-1_(250428)_page-0012.jpg]]

![[강화학습_9-1_(250428)_page-0013.jpg]]

![[강화학습_9-1_(250428)_page-0014.jpg]]

![[강화학습_9-1_(250428)_page-0015.jpg]]

![[강화학습_9-1_(250428)_page-0016.jpg]]
- importance는 weight 정도로 생각하면 된다.
- importance sampling은 weighted sampling이라고 생각하면 된다.

- 수식: P에 따른 분포 -> Q에 따른 분포
- 분포를 바꿀 때 Importance sampling 기법을 사용한다.

![[강화학습_9-1_(250428)_page-0017.jpg]]
- 지금 정책이 아니라 다른 정책을 따랐을 때 Return(G)값이 어떻게 되었을까? 를 추측할 수 있다.

![[강화학습_9-1_(250428)_page-0018.jpg]]


## Q-Learning
![[강화학습_9-1_(250428)_page-0019.jpg]]

![[강화학습_9-1_(250428)_page-0020.jpg]]

![[강화학습_9-1_(250428)_page-0021.jpg]]

![[강화학습_9-1_(250428)_page-0022.jpg]]
- Sarsa도 off-policy로 바꿀 수 있다.
- Q-learning의 경우는 importance sampling을 생략해도 같은 결과가 나온다. 
	- 그래서 구현이 용의하다.
	- 다만 반드시 max 값을 사용해야한다. ![[Pasted image 20250429133104.png|300]]
	- 안 쓰면 그냥 off policy sarsa이다.

![[Pasted image 20250429133323.png|400]]



## Sarsa와 Q-learning의 동작 차이는 어떤 것이 있을까?
![[강화학습_9-1_(250428)_page-0023.jpg]]
![[Pasted image 20250429133437.png|400]]





![[강화학습_9-1_(250428)_page-0024.jpg]]
- 많은 Episode를 이용할 수 있는 환경이라면 Sarsa가 적합하다.(Sarsa가 더 stable하다.)
	- Sarsa가 stable 해질 수 있는 이유는 다음과 같다.
		- noisy한 환경이다.
			- Transition prob이 deterministic하지 않다.
			- 이는 Q-learning의 성능을 저하시킨다.
- 하지만 사용할 수 있는 Episode가 매우 적다면, 보다 공격적인 Q-Learning을 사용하는 것이 좋을 수 있다.(High risk high return)
	- Q-learning은 환경이 너무 광범위해서 배워야하는 요소들이 너무 많을 때 적합하다.
	- "너무 노력하지 말고 퍼뜩퍼뜩 해보자"라는 컨셉이다.
	- 그래서 실제로 산업에서는 Q-learning을 많이 사용한다.
	- 다만 위험성이 있으므로 자율주행처럼 mission critical한 작업에 대해서는 Sarsa를 사용한다.

- 요약
	- Stable: Sarsa
	- Unstable but expectable a good performance: Q-learning


![[강화학습_9-1_(250428)_page-0025.jpg]]
- 이 슬라이드는 개념 정리하는 용도로 가볍게 보기

![[강화학습_9-1_(250428)_page-0026.jpg]]- 이 슬라이드는 개념 정리하는 용도로 가볍게 보기