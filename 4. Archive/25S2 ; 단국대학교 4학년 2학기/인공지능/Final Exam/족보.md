1. KNN 과 Kmeans에서 n값이 무슨 차이냐
2. 그 역전파 chain rule 식 하나주고 식 의미 적어라
3. 그래프 3개 주고 뭐가 과소적합 최적 과대적합인지 알아내고 과대적합일 때 무슨 기법 적용?
4. 지금까지 배운 알고리즘
5. 베이즈 정리 확률 계산
6. Prolog에서도 한 문제 
7. 퍼셉트론 Xor 문제로 mlp나왔다. 선형문제에 대해서 물어보는거. 퍼셉트론이 어떤 문제를 해결할 수 있냐. -> 그러면 선형문제 wx +b 선형문제풀고 "Xor 문제로 mlp나왔다."를 설명하면 됨.
8. 지도학습과 비지도학습의 차이는? -> 라벨의 유무

## 1. KNN의 K 값과 K-means의 K 값 차이

### 핵심 정리
- KNN의 k: "분류 시 참고하는 이웃 데이터 수"
- K-means의 k: "데이터를 나눌 클러스터의 개수"

### 상세 설명

KNN은 지도학습 알고리즘으로, 레이블이 있는 데이터를 기반으로 새로운 점을 분류한다. 새로운 데이터 주변에서 가장 가까운 k개의 이웃을 찾고 다수결로 클래스를 결정한다.
반면 K-means는 비지도학습 알고리즘으로, 데이터 내부 구조를 파악하여 k개의 중심(centroi)을 만들고 각 점을 가장 가까운 중심에 배정해 k개의 군집을 형성한다.

자료에서도 KNN은 "가장 가까운 이웃 클래스 정보를 따르는 분류 방법", K-means는 "주어진 n개의 관측값을 k개의 클러스터로 분할하는 군집화 알고리즘"으로 소개된다.


**KNN의 k**
- KNN은 새로운 데이터를 분류하기 위해 가장 가까운 이웃 k개를 사용한다.
	- 자료에서 "k-NN 알고리즘은 새로운 데이터를 가장 가까운 이웃 클래스로 할당"한다고 언급

- k는 보통 홀수로 선택

> 즉, KNN의 k는 "투표에 참여하는 이웃 수"이다.

**K-means의 k**
- K-means는 데이터를 k개의 클러스터로 나누는 비지도 학습
	- "주어진 n개의 관측값을 k개의 클러스터로 분할"
- k는 클러스터의 개수이며, 초기 중심 k개 생성 -> 반복적으로 업데이트

> 즉, K-means의 k는 "미리 정해 둔 군집의 개수(만들고 싶은 그룹 수)"이다.


## 2. 역전파 chain rule 식과 의미

### 핵심 정리
역전파에서 가중치 업데이트는 다음 식으로 표현된다:
$$W = W - \alpha \frac{\partial Loss}{\partial W}$$

이는 Chain Rule을 이용해 출력층의 오차가 은닉층 방향으로 역방향 전파된다는 의미.

### 상세 설명
- MLP의 학습은 순전파로 출력을 계산한 뒤, 출력과 정답의 차이(Loss)를 통해 오차를 역방향으로 전달한다. 이때 각 층에 도달하는 오차의 크기는 chain rule을 이용해 계산된다.
- 즉, 출력층에서 시작된 오차가 은닉층으로 단계별로 전달되며, 각 가중치가 전체 Loss에 얼마나 영향을 미쳤는지 계산하여 가중치를 수정한다.

### 개념 요약
- 역전파(backpropagation) = Chain rule로 오차를 나누어 각 가중치에 부여하는 과정


## 3. 과소적합/최적/과대적합 그래프 판별 & 과대적합 해결법

### 핵심 정리
- 과소적합(underfitting): 모델이 너무 단순해 패턴을 학습하지 못함
- 적절한 모델(best fit): 훈련/테스트 모두에서 좋은 성능
- 과대적합(overfitting): 훈련 데이터에만 과도하게 맞추고 일반화 실패


### 상세 설명

선형 회귀와 딥러닝 강의 모두에서 등장하는 내용인데, 과소적합은 훈련 데이터에서도 오차가 큰 형태의 그래프로 나타나고, 최적 모델은 전체 데이터의 경향만 잘 따라가며, 과대적합 그래프는 훈련 데이터의 작은 요동까지 지나치게 따라가는 매우 구불구불한 형태를 보인다.

딥러닝 챕터에서는 과대적합을 줄이기 위한 대표적인 방법으로 드롭아웃(dropout), 규제항(regularization), 데이터 증강(data augmentation) 등이 소개된다.

### 개념 요약
- 과대적합 해결 -> 드롭아웃, Regularization, 데이터 증가, 적절한 모델 복잡도 조절

## 4. 지금까지 배운 알고리즘 전체 정리

### 핵심 정리
한 학기 동안 다룬 알고리즘들은 크게 지도학습 / 비지도학습 / 규칙 기반 AI / 신경망 / 확률, 퍼지 기반 AI로 나눌 수 있다.

### 상세 설명

#### 지도 학습
- 선형 회귀(Linear Regression)
- KNN 분류
- 퍼셉트론(Perceptron)
- MLP, DNN, CNN 등 딥러닝 모델 (회귀, 분류 전반 포함)

#### 비지도 학습
- K-means clustering
	-> "레이블 없이 스스로 그룹을 찾는 학습"으로 소개됨

#### 논리 기반 AI
- 술어 논리 기반 추론
- Prolog 규칙 / 사실 / 질의 처리

#### 확률 기반 AI
- 베이즈 정리, 조건부 확률
- 불확실성 처리(Certainty Factor)

#### 퍼지 논리
- 퍼지 집합, 소속함수, 퍼지 추론

### 개념 요약
- 수학적 모델(회귀) + 통계와 확률(베이즈) + 논리(Prolog) + 신경망(MLP, CNN) = 전체 AI 기초 구성


## 5. 베이지 정리 확률 계산

### 핵심 정리
$$P(H|E)=\frac{P(E|H)P(H)}{P(E|H)P(H)+P(E|\neg H)P(\neg H)}$$


### 상세 설명

강의에서는 과일 문제, 카드 문제, Z-바이러스 문제 등 다양한 예제를 통해 조건부 확률 계산을 연습한다.
핵심은 "특정 사건이 관측되었을 때, 그 원인이 무엇일 확률인가?"를 계산하는 식이며, 사전 확률, 사후 확률 개념을 함께 사용한다.
베이즈 추론은 불확실한 상황에서 결론의 신뢰도를 정량적으로 계산하는 기법이다.

#### H와 E의 풀네임

> H = Hypothesis(가설)

- 우리가 "원인"으로 삼고 싶은 사건, 즉 설명하려는 대상
- 설명하고 싶은 원인
- 예: "바이러스 감염이다.", "해당 과일이 그릇 \#1에서 왔다.", "스팸 메일이다."


> E = Evidence (증거)

- 관측된 정보, 즉 실제로 관찰된 현상
- 이미 관측된 현상
- 예: "검사 결과가 양성이다.", "선택한 과일이 바나나다", "메일 제목에 특정 단어가 있다"

따라서 $P(H|E)$는 "증거 E가 관측되었을 때, 가설 H가 참일 확률을 의미한다."

#### 베이즈 정리를 왜 사용하는가?
- 현실의 정보는 불확실성을 포함한다.
- 예를 들어 "기침(E)을 한다고 해서 무조건 감기(H)는 아님."
- 확률적으로 어느 정도 가능성이 있는지를 정량화해야 한다.
- 이런 상황에서 베이즈 정리는 "역확률(inverse probability)"를 계산하는 유일하고 수학적으로 일관된 도구이다.

즉, "관측된 사실 E가 주어졌을 때, 어떤 원인 H가 가장 그럴듯한가?"를 계산하기 위해 사용한다.

### 개념 요약

- 관측(E)이 주어졌을 때 원인(H)의 확률을 역으로 추론하는 방식
- 베이즈 정리의 핵심은
	- 관측(E) -> 가능한 원인(H)들 중 어느 것이 더 그럴듯한가?


## 6. Prolog 문제(사실, 규칙, 질의 이해)

### 핵심 정리

Prolog는 규칙과 사실로 지식을 표현하고, 질의(query)를 통해 추론하는 논리 프로그래밍 언어이다.

### 상세 설명

프롤로그에서 규칙은
```
father(Y,X) :- child(X,Y), male(Y).
```
처럼 Head :- Body 구조르르 가지며, Body 조건이 참일 때 Head가 성립한다.
또한 변수는 대문자로 시작하고, 통일화(unification)을 이용해 질의에 맞는 값을 자동으로 찾아준다.

### 개념 요약

- Prolog = 논리 규칙 기반 AI
- 사실 + 규칙 -> 추론
- 질의 -> 가능한 해를 순차적으로 탐색(backtracking)


## 7. 퍼셉트론이 해결 가능한 문제 & XOR 등장 이유

### 핵심 정리

- 퍼셉트론은 선형 분리가 가능한 문제만 해결 가능
- XOR 문제는 선형 분리 불가능하여 퍼셉트론 1층 구조로 해결 불가
- 이를 해결하기 위해 MLP(다층 퍼셉트론) 등장

### 상세 설명

퍼셉트론은 입력의 가중합을 기준으로 직선(또는 초평면) 하나로 데이터를 나눈다.
따라서 AND/OR 같은 선형 분리가 가능한 문제는 해결하지만, XOR처럼 두 클래스를 하나의 직선으로 나눌 수 없는 문제는 학습되지 않는다.

자료에서도 "퍼셉트론은 XOR을 학습할 수 없다."가 명확하게 제시되며, MLP의 은닉층이 비선형 변환을 제공해 XOR을 해결할 수 있음이 보여진다.

### 개념 요약

- 퍼셉트론 -> 직선 분류기
- XOR -> 직선으로 분리가 안 됨 -> MLP 필요


## 8. 지도학습 vs 비지도학습 (라벨 유무)

### 핵심 정리

- 지도학습: 입력 + 정답(label)을 제공하여 학습
- 비지도학습: 레이블 없이 구조 / 패턴을 스스로 학습

### 상세 설명

강의에서는 지도학습을 회귀, 분류로 나누어 설명하며, 모델이 "정답을 보면서 학습하는 방식"임을 강조한다.
반면 비지도학습은 라벨이 없으며, 대표적으로 K-means처럼 "데이터의 유사성을 기반으로 묶는 방식"을 사용한다.

### 개념 요약

- 지도학습 -> "정답이 있는" 교육
- 비지도학습 -> "정답 없이 스스로 구조를 찾는" 방식


