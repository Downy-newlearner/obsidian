
  

이 자료의 본질은 단순히 “RAG가 무엇인가”가 아니라,

  

> **LLM의 구조적 한계를 어떻게 ‘시스템 설계’로 보완하는가**

> 를 이해시키는 데 있습니다.

---

## **1장: RAG의 등장 배경 – 왜 RAG가 필요한가**

  

### **핵심 내용 (2문단)**

  

RAG는 **LLM이 학습하지 않은 정보, 생성 이후의 최신 정보, 접근할 수 없는 내부 데이터를 다룰 수 없다는 한계**에서 출발한다. LLM은 고정된 학습 데이터와 파라미터를 기반으로 동작하기 때문에, 아무리 큰 모델이라도 **지식의 최신성·정확성·도메인 특수성**에서 구조적 제약을 가진다.



이 한계를 해결하기 위해, 질문과 함께 **외부 지식을 검색하여 컨텍스트로 제공**하는 방식이 RAG이다. 즉, LLM이 “모든 것을 알고 있다”는 전제를 버리고, **필요한 정보를 그때그때 찾아서 참고하도록 만드는 구조**다. 이는 모델 성능 향상이 아니라 **시스템 아키텍처의 변화**라는 점이 핵심이다.

---

### **핵심 질문**

- LLM의 어떤 한계가 RAG를 필연적으로 요구하게 되었는가?
    
- Fine-tuning으로는 해결할 수 없는 문제는 무엇인가?
    
- RAG는 모델을 바꾸는가, 시스템을 바꾸는가?
    

---

### **시험 문제(예시)**

- Retrieval Augmented Generation이 등장하게 된 배경을 설명하라.
    
- LLM 단독 사용의 한계를 예를 들어 설명하라.
    

---

## **2장: RAG의 전체 구조 (Pipeline 관점)**

  

### **핵심 내용 (2문단)**

  

RAG는 일반적으로 **Document Parsing → Chunking → Embedding → Vector DB 저장 → Query Embedding → Similarity Search → Retrieval → Generation**의 파이프라인으로 구성된다. 이 중 어느 하나라도 부실하면 전체 품질이 급격히 저하된다. 즉, RAG는 “검색 + 생성”이 아니라 **다단계 시스템**이다.

  

특히 중요한 점은 Retrieval 결과가 LLM의 입력 컨텍스트로 **직접 포함**된다는 것이다. 이 때문에 검색 정확도뿐 아니라, **문서 분할 방식, 컨텍스트 길이, 중복 정보, 노이즈**까지 모두 생성 결과에 영향을 준다. RAG의 품질 문제는 LLM 문제가 아니라 **파이프라인 설계 문제**인 경우가 대부분이다.

---

### **핵심 질문**

- RAG 파이프라인에서 가장 품질에 민감한 단계는 어디인가?
    
- 검색 품질이 좋아도 생성 품질이 나쁠 수 있는 이유는 무엇인가?
    
- RAG는 왜 “엔드투엔드 모델”이 아니라 “조합 시스템”인가?
    

---

### **시험 문제(예시)**

- RAG의 전체 처리 흐름을 단계별로 설명하라.
    
- RAG 시스템에서 chunking이 중요한 이유를 설명하라.
    

---

## **3장: Document Parsing & Chunking**

  

### **핵심 내용 (2문단)**

  

Document Parsing은 PDF, 웹 문서, 이미지 등 **비정형 문서를 검색 가능한 텍스트 단위로 변환**하는 과정이다. 여기에는 단순 텍스트 추출뿐 아니라, **레이아웃 파싱, 테이블 인식, 이미지 설명 추출**까지 포함될 수 있다. 이 단계에서 정보가 누락되면, 이후 단계에서 복구할 방법이 없다.

  

Chunking은 문서를 **임베딩 가능한 적절한 크기의 조각으로 나누는 작업**이다. 너무 작으면 문맥이 깨지고, 너무 크면 검색 정밀도가 떨어진다. 따라서 chunking은 고정 길이 문제가 아니라, **의미 단위와 구조를 고려한 설계 문제**다.

---

### **핵심 질문**

- Document Parsing 품질이 RAG 전체 성능에 미치는 영향은 무엇인가?
    
- Chunk 크기를 크게/작게 했을 때의 트레이드오프는 무엇인가?
    
- 의미 기반 chunking이 필요한 이유는 무엇인가?
    

---

### **시험 문제(예시)**

- RAG에서 Document Parsing과 Chunking이 중요한 이유를 설명하라.
    
- Chunking 전략에 따라 검색 결과가 어떻게 달라질 수 있는지 설명하라.
    

---

## **4장: Embedding, Vector DB, Similarity Search**

  

### **핵심 내용 (2문단)**

  

RAG에서 Embedding은 문서와 질의를 **같은 벡터 공간으로 사상**하는 핵심 단계다. 이때 사용하는 임베딩 모델의 특성(차원, 언어 성능, 도메인 적합성)은 검색 결과에 직접적인 영향을 준다. 문서 임베딩과 질의 임베딩이 **같은 분포를 공유하지 않으면**, 검색은 실패한다.

  

Similarity Search는 고차원 공간에서 질의 벡터와 가장 가까운 문서 벡터를 찾는 과정이다. 이 과정은 대부분 근사 검색(ANN)을 사용하며, 정확도와 속도 사이의 균형이 중요하다. 이 단계에서의 오류는 **LLM이 잘못된 근거를 바탕으로 그럴듯한 답을 생성하는 환각**으로 이어질 수 있다.

---

### **핵심 질문**

- 문서 임베딩과 질의 임베딩은 왜 같은 모델을 사용해야 하는가?
    
- Similarity Search의 오류가 왜 치명적인가?
    
- 벡터 검색 결과가 “정답”이 아닐 수 있는 이유는 무엇인가?
    

---

### **시험 문제(예시)**

- RAG에서 임베딩 모델 선택이 중요한 이유를 설명하라.
    
- Similarity Search 결과가 잘못되었을 때 발생할 수 있는 문제를 설명하라.
    

---

## **5장: Retrieval & Generation (Augmented Query)**

  

### **핵심 내용 (2문단)**

  

Retrieval 단계에서는 Similarity Search를 통해 선택된 문서 조각들이 **LLM 입력 컨텍스트로 결합**된다. 이때 단순히 문서를 나열하는 것이 아니라, **질문과 결합된 Augmented Query** 형태로 구성된다. 이 구성 방식에 따라 LLM이 참고하는 정보의 비중과 해석이 달라진다.

  

Generation 단계에서 LLM은 Retrieval된 정보에 **강하게 의존**하지만, 여전히 확률 기반 생성 모델이라는 점은 변하지 않는다. 따라서 RAG는 환각을 “완전히 제거”하는 것이 아니라, **환각 가능성을 구조적으로 낮추는 접근**임을 이해해야 한다.

---

### **핵심 질문**

- Augmented Query란 무엇이며, 왜 필요한가?
    
- RAG가 환각을 완전히 제거하지 못하는 이유는 무엇인가?
    
- Retrieval 결과가 많을수록 항상 좋은가?
    

---

### **시험 문제(예시)**

- RAG에서 Retrieval 결과가 Generation에 반영되는 방식을 설명하라.
    
- RAG가 hallucination을 줄이는 원리를 설명하라.
    

---

## **12번 자료를 관통하는 한 줄 핵심**

  

> **RAG는 “LLM을 더 똑똑하게 만드는 기술”이 아니라**

> **“LLM을 믿지 않고도 쓸 수 있게 만드는 시스템 설계”다.**

---

이제 흐름이 매우 명확해졌습니다.

- **10번**: 임베딩과 벡터 공간
    
- **11번**: LLM의 능력과 한계
    
- **12번**: 그 한계를 구조적으로 해결하는 RAG
    

  