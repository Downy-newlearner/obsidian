## 1. Hadoop의 등장 배경
### 2000년대 이전 분산처리 시스템의 기본 개념과 한계점
- 당시 분산 시스템은 주로 학술 및 과학 연구용으로, 수십~수백 노드로 제한된 규모였음
- Grid Computing, Beowulf Cluster, GFS 등의 시도가 있었으나,
	- 고비용 하드웨어 필요
	- 중앙 집중식 제어 -> SPOF(HA의 반대)
	- 확장성 한계 -> 수직적 확장 방식

- 수직적 확장이란 기존의 한 대의 서버에 CPU, 메모리, 디스크 등을 더 추가하는 방식이다. 한 대에 서버의 성능에 의존을 하다보니 무한히 부품을 추가할 수도 없는 노릇이어서 확장성에 한계가 있었다.
- 반면 HDFS가 가지는 장점중 선형적 확장이 가능하다는건 다시 말해 수평적 확장이 가능하다는 건데, 성능 확장이 필요하면 서버를 더 추가하는 방식이었다. 그러니 서버만 2배 늘리면 성능도 거의 2배 향상되는 이상적인 수평 확장 형태였던 것이다.
- HDFS는 수천 대 노드(서버)까지 선형 확장이 가능하다고 평가받으며 높은 확장성이 장점으로 꼽힌다.

### 기존 시스템의 실패 원인 분석하기
- 수직적 확장 한계: 서버 성능에만 의존 -> 물리적/경제적 제약
- 장애 허용성 부족: 하나의 노드 장애가 전체 시스템 중단으로 이어짐
- 고비용 유지비
- 확장 복잡성

### Doug Cutting과 Yahoo의 Hadoop 탄생 배경과 역사적 맥락 파악하기
- Lucene/Nutch 개발자 Doug Cutting이 GFS/MapReduce 논문(2003)에 영감을 받아 Hadoop 구상
- 2006년 Yahoo가 Cutting을 영입하며 Hadoop 개발 본격화
- 2008년부터 Yahoo가 프로덕션에 도입하며 성장 가속
- Hadoop은 이후 아파치 최상위 프로젝트로 승격

## 2. HDFS - Hadoop의 분산 파일 시스템
### HDFS의 철학과 사상
- Write-once, Read-many 철학
- 수정 불가 / 생성,삭제 중심의 설계
- 동시성 제어 불필요 -> 복잡성 감소
- 장애 복구 간단화 / 처리량 최적화
### 아키텍처와 주요 컴포넌트
- NameNode: 메타데이터 관리
- DataNode: 실제 데이터 블록 저장 및 주기적 보고
- Secondary NameNode: 메타데이터 체크포인트 역할 (백업 아님)

- 대용량 블록 사용의 장점
	1. 탐색 비용 감소: 디스크 탐색 시간을 줄여 처리 속도 향상
	2. 네트워크 오버헤드 감소: 데이터 처릴 시 메타데이터 교환 비용 절감
	3. 관리 단순화: 수백만개의 작은 블록보다 수천 개의 큰 블록 관리가 효율적
	4. ==MapReduce 작업에 최적화: 대용량 데이터 처리를 위한 병렬 작업 단위로 적합==
### 버전별 발전과 혁신
#### Hadoop 1.x
- ==단일 Namenode 구조를 사용==
	- SPOF(Single Point of Failure): Namenode에 장애 발생 시 전체 클러스터 사용 불가, 한 부분만 죽어도 전부 망하는 구조
	- 확장성 제한: 최대 4000 노드, 메타데이터를 단일 서버 메모리에 저장
	- 자원 관리 한계: ==MapReduce가 자원 관리와 데이터 처리 모두 담당==(추후 YARN 도입으로 해결)
		- Hadoop 2.x에서 YARYN 도입 이후에 MapReduce가 자원 관리 역할에서 완전히 손을 뗐다.
	- MapReduce 이외 처리 방식 지원 어려움

_JobTracker_  ← MapReduce의 중앙 관리자 (자원 + 작업 관리)
   │
   └── _TaskTracker_ (각 노드에서 맵/리듀스 실행)
- ==JobTracker==: 전체 클러스터의 자원 상태를 관리하면서 동시에 Job 스케줄링, Task 실패 복구, 상태 모니터링 등을 담당
- ==TaskTracker==: 실제 데이터를 처리하는 노드
-> 자원 관리 + 작업 실행이 한 시스템(MapReduce)에 통합되어있었다.

하지만 문제는 JobTracker가 모든 노드를 관리하다보니 확장성 부족
하나가 죽으면 전부 멈추는 SPOF
다른 형태의 워크로드(예: Spark, Tez)를 동시에 돌리기 어려움


#### Hadoop 2.x
- High Availability(HA): 액티브-스탠바이 Namendoe 구성으로 단일 장애점 제거
- HDFS Federation: 네임 스페이스 볼륨 분할로 확장성 개선, 네임노드 메모리 한계 극복
- YARN 도입
- Snapshot 지원

_YARN ResourceManager_ ← 자원 관리 담당 (클러스터 공통)
     │
     ├── _NodeManager_ ← 각 노드의 자원 관리
     └── _ApplicationMaster_ ← 개별 애플리케이션 관리
            └── _MapReduce v2 (MRv2)_

- 자원 관리: YARN(리소스 매니저 + NodeManager) -> 클러스터 전체 CPU, 메모리, 컨테이너 관리
- 작업 스케줄링: YARN ApplicationMaster
- 데이터처리: MapReduce v2
#### Hadoop 3.x
- Erasure Coding(저장용량 낭비를 줄이면서 복원성 확보)
	- Reed-Solomon 알고리즘
	- 6+3 구성
		- 데이터블록 6개 + 패리티 블록 3개
- Java 11 지원, Docker/Kubernetes 통합 지원
### 현대 환경과의 연계(클라우드, Kubernetes와 HDFS의 통합 및 변화하는 역할)
- 클라우드 통합: 온프레미스+클라우드 하이브리드 구성 가능
- Kubernetes와는 상성이 좋지 않음
	- HDFS는 Stateful, K8s는 Stateless 지향
		- HDFS는 내부에 저장 상태가 꼭 필요(컨테이너 재시작이 어렵고 위험)
		- K8s는 Stateless 환경 지향(HDFS처럼 상태를 유지해야 하는 시스템에는 부적합)
	- 볼륨 관리 복잡 / 자동 확장 어려움
- 점점 S3, GCS, Azure Blob 등 클라우드 오브젝트 스토리지로 대체되는 추세


-> 참고로 Spark는 Stateless다.(계보를 기억하긴 하지만 정확히는 "연산 계보"를 기억하는거지 메모리 상태(state)를 기억하는게 아니다.)
-> Spark Streaming은 Stateful이 될 수 있다.
### 생태계 확장성(HDFS 기반 오픈소스/상용 솔루션과 미래 전망)

#### HDFS의 성공 이유
1. 다양한 빅데이터 도구들과의 호환성
2. 안정성: 데이터 복제 매커니즘(3-way, Erasure coding)을 통한 내결함성 확보
3. 확장성: 수천 노드까지 선형적 확장이 가능한 구조로 페타바이트 규모 데이터 처리 역량
4. 비용 효율성: 일반 상용 하드웨어 활용으로 구축 및 유지비용 절감
#### HDFS의 한계 및 도전과제
1. 작은 파일 처리 비효율성
2. 실시간 데이터 처리 불가
3. 낮은 IO 비율로 고속 트랜잭션 처리에 부적합

그래서 요즘은 S3, GCS, Azure Blob Stoarge로 대체하는 추세이다.

## 3. ==MapReduce== - 분산 처리 시스템

![[Pasted image 20251029060142.png]]
### MapReduce의 역할과 Hadoop에서의 분산처리 원리
- Google이 대규모 웹 인덱싱을 위해 고안한 프로그래밍 모델
- 데이터를(Key, Value) 쌍으로 분할하여 Map 단계에서 분산 처리하고, 이후 Reduce 단계에서 집계
- HDFS에서 데이터를 읽고 작업을 병렬로 분산 실행
- 분산 환경에서도 데잍어 손실 없이 자동 복구되는 구조
- JobTracker(TaskTracker)가 모든 분산 작업의 흐름을 제어

### Spark와의 비교
- MapReduce는 디스크 기반 처리 -> 반복 연산/대화형 작업에 비효율
- Spark는 메모리 기반 처리(RDD) -> 빠른 처리 속도 및 반복 연산에 최적화
- MapReduce는 단순 배치 작업에 적합, Spark는 머신러닝,실시간 분석에도 적합
- 처리 속도: Spark가 MapReduce 대비 최대 100배 빠름

### 발전사와 전망
- 초기에는 Hadoop 생태계의 중심
- 이후 Spark, Flink 등 고속/유연한 엔진의 등장으로 역할 축소
- 단순 배치 처리에는 여전히 안전적인 선택지

### MapReduce에 대한 Hadoop 1.x에서의 중심적 역할에서 Hadoop 3.x에서의 위치 변화
- Hadoop 1.x: MapReduce가 자원 관리 + 데이터 처리까지 전담하는 구조
	- JobTracker(JT)의 병목 및 SPOF 문제 존재
- Hadoop 2.x: YARN의 등장 -> 자원 관리 기능 분리, MR는 단순 처리 엔진으로 전환
- Hadoop 3.x: Spark, Tez, Hive LLAP 등 다양한 처리 방식과 공존하는 하나의 선택지
	- 독점적 위치에서 벗어나 역할이 제한됨

### Spark와 같은 인메모리 기반 엔진으로의 진화 흐름
- Spark(RDD 기반), Flink, Dask 등 다양한 후속 기술들 등장
- MR는 단순 배치 처리용 백엔드로 잔존하고 있음


## 4. ==Yarn== - 자원 관리/제어 시스템

### 등장 배경과 목적
- Hadoop 1.x의 JobTracker 단일 구조 한계 극복을 위해 도입
	- 확장성 부족, SPOF, 다양한 워크로드 지원 불가 등의 문제
- Resource Management와 Application Execution을 분리
- 다양한 데이터 처리 엔진(MR, Spark, Hive 등)의 통합 자원 관리자 역할 수행

### 핵심 컴포넌트와 아키텍처
- ResourceManager: 전체 클러스터 자원 관리
- NodeManager: 개별 노드의 자원 사용 모니터링 및 보고
- ApplicationMaster: 각 애플리케이션 별로 생성되어 작업 스케줄링과 실행 제어 담당

                  ResourceManager
                     │
 ┌───────────┴───────────┐
 │                                                                                            │
NodeManager                                                      NodeManager
     │                                                                                   │
 ApplicationMaster                                            ApplicationMaster

### 스케줄링과 자원 할당
- Container 단위로 자원 할당
- 다양한 스케줄러 제공:
	- Capacity Scheduler: 멀티 태넌스 환경에 적합
	- Fair Scheduler: 모든 작업에 공정한 자원 배분
- YARN은 자원 사용량 기반의 동적 할당을 지원하여 클러스터 효율을 극대화함
### 생태계 연동과 미래
- YARN 위에서 동작 가능한 엔진: MR v2, Spark, Tez, Flink, Hive 등
- Docker, GPU 등 리소스도 관리 가능하도록 발전
- Hadoop 3.x부터 Kubernetes와의 연동도 일부 지원
- 미래에는 K8s 등 클라우드 네이티브 기술로 자원 관리 중심이 점차 이동할 수 있음

## 5. Hive - Hadoop 기반 SQL 엔진
### SQL on Hadoop의 시작
### 핵심 아키텍처 구성요소
### 실행 엔진의 진화
- 초기: MR 기반 실행 -> 그러나 처리 속도 느리고 실시간 분석에 부적합
- Tez 도입: DAG 기반 실행으로 속도 개선
- Hive on Spark: Spark 엔진 기반으로 처리 속도 및 병렬성 향상
- LLAP(Live Long and Process): 실행 엔진을 장시간 유지하여 응답 속도 개선
### Hive 4.x와 현대적 활용
• ACID 트랜잭션 지원 강화 (삽입/갱신/삭제 연산)
• HiveServer2 성능 개선 및 JDBC/ODBC 연동 강화
• **Iceberg, Hudi, Delta Lake**와 같은 최신 테이블 포맷과의 통합 지원
• Spark, Trino, Flink 등의 다양한 처리 엔진과 함께 사용되는 **레거시를 넘은 데이터 레이크 플랫폼**
• 여전히 **배치 처리 기반 대규모 분석**에 강점을 가지며, 점차 **데이터 웨어하우스와 레이크하우스의 중간지대 역할** 수행 중

## 6. Hive MetaStore(HMS)와 ==IceBerg==

### HMS의 메타데이터 저장소로서의 중요성과 역할
### 테이블/파티션/스키마 관리를 위한 HMS 아키텍처
### Iceberg와 HMS의 연계를 통한 현대적 데이터 레이크 구축
### Spark, Flink, Trino 등 다양한 엔진과의 통합 사례
### 클라우드 네이티브 환경에서의 HMS 활용 전략

### **🧠 “데이터는 어디 있고, 어떻게 생겼지?” → 그걸 기억하는 게 HMS!**

• Hive Metastore(HMS)는 **데이터의 주소록 + 설명서** 역할을 하는 메타데이터 저장소예요.
• 데이터를 직접 저장하진 않지만, **“이 테이블은 어디에 있고, 어떤 컬럼이 있고, 파티션은 몇 개야”** 같은 정보를 정확히 기억합니다.
• Hive뿐 아니라 **Spark, Flink, Trino, Presto** 같은 여러 엔진들이 이걸 참조해서 테이블을 읽거나 씁니다.

→ 즉, **빅데이터 시스템의 공용 네비게이션** 같은 존재!


### **🧱 구조는 어떻게 생겼을까? (간단히 3단계)**

1. **Catalog Layer**: 테이블, DB, 파티션 같은 개념적인 정보 관리
2. **Storage Layer**: 실제 메타데이터는 **MySQL/PostgreSQL** 같은 DB에 저장됨
3. **API Layer**: 외부 툴(Spark 등)이 Thrift API로 접속해서 메타데이터를 조회/수정함

다양한 분석 엔진들이 한 눈에 서로의 데이터를 볼 수 있게 만드는 **공용 언어**이자 **연결 통로**
  

### **🧊 Iceberg는 왜 갑자기 뜨는 걸까?**

• 넷플릭스가 만든 **현대식 오픈 테이블 포맷**!

• 기존 Hive 테이블을 관리하던 방식보다 훨씬 유연하고 강력함:
• **스키마 변경 OK**
• **데이터 시간여행 OK**
• **트랜잭션 처리도 OK**

→ 그리고 이 Iceberg도 **메타데이터는 HMS에 등록 가능**
→ 덕분에 기존 Hive 생태계를 **갈아엎지 않고** 현대화할 수 있음!


### **🔗 Spark, Flink, Trino 다 붙일 수 있어?**


Yes!

• **Spark**: Iceberg로 쿼리하면 ACID 트랜잭션, 버저닝, 증분 처리까지 가능
• **Flink**: 실시간 스트리밍 데이터를 Iceberg 테이블로 저장
• **Trino/Presto**: Hive 커넥터로 Iceberg 테이블 읽기 가능

→ 즉, HMS + Iceberg 조합 하나면 **모든 빅데이터 엔진이 하나의 메타데이터로 동기화됨**

  

### **☁️ 클라우드 환경에서도 잘 쓸 수 있을까?**

완전 가능!

• HMS는 **온프레미스 HDFS든, S3 같은 클라우드든 문제 없음**
• **Kubernetes 환경**에서도 컨테이너로 띄워서 독립 운영 가능
• Iceberg와 붙이면, Hudi/Delta와 달리 **Spark 전용에 묶이지 않고** 범용성이 높음

  

결국 “**HMS + Iceberg**”는
👉 전통 Hive에서 출발했지만
👉 클라우드 네이티브까지 커버하는 **데이터 레이크의 진화형 조합**




## 7. 요약 및 결론