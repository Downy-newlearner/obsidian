


이 자료는 단순한 “LLM 소개”가 아니라,

  

> **Language Model → Large Language Model → Transformer → Latent Space → Attention**

> 이라는 **개념적 확장 구조**를 이해시키는 데 목적이 있습니다.

---

## **1장: Language Model (언어 모델의 본질)**

  

### **핵심 내용 (2문단)**

  
Language Model은 본질적으로 **이전 토큰(단어들)이 주어졌을 때, 다음 토큰의 확률 분포를 예측하는 모델**이다. 즉, 언어를 “의미”가 아니라 **확률적 시퀀스**로 다루며, 말뭉치(corpus)를 통해 단어 간의 통계적 관계를 학습한다. 이 단계에서는 모델의 크기나 구조보다도 “언어를 확률로 모델링한다”는 관점이 핵심이다.

  

중요한 점은 Language Model이 **이해한다기보다, 조건부 확률을 계산한다**는 사실이다. 문장이 자연스럽게 이어지는 것처럼 보이지만, 실제 내부에서는 “어떤 단어가 다음에 올 가능성이 높은가”를 반복 계산할 뿐이다. 이 관점은 이후 LLM의 한계(환각, 그럴듯한 오답)를 이해하는 출발점이 된다.

---

### **핵심 질문**

- 언어 모델은 “의미를 이해”하는가, 아니면 “확률을 계산”하는가?
    
- Language Model의 출력이 자연스러워 보이는 이유는 무엇인가?
    
- Language Model과 검색 시스템의 본질적 차이는 무엇인가?
    

---

### **시험 문제(예시)**

- Language Model은 어떤 데이터를 학습하며, 무엇을 예측하는 모델인가?
    
- “언어 모델은 언어를 이해하지 않는다”는 말의 의미를 설명하라.
    

---

## **2장: Large Language Model (LLM)**

  

### **핵심 내용 (2문단)**

  

Large Language Model은 기존 Language Model을 **대규모 데이터(Large Corpus)**와 **대규모 파라미터(Large Parameters)**로 확장한 모델이다. 단순히 크기만 커진 것이 아니라, Transformer 구조를 기반으로 **장기 의존성(long-range dependency)**을 효과적으로 학습할 수 있게 되었다. 이로 인해 문맥 유지, 추론, 요약, 생성 능력이 비약적으로 향상되었다.

  

하지만 LLM 역시 본질은 여전히 **확률 기반 언어 모델**이다. “생각하는 것처럼 보이는” 현상은 **규모 확장(scale)**의 결과이며, 이는 In-context learning, Few-shot 학습과 같은 현상을 가능하게 하지만, 동시에 **환각(hallucination)**이라는 구조적 한계를 동반한다.

---

### **핵심 질문**

- Language Model과 LLM의 차이는 “질적 차이”인가, “규모의 차이”인가?
    
- LLM이 추론을 잘하는 것처럼 보이는 이유는 무엇인가?
    
- LLM의 한계는 구조적 한계인가, 데이터 한계인가?
    

---

### **시험 문제(예시)**

- Large Language Model이 기존 Language Model보다 뛰어난 이유를 설명하라.
    
- LLM에서 hallucination이 발생하는 구조적 이유를 설명하라.
    

---

## **3장: Latent Space (잠재 공간)**

  

### **핵심 내용 (2문단)**

  

Latent Space는 입력 데이터(텍스트, 이미지 등)를 **고정된 차원의 벡터 공간으로 투영한 추상적 표현 공간**이다. 예를 들어 384차원, 768차원이라는 것은 “실수 값 384개, 768개로 표현된 공간”을 의미하며, 사람이 직접 해석할 수 있는 차원이 아니다. 이 공간은 원본 데이터의 모든 정보를 보존하지 않고, **중요한 정보만 압축**한다.

  

이 과정에서 필연적으로 **정보 손실(lossy compression)**이 발생한다. 따라서 모델의 성능은 “얼마나 중요한 정보에 집중하여 latent space에 남겼는가”에 달려 있다. LLM에서의 기억 한계, 맥락 길이 제한, 요약 시 정보 누락 등의 현상은 모두 이 잠재 공간 개념과 연결된다.

---

### **핵심 질문**

- Latent Space란 무엇이며, 왜 고차원 공간이 필요한가?
    
- 차원이 커질수록 항상 좋은 표현이 되는가?
    
- Latent Space에서 정보 손실은 왜 필연적인가?
    

---

### **시험 문제(예시)**

- 384차원 latent space란 무엇을 의미하는가?
    
- Latent space에서 정보 손실이 발생하는 이유와 그 영향에 대해 설명하라.
    

---

## **4장: Attention & Transformer**

  

### **핵심 내용 (2문단)**

  

Attention 메커니즘은 입력 토큰들 사이의 관계를 **Query, Key, Value 연산을 통해 동적으로 계산**하는 방식이다. 모든 토큰을 동일하게 보지 않고, 현재 토큰이 어떤 다른 토큰에 더 집중해야 하는지를 수치적으로 결정한다. 이를 통해 긴 문장에서도 중요한 문맥을 유지할 수 있다.

  

Transformer는 이러한 Attention을 기반으로 한 구조로, Encoder와 Decoder로 구성된다. BERT 계열은 Encoder 중심, GPT 계열은 Decoder 중심 구조를 사용한다. 이 구조적 차이는 **임베딩 생성 vs 텍스트 생성**이라는 용도 차이로 이어지며, 이후 RAG에서 “어떤 모델을 어디에 쓰는가”라는 설계 판단의 근거가 된다.

---

### **핵심 질문**

- Attention은 기존 RNN 계열의 어떤 한계를 해결했는가?
    
- Encoder 기반 모델과 Decoder 기반 모델의 역할 차이는 무엇인가?
    
- 왜 GPT 계열 모델이 생성에 특화되어 있는가?
    

---

### **시험 문제(예시)**

- Attention 메커니즘의 핵심 아이디어를 설명하라.
    
- Encoder 기반 모델과 Decoder 기반 모델의 차이를 예를 들어 설명하라.
    

---

## **5장: LLM의 확장 요소와 최신 흐름 (개념 장)**

  

### **핵심 내용 (2문단)**

  

최근 LLM은 단순 Transformer를 넘어, **Multi-Head Attention, GQA, MoE(Mixture of Experts), RL 기반 학습** 등 다양한 기법으로 확장되고 있다. 이는 모델 크기를 무작정 키우는 대신, **효율성과 추론 안정성**을 확보하기 위한 방향이다.

  

특히 “Reasoning”, “Thought”, “Attention”과 같은 표현은 LLM 내부에 사람이 이해하는 사고 과정이 있다는 의미가 아니라, **구조적으로 그런 현상이 나타나도록 설계되었다는 뜻**이다. 이 점을 오해하면 LLM을 과대평가하거나 잘못된 시스템 설계를 하게 된다.

---

### **핵심 질문**

- LLM의 “추론 능력”은 어디에서 기인하는가?
    
- 최신 LLM 구조는 왜 효율성을 강조하는 방향으로 발전하는가?
    
- LLM을 ‘지능’으로 오해하면 어떤 문제가 발생하는가?
    

---

### **시험 문제(예시)**

- LLM의 reasoning 능력을 가능하게 하는 요소를 설명하라.
    
- LLM을 단독으로 사용할 때 발생하는 한계를 설명하라.
    

---

## **11번 자료를 관통하는 한 줄 핵심**

  

> **이 자료는 “LLM이 무엇을 할 수 있는가”보다**

> **“LLM이 왜 그럴듯하게 보이며, 왜 한계가 구조적으로 존재하는가”를 묻는다.**

---

