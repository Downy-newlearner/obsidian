
## 10
### 자신만의 방식으로 구체적인 예를 들어, 임베딩/벡터화에 대해서 설명하시오.

임베딩이란 사람이 사용하는 비정형 데이터(예: 단어, 문장, 문단 등)를 고정된 차원의 수치 벡터로 변환하는 과정이다.

예를 들어,
- "사과"
- "사과는 과일이다"
- "사과는 비타민이 풍부하고 건강에 좋은 과일이다"

라는 서로 길이가 다른 입력이 있을 때, 임베딩 모델은 이들을 각각 같은 차원의 벡터(예: 768차원 실수 벡터)로 변화한다. 이때 문단이 더 길다고 해서 벡터 차원이 더 커지지는 않으며, 모델은 입력 전체를 latent space로 압축하여 중요한 의미 정보만 남긴다.

이 결과, "사과"와 "바나나"는 벡터 공간에서 가깝고, "사과"와 "자동차"는 멀어지게 된다.
즉, 임베딩의 목적은 비교 불가능한 텍스트를 비교 가능한 수치 공간으로 사상하는 것이다.

### 다음 세 가지 “단어”, “문장“, “문단”은 같은 벡터 공간으로 임베딩 할 수 있나요? 다르게 말하면, “단어”, “문장“, “문단”을 같은 크기의 벡터로 변환할 수 있나요?

“단어”, “문장“, “문단”은 같은 벡터 공간으로 임베딩 할 수 있으며, 같은 크기의 벡터로 변환할 수 있다.

그 이유는 임베딩 모델이 입력의 길이나 구조가 아니라, 의미를 고정 차원 공간으로 투영하도록 설계되어 있기 때문이다.
임베딩 차원(예: 384, 768)은 입력 데이터의 크기를 의미하는 것이 아니라, 모델이 의미를 표현할 수 있는 표현 공간의 크기를 의미한다.

따라서 문단이 단어보다 "더 많은 정보를 담고 있다"고 해서 더 큰 벡터가 필요한 것은 아니며,
모델은 중요한 의미만을 선택적으로 압축하여 동일한 차원의 벡터로 표현한다.
이 점이 벡터 검색과 RAG가 가능한 핵심 전제 조건이다.


## 11
### Attention 이 무엇이며, 어떤 효과를 내는지 , NLP에서는 이걸 어떻게 적용하는지 설명해 보시오. 

Attention은 입력 데이터 내부에서 어떤 요소가 더 중요한지를 동적으로 계산하는 매커니즘이다.
즉, 모든 단어를 동일하게 보지 않고, 현재 처리 중인 토큰이 어떤 다른 토큰에 더 집중해야 하는지 가중치를 부여한다.

Attention은 Query, Key, Value 구조를 사용하여
- Query: 지금 보고 있는 정보
- Key: 참고 대상
- Value: 실제 전달할 정보
간의 유사도를 계산하고, 그 결과를 가중합으로 반영한다.

Attention의 핵심 효과는 다음과 같다.
1. 장거리 의존성 문제 해결
2. 문맥 기반 의미 해석 가능 - 같은 단어라도 문맥에 따라 다른 의미를 가질 때, Attention은 주변 단어들과의 관계를 통해 이를 구분할 수 있다.

NLP에서 Attention이 Transformer 구조의 핵심 구성 요소로 사용된다.
BERT 계열 모델은 Encoder 기반 Attention을 통해 문장 이해(임베딩)에 강하고,
GPT 계열 모델은 Decoder 기반 Attention을 통해 텍스트 생성에 특화되어 있다.

결과적으로 Attention은 단순한 성능 개선 기법이 아니라, LLM이 문맥을 이해하고 그럴듯한 답변을 생성할 수 있게 만드는 핵심 구조다.


## 12
![[Pasted image 20251217215122.png]]

1. Retrival 결과가 "사실 검증"을 보장하는 것처럼 표현됨.
	- 이미지에서는 RAG를 사용하면 정확한 분석이 가능한 것처럼 표현되어있다. 이는 잘못된 인상을 줄 우려가 있다. RAG는 사실 검증 시스템이 아니라, 검색된 문서를 컨텍스트로 제공하는 구조일 뿐이다. Retrival 결과가 부정확하거나 편향되어있다면, LLM은 여전히 그럴듯한 오답(환각)을 생성할 수 있다.
2. Retrival 문서의 신뢰성과 시점 문제가 고려되지 않음
	- 이미지의 예시는 OpenAI CEO 해임/복귀와 같은 시점, 해석이 민감한 사건을 다루고 있다.
	- 그러나 RAG는 "문서가 사실인지", "최신 정보인지", "해석인지 보도인지"를 스스로 판단하지 못한다. 즉, RAG를 사용해도 "올바른 해석"이 자동으로 보장되지는 않는다.
3. RAG의 역할이 과대평가됨
	- 실제로 RAG는 LLM을 덜 틀리게 만드는 구조적 보완책일 뿐이며, 품질은 Retrival 이전 단계(문서 선택, chunking, embedding)에 크게 의존한다.

결론적으로 이 이미지에서 잘못된 점은 "RAG가 정확한 답을 보장한다는 착각을 유도할 뿐이다."

## 13

### 각 기업에서 ChatGPT Project나 NotebookLM을 사용하면 내부 자료로 RAG 기술을 활용할 수 있나요? 활용할 수 있다면, 이유는 무엇인가요? 활용할 수 없다면, 이유는 무엇인가요?

제한적으로만 가능하다.

ChatGPT Project나 NotebookLM은 사용자가 제공한 문서를 컨텍스트로 활용하여 답변을 생성한다.
이는 기술적으로 In-Context Learning과 유사하다.

따라서, 개인 업무용 문서나 제한된 범위의 참고 자료에 대해서는 RAG처럼 활용할 수 있다.


그러나 기업용 RAG로는 한계가 있다.

1. 데이터 통제 문제
	- 기업 내부 전체 DB, 실시간 시스템과의 연동이 불가능
2. 파이프라인 제어 불가
	- Document parsing, chunking, embedding 전략을 기업이 직접 설계할 수 없음
3. 보안, 감사 한계
	- 어떤 문서가 어떤 답변에 사용되었는지 완전한 추적이 어렵다.

그러니 결국 기업 환경에서는 자체 벡터 DB + 자체 RAG 파이프라인이 필요하다.


## 14
### all-MiniLM-L6-v2 모델은 384차원 embedding model 이고, jhgan/ko-sroberta-multitask 모델은 768차원 embedding model 이다. 차원의 크기가 2배라는 것은 어떤 의미이며, 각각의 차원은 어떤 의미인가?

입력 텍스트를 표현하는 잠재 공간(latent space)의 해상도 차이를 말한다. 의미를 표현할 수 있는 축이 2배 더 많아져 더 정교한 의미 분리가 가능해진다는 의미이다. 즉, 문장 간 미묘한 차이나 한국어 특유의 문맥을 더 잘 보존할 수 있다.

결과적으로 768차원 임베딩은 더 많은 계산 자원과 메모리를 요구하지만, 특히 한국어 문장, 문단 단위의 의미 검색에서는 검색 정확도와 안정성이 높아지는 경향이 있다. 384차원 모델은 속도와 비용 면에서 유리하므로, 사용 목적과 환경에 따라 선택해야 한다.


![[Pasted image 20251218131124.png|500]]
![[Pasted image 20251218131132.png|500]]
### 앞선 상황에서, Limit 값은 4에서 10으로 변경했을 때, 동일한 질문에 대해서 더 상세한 설명이 답변으로 돌아왔다. 이유가 무엇인가?

Limit 값은 벡터 검색 단계에서 Retrival되는 문서 chunk의 개수를 의미한다. Limit을 4에서 10으로 늘렸다는 것은, 동일한 질문에 대해 더 많은 관련 문서 조각을 LLM의 입력 컨텍스트로 전달했다는 뜻이다. 그 결과 LLM은 더 풍부한 근거 정보를 바탕으로 답변을 생성할 수 있어, 설명이 길고 상세해진 것이다.


![[Pasted image 20251218131217.png|500]]
### ChatGPT Project 기반의 RAG가 n8n RAG에 비해 더 상세한 답변을 해주는 이유는 무엇인가?

ChatGPT Project 기반 RAG가 n8n RAG보다 더 상세한 답변을 제공하는 가장 큰 이유는 RAG가 LLM의 핵심 사고(reasoning) 루프에 더 깊이 통합되어 있기 때문이다. ChatGPT Project에서는 Retrival된 문서들이 단순 참고 자료가 아니라, LLM이 답변을 생성할 때 기본 전제로 삼는 컨텍스트로 작동한다. 반면 n8n RAG는 종종 Retrival을 하나의 Tool 호출로 취급하여, LLM의 주된 추론 흐름과 분리되는 경우가 많다.

ChatGPT Project는 RAG 프로세스가 플랫폼 내부적으로 최적화되어있다. 단순히 "많은 문서를 가져온다"가 아니라, 질문에 맞게 구조화된 설명 형태로 재조합하여 LLM에 전달한다. 이로 인해 같은 문서라도 n8n보다 더 일관되고 맥락적인 답변이 가능하다.