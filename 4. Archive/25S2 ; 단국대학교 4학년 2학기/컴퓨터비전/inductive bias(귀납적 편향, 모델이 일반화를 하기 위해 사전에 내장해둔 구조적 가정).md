“Inductive bias”라는 표현은 처음 보면 **‘귀납적으로 생긴 편향’**처럼 느껴지지만, **정확한 뜻은 전혀 다르다.**

오히려 정반대에 가깝다. 아래에서 정확하게 설명해줄게.

---

# **Inductive Bias = 모델이 학습하기 전에 ‘가지고 있는 가정(assumptions)’**

즉,

**모델이 데이터를 어떻게 해석해야 하는지 미리 알고 있는 규칙/제약**이다
- “bias” = 잘못된 편향이 아니라, **모델 설계에 포함된 ‘선험적 가정(경험 또는 실험 이전에 성립한다고 가정되는 것)’**
- “inductive” = 데이터로부터 일반화를 하기 위해 필요한 가정

### **⚡ 한 문장 정의:**

> **Inductive bias = 모델이 일반화(generalization)를 하기 위해 사전에 내장해둔 구조적 가정**

---

# **🔍 왜 이런 가정이 필요할까?**

  

기계학습에서는 **입력 데이터만 보고 완전한 일반화는 절대 불가능** 하다.

그래서 모델은 반드시 _어떤 형태로든_ **추론을 돕는 가정**을 내장해야 한다.

  

예시:

- **선형 회귀** → 세상은 선형적으로 설명될 수 있다
- **CNN** → 가까운 픽셀끼리 의미가 있다(locality), 패턴은 위치가 바뀌어도 동일하다(translation equivariance)
- **Transformer** → 특별한 구조적 가정 대신, attention으로 관계를 ‘데이터에서’ 학습한다

---

# 🧠 Inductive Bias는 “선입견”이라기보다 지능적 제약이다

  

학습을 효율적으로 하기 위한 _유리한 시작점_ 같은 것.

  

예를 들어 **CNN의 inductive bias**는 이미지에 딱 맞는 구조적 가정이다:

  

### **✔ Locality**
→ 가까운 픽셀끼리 의미가 있다


  ### **✔ Translation equivariance**
→ 고양이 귀는 이미지 어디에 있어도 같은 객체

  

이런 가정 덕분에 CNN은 **적은 데이터로도 잘 학습**한다.

---

# **🔄 Transformer가 “weak inductive bias”라고 불리는 이유**

  

Transformer는 CNN처럼 구조적으로 강한 가정을 넣어두지 않는다.

대신:

- 모든 토큰 관계를 attention으로 학습해야 하고
- 이미지 구조에 맞는 힌트를 거의 주지 않기 때문에


  

**데이터가 많아야** “이미지란 원래 이런 구조구나”를 스스로 파악할 수 있다.

  

그래서 ViT가 처음 등장했을 때도

→ 논문에서 ImageNet-21k 같은 **초대형 데이터셋을 요구**한 이유가 여기에 있다.

---

# **⭐ 핵심 요약**

| **용어**                      | **의미**                                             |
| --------------------------- | -------------------------------------------------- |
| **Inductive bias**          | 모델이 학습 전에 이미 갖고 있는 ‘가정’, ‘구조적 제약’, ‘일반화 전략’        |
| CNN의 inductive bias         | locality, translation equivariance 등 이미지 특성에 맞춘 구조 |
| Transformer의 inductive bias | 상대적으로 약함 → 데이터로 관계를 직접 배우는 구조                      |
| 왜 중요?                       | bias가 강하면 적은 데이터로 잘 학습, 약하면 데이터를 많이 요구             |

---

# **📌 오해 정리**

“귀납적으로 생긴 편향”이 아니라

**귀납 추론(inductive inference)을 가능하게 하는 ‘사전 가정’**이 inductive bias다.

