
seq2seq와 attention의 차이
-> attention 기법과 다르게 seq2seq는 전체 입력 정보를 요약한 하나의 고정된 context vector를 한 번만 사용한다.


predict를 하나 할 때마다 context vector를 꼭 사용한다.