
아래는 **Week 6 ~ Week 14 전체 강의 자료**를 기준으로,

**각 주차(자료)마다 최상 난이도 서술형 문제 3문제씩** 출제한 것입니다.

요구하신 대로 **정답·해설은 제공하지 않으며**, 모든 문제는 **핵심 개념 + 수식/구조/설계 논리까지 설명을 요구**하도록 설계했습니다.

---

## **Week 6 — MobileNet & EfficientNet (Lightweight CNN)**

  

  

### **문제 6-1**

  

Depthwise Separable Convolution이 Standard Convolution 대비 연산량을 줄이는 원리를 **수식 수준에서 유도**하라.

이때 입력 채널 수 C, 출력 채널 수 K, 커널 크기 D_k \times D_k를 명시적으로 사용하여 **연산량 감소 비율**을 도출하고, 이 구조가 항상 성능 이득으로 이어지지 않는 이유를 설명하라.

  

### **문제 6-2**

  

MBConv가 “**Mobile Inverted Bottleneck**”이라 불리는 이유를,

(1) 전통적 bottleneck 구조와의 대비

(2) expansion–depthwise–projection 흐름

(3) residual connection이 적용되는 조건

의 세 관점에서 논증하라.

  

### **문제 6-3**

  

EfficientNet의 Compound Scaling에서

\alpha \cdot \beta^2 \cdot \gamma^2 \approx 2

라는 제약식이 등장하는 이유를 **FLOPs와 메모리 관점에서 유도**하고,

임의로 depth·width·resolution 중 하나만 키울 경우 발생하는 비효율을 구체적인 예로 설명하라.

---

## **Week 7 — FCN & U-Net (Segmentation)**

  

  

### **문제 7-1**

  

Classification CNN이 Semantic Segmentation에 직접 적용될 수 없는 이유를

**Translation Invariance / Equivariance 개념**과

**Pooling·FC layer가 공간 정보를 어떻게 파괴하는지**를 중심으로 구조적으로 설명하라.

  

### **문제 7-2**

  

FCN에서 Fully Connected layer를 Convolution layer로 변환(convolutionalization)할 수 있는 이유를

**파라미터 수 보존**, **수용 영역(receptive field)**, **입력 크기 자유도** 관점에서 수식과 함께 설명하라.

  

### **문제 7-3**

  

U-Net의 Skip Connection이 단순한 정보 전달이 아니라는 점을

(1) localization–context trade-off

(2) valid padding으로 인한 crop의 필연성

(3) boundary precision

의 관점에서 분석하라.

---

## **Week 9 — Attention & Encoder–Decoder**

  

  

### **문제 9-1**

  

Seq2Seq 모델에서 **고정 길이 Context Vector**가 본질적 병목이 되는 이유를

정보 이론적 관점 또는 표현력 관점에서 설명하고,

Attention이 이 병목을 어떻게 구조적으로 해소하는지 서술하라.

  

### **문제 9-2**

  

Attention을

\text{Attention}(Q,K,V)=\sum_i \alpha_i V_i

로 정의할 때,

Query·Key·Value가 **같은 벡터일 경우 발생하는 문제점**을 설명하고

왜 역할 분리가 필수적인지 논증하라.

  

### **문제 9-3**

  

Channel / Spatial / Temporal / Branch Attention을

**“무엇·어디·언제·어떤 경로”**라는 질문으로 재해석하고,

각 attention 유형이 실패했을 때 발생하는 대표적인 성능 저하 사례를 제시하라.

---

## **Week 10–11 — Transformer & Self-Attention**

  

  

### **문제 10-1**

  

Scaled Dot-Product Attention에서

\frac{QK^\top}{\sqrt{d_k}}

로 scaling을 수행하지 않을 경우 발생하는 문제를

**분산(variance) 분석을 통해 수식적으로 증명**하라.

  

### **문제 10-2**

  

Multi-Head Attention이 단일 Head Attention보다 표현력이 높은 이유를

(1) subspace 분리

(2) 정보 병렬성

(3) output projection의 역할

관점에서 설명하라.

  

### **문제 10-3**

  

Encoder–Decoder Attention과 Decoder Masked Self-Attention의 차이를

**Query·Key·Value의 출처**, **정보 흐름 방향**, **auto-regressive 제약** 관점에서 비교 분석하라.

---

## **Week 12–14 — Vision Transformer (ViT)**

  

  

### **문제 12-1**

  

ViT가 CNN의 locality·translation equivariance라는 inductive bias를 제거함으로써

얻는 이점과 잃는 것을 **데이터 효율성·일반화·스케일링 관점**에서 비교하라.

  

### **문제 12-2**

  

Patch Embedding 과정에서

Flatten → Linear Projection을 사용하는 이유를

CNN feature map과 비교하여 설명하고,

이 선택이 self-attention의 성질에 어떤 영향을 주는지 분석하라.

  

### **문제 12-3**

  

[CLS] 토큰을 사용하는 설계가

(1) Global Average Pooling

(2) 모든 patch embedding 평균

보다 유리한 이유를 **self-attention의 동작 방식**과 연계하여 설명하라.

---

### **정리**

- 총 문제 수: **15문제**
    
- 전부 **최상 난이도 서술형**
    
- 단순 정의 암기 불가, **구조·수식·설계 철학 설명 필수**
    
- **기말/구술/대학원 수준 대비용**
    

  

원하시면

- “**각 문제 채점 기준(루브릭)**”
    
- “**교수님 스타일 모범답안 구조만 제시**(정답 없이 뼈대만)”
    
- “**이 중 시험에 나올 확률 높은 문제 표시**”
    
    도 추가로 제공할 수 있습니다.