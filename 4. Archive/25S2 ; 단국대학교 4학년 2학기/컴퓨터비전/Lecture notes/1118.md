Lecture materials: week7, week9
## Key Training Strategies for High-Quality Segmentation

### Strategy 3. Weighted Loss for Boundary Separation


## FCN & U-Net: Foundations of Modern Segmentation
### 1. FCN(Fully Convolutional Network)
- Key Idea 
- Upsampling
- Skip Architecture
- Impact

### 2. U-Net
- Architecture
	- Symmetricc U-Shape with 
		- Contracting Path
		- Expansive path(with Skip connection)
- Skip Connection
- Specialized Training Strategies
- Impact


### 3. Key Insight

---
# Attention Mechanism

## Focusing Self-Attention & Transformers
모델은 모든 파트를 동일하게 다루는게 아니라, 특정 지역에 집중(attention)한다.


## Why "focusing" matters in computer vision models

### 3. Improves performance in tasks requiring global context
scene understanding, semantic segmentation, object detection

### Key Takeaway for Computer Vision
1. CNN -> fixed & local
2. Attention -> adaptive & global
3. Attention allows the model to 
	- Capture long-range dependencies in a single step


# Seq2Seq

## How the Encoder Works

### Step 1. Tokenization


## Step 2. Vectorization
각 토큰을 seq2seq 모델이 이해할 수 있는 numeric vector로 변환한다.

### Step 3. Context Vector Generation
목표: 전체 입력 시퀀스를 single fixed-length 벡터(Context Vector = Final hidden state)로 요약하기

한계
- 정보 손실
- bottleneck


## How the Decoder Works
디코더는 context vector와 hidden state를 인코더로부터 받아, step-to-step으로 output sequence를 생성한다.