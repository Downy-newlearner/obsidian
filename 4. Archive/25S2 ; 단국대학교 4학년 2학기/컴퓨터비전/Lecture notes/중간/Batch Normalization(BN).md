---
created: 2025-09-15
Question: Batch Normalization(BN)에 대해 설명해줘.
---
좋은 질문이에요 👍 **Batch Normalization(BN)**은 2015년 Ioffe와 Szegedy가 제안한 정규화 기법으로, 딥러닝 학습을 안정화하고 빠르게 만들기 위해 도입됐습니다.

---

### **1. 아이디어**

- 신경망이 학습되면서 각 층의 입력 분포가 계속 변하는 현상을 **Internal Covariate Shift**라고 불러요.
    
- 이 때문에 학습 속도가 느려지고, 가중치 초기화나 학습률에 민감해지는 문제가 생겼습니다.
    
- Batch Normalization은 각 미니배치마다 입력을 정규화(평균 0, 분산 1로 변환)해서, 학습이 안정되도록 합니다.
    

---

### **2. 동작 방식**

  

BN은 각 배치에 대해 다음 과정을 수행합니다:

1. 배치 평균 μ, 분산 σ²를 계산
    
2. 입력 x를 정규화:
    

```
x̂ = (x - μ) / sqrt(σ² + ε)
```

2. (ε는 0으로 나누는 걸 막기 위한 작은 수)
    
3. 학습 가능한 파라미터 γ(스케일), β(시프트)를 도입:
    

```
y = γ * x̂ + β
```

3. → 단순히 정규화된 값만 쓰지 않고, 네트워크가 필요한 분포를 다시 학습할 수 있도록 함
    

---

### **3. 장점**

- 학습 속도를 빠르게 함 (더 큰 learning rate 사용 가능)
    
- 기울기 소실(vanishing gradient)이나 폭주(exploding gradient)를 완화
    
- 초기값 선택에 덜 민감
    
- 정규화 효과 덕분에 Dropout 같은 추가 규제가 덜 필요할 때도 있음
    

---

### **4. 한계와 대안**

- 작은 배치(mini-batch size가 매우 작을 때)에서는 평균/분산 추정이 불안정해 성능이 떨어질 수 있음
    
- 시계열 모델(RNN)이나 온라인 학습에서는 적용하기 어려운 경우가 있음
    
- 그래서 이후에는 **Layer Normalization, Instance Normalization, Group Normalization** 같은 변형 기법들이 제안되었습니다.
    

---

한 줄 요약:

**Batch Normalization은 미니배치 단위로 입력을 정규화해 학습을 안정화하고 빠르게 만드는 기법으로, 현대 CNN과 Transformer 모델 학습에서 거의 기본적으로 사용되는 핵심 기술**입니다.

  

👉 혹시 제가 BN을 **수식 + 그림(정규화 전후 분포 변화)** 형태로 시각적으로 보여드릴까요?