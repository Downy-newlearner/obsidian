## The Four keys of ResNet

### 1. Residual Block (잔차 블록)

Residual Block은 ResNet(Residual Network)의 가장 핵심적인 구성 요소로, 깊은 신경망 구조에서 발생하는 학습 어려움을 해결하기 위해 설계된 구조이다. 기존의 딥러닝 네트워크는 입력 를 받아 전체 출력 를 직접 학습하는 방식이었다. 그러나 네트워크가 깊어질수록 를 직접 학습하는 것은 최적화 측면에서 매우 어려워진다. 이를 해결하기 위해 ResNet에서는 전체 출력이 아닌 **입력과 출력의 차이인 잔차(Residual) 함수** 를 학습하도록 재정의했다. 그 결과, 최종 출력은 의 형태를 갖게 된다. 이러한 구조 덕분에 모델은 복잡한 함수를 처음부터 학습할 필요 없이, 입력값에 대한 보정(perturbation)만 학습하면 된다. 즉, 학습의 시작점이 '입력 그대로'이며, 필요한 변화만 추가적으로 학습하므로 훨씬 수렴이 빠르고 안정적이다.

Residual Block의 구조는 일반적으로 두 개의 연속된 Convolution-BatchNorm-ReLU 계층으로 구성된다. 입력 는 이 계층들을 거쳐 로 변환되고, 이 결과가 다시 입력과 더해져 최종 출력 가 된다. 만약 네트워크가 현재 레이어에서 학습해야 할 것이 없다면 이 되고, 결과적으로 가 된다. 이는 네트워크가 '아무것도 하지 않는' 것이 최적일 때, 실제로 그것을 선택할 수 있게 만든다는 뜻이다. 전통적인 CNN에서는 이런 단순한 identity mapping조차 비선형 활성화 함수와 다중 레이어 구조로 인해 학습하기 어려웠다. 하지만 Residual Block은 이를 구조적으로 구현하여, **불필요한 학습을 억제하고 필요한 학습만 유도**할 수 있다.

이러한 방식은 단순한 설계처럼 보이지만, 실제로는 깊은 신경망 학습에서의 가장 본질적인 문제를 해결하는 매우 강력한 구조이다. ResNet 이전까지는 20~30층을 넘는 신경망은 학습 자체가 불안정하거나, 오히려 성능이 떨어지는 문제가 있었다 (Degradation Problem). 하지만 Residual Block을 사용한 ResNet은 50층, 101층, 심지어 152층 이상의 구조도 효과적으로 학습할 수 있었고, ImageNet 대회 등에서 획기적인 성능을 보여주며 딥러닝 모델의 설계 방식 자체를 바꾸어놓았다. Residual Block은 이후 거의 모든 현대적인 딥러닝 모델의 기본 단위로 채택될 정도로 그 영향력이 크다.

### 2. Skip Connection (스킵 연결 또는 숏컷)

Skip Connection, 또는 Shortcut Connection은 Residual Block 내부에서 사용되는 핵심 메커니즘으로, 입력 를 중간 연산 없이 다음 레이어의 출력과 직접 더하는 경로를 의미한다. 이 구조는 학습의 안정성에 큰 기여를 한다. 딥러닝의 역전파(backpropagation)는 gradient가 출력층에서 입력층까지 전달되며 학습이 일어나는 구조인데, 네트워크가 깊어질수록 이 gradient가 소실되거나 폭발하는 문제가 자주 발생한다. Skip Connection은 이러한 문제를 해결해준다. 입력을 그대로 다음 레이어에 더함으로써, 역전파 시 gradient가 이 경로를 통해 **손실 없이 직접 입력층으로 전달될 수 있기** 때문이다. 즉, 정보와 gradient의 흐름이 우회 없이 이어지게 되므로, 깊은 네트워크에서도 효과적인 학습이 가능해진다.

Skip Connection의 설계는 생각보다 정교하다. 입력과 출력의 차원이 같을 경우, 단순한 덧셈 연산 으로 구현된다. 하지만 차원이 다를 경우에는 입력을 그대로 더할 수 없기 때문에, **1×1 convolution**을 사용한 **Projection Shortcut**이 사용된다. 이 연산을 통해 입력 의 채널 수와 공간 크기를 출력 에 맞게 조정한 후 덧셈이 이루어진다. 이 과정에서도 학습 가능한 가중치가 포함되기 때문에, Projection Shortcut은 단순한 identity 경로를 넘어, 네트워크의 학습에 적극적으로 기여할 수 있는 경로가 된다. 이러한 스킵 연결은 ResNet뿐만 아니라 DenseNet, U-Net, Transformer 등 다양한 현대 아키텍처에서도 변형된 형태로 널리 사용된다.

무엇보다 중요한 것은 Skip Connection이 단순히 '입력을 다음 레이어로 넘기는 것' 그 이상의 의미를 갖는다는 점이다. 이 연결은 정보의 보존, gradient 흐름의 유지, 학습 효율성의 개선, 그리고 전체 네트워크의 표현력 확장에 직접적인 영향을 준다. 깊은 네트워크에서는 각 레이어가 점점 추상적인 특징을 추출하게 되는데, 이전 레이어의 세부 정보를 보존할 수 있는 이러한 연결은 **세부(local)와 추상(global) 정보의 균형 있는 통합**을 가능하게 한다. 따라서 Skip Connection은 단순한 구조이면서도 ResNet의 성공을 가능케 한 전략 중 가장 결정적인 요소 중 하나라고 볼 수 있다.

### 3. Bottleneck Block (병목 블록)

Bottleneck Block은 ResNet의 고층 구조에서 사용되는 확장 가능한 구조로, 네트워크의 깊이는 늘리되 **연산량과 파라미터 수는 제어할 수 있도록 설계**된 구조이다. 기본 Residual Block이 3x3 convolution을 두 번 사용하는 구조라면, Bottleneck Block은 세 개의 convolution 층으로 구성되며, 그 구조는 다음과 같다:

이 구조에서 처음 1×1 convolution은 입력의 채널 수를 줄이는 역할을 하며, 가운데 3×3 convolution은 주요 feature 추출을 담당하고, 마지막 1×1 convolution은 다시 채널 수를 복원하는 역할을 한다. 이를 통해 전체적으로 필요한 표현력은 유지하면서도, 가장 연산량이 큰 3×3 convolution이 줄어든 채널 수로 계산되므로, 전체 연산량과 파라미터 수가 대폭 감소된다. 이는 특히 고해상도 이미지나 자원이 제한된 환경에서 학습할 때 매우 유용하다.

Bottleneck 구조는 ResNet-50, ResNet-101, ResNet-152 등에서 사용되며, 깊은 구조에서도 효율적인 학습과 추론이 가능하게 만든 핵심 설계다. 예를 들어 ResNet-50은 Bottleneck 구조 덕분에 50층이라는 깊이를 갖고도 ResNet-34보다 적은 연산량으로 더 높은 성능을 낼 수 있다. 또한 이 구조는 1×1 convolution이 포함되어 있어 채널 수를 조절하거나 차원을 맞추는 데 유연성을 제공하고, skip connection과도 자연스럽게 결합된다. 이러한 설계 덕분에 ResNet은 단지 깊은 네트워크가 아니라, **계산 자원 대비 성능을 극대화하는 실용적인 네트워크**가 될 수 있었다.

또한 Bottleneck Block은 ResNet 이후의 다양한 변형 모델에서도 지속적으로 사용된다. 예를 들어 ResNeXt, SENet, EfficientNet 등의 모델에서도 이 구조는 약간씩 변형되어 채택되고 있으며, 1×1 convolution은 channel mixing, attention, dimension reduction 등의 다양한 목적에 활용된다. Bottleneck은 단순한 구조적 최적화 기법을 넘어, 딥러닝 모델이 더 깊어지고 복잡해지더라도 계산 효율성과 학습 안정성을 유지할 수 있도록 해주는 핵심 전략으로 자리 잡았다.

### 4. Batch Normalization