## LeNet-5 소개
- LeNet-5는 1998년 Yann LeCun이 발표한 초기의 대표적인 CNN 모델이다.
- MNIST 이미지로부터 사람이 직접 특징을 설계하지 않고 네트워크가 스스로 특징을 학습하도록 end-to-end 접근을 제시했다.
- 다시 말해, 입력(이미지) -> 출력(숫자 레이블)까지 전체 네트워크를 backpropagation으로 end-to-end 학습한 최초의 CNN 구조이다.
- 즉, 특징 추출(feature extraction)과 분류(classification) 단계를 분리하지 않고 하나의 모델 안에서 통합 학습한 첫 성공 사례이다.

- 입력층은 32x32 grayscale image를 받는다.(1 channel)
- 첫 번째 계층 ==C1==은 6개의 5x5 필터로 합성곱을 수행해 지역적인 저수준 특징(모서리, 간단한 패턴 등)을 뽑아낸다.
	- padding 없음
	- stride = 1
	- tanh 활성화 함수 사용
- 이어지는 ==S2==는 2x2 Average pooling(subsampling)으로 공간 크기를 절반으로 줄여서 특징 맵을 압축하면서도 위치 변화에 대해 조금 더 안정적인 표현을 만들었다.
- 두 번째 합성곱 계층 ==C3==는 16개의 필터를 사용했지만, 모든 입력 맵에 완전 연결되지 않고 일부 맵 조합만 연결되도록 설계되어 파라미터 수를 줄이면서 다양한 조합의 특징을 학습할 수 있게 했다. 이를 Grouped connection이라고 한다.
- ==S4== 풀링 계층은 다시 Average pooling을 통해 차원을 축소해 translation invariance를 강화했다.
- 그 다음 ==C5==는 사실상 fully connected에 가까운 합성곱 계층으로, 120개의 feature를 출력하여 앞선 계층에서 추출된 지역 특징들을 종합적으로 통합하는 역할을 했다.
- 이후 ==F6==는 84개의 뉴런으로 이루어진 전결합 계층으로 120차원의 입력을 받아 더 압축된 표현을 만들어 분류기로 넘어가기 전에 중간 표현 차원을 줄이고 일반화 성능을 높이도록 설계됐다.
- 마지막으로 출력층은 10개의 뉴런을 가지며 숫자 0부터 9까지의 클래스를 RBF(Radial Basis Function)로 구분하는 분류기를 담당했다.
	- Radial은 반지름(radius)에서 나온 말로, 중심점에서 얼마나 떨어져 있는가를 의미한다.
	- 즉, 입력 벡터가 중심점에서 멀어질수록 함수값이 달라지는 구조이다.
	- Basis Function은 여러 함수를 조합해서 전체 함수를 구성할 때, 기초가 되는 함수(기저함수)를 뜻한다.

- 합성곱이 특징 추출
- 풀링이 차원 축소와 불변성 강화를
- Fully connected layer가 통합과 분류를 맡아 각각의 레이어가 유기적으로 협력하여 손글씨 숫자 인식이라는 목표를 달성하도록 설계되었다.


## LeNet-5의 역사적 중요성

#### 1. 현실 세계에 적용
- LeNet-5는 원래 숫자 인식 작업을 위해 개발됨
- 우편번호나 계좌번호 인식 등

#### 2. 특징 추출을 자동으로 함
- 'trainable', 'end-to-end pipeline'을 통해 엣지, 모양, 질감 표현을 raw 픽셀 데이터로부터 학습

#### 3. 효율적인 디자인 철학
- LeNet-5는 아래 기술들을 통해 파라미터 수를 획기적으로 감소시킴
	- Shared weights
	- Local receptive fields
	- Subsampling(average pooling)

#### 4. 일반화를 위한 디자인
- LeNet-5는 모델 복잡도를 낮게 유지하는 것이 데이터가 제한된 경우 중요하다고 말함.
- ![[Pasted image 20251026200815.png|200]]
	- h는 모델 복잡도, P는 데이터의 양으로, 모델을 복잡하지 않게, 데이터의 양은 충분하게 하는 것이 일반화 성능을 높일 수 있다고 함.

- LeNet-5는 h를 작게 유지하기 위해 세밀하게 설계되었는데 아래 기술을 사용함.
	- Shared weights
	- Local connections
	- Subsampling

## 딥러닝에서 Convolution이란?
### Convolution layer 소개
- convolution layer는 아래 이유들 때문에 사용된다.
	1. 입력 이미지에서 ==지역 특성(local feature)==를 추출
	2. 작은 필터(filter, kernel)를 입력 이미지 위에서 슬라이드함
	3. 각 위치에서, 필터와 겹치는 값들을 곱하고 더함
	4. ==패턴을 강조하는 feature map을 생성함==

- **필터는 Training 동안 학습된다.**

### Convolution layer에 대한 Output 사이즈 계산하는 법
![[Pasted image 20251026202945.png|300]]
- $W_{in}, H_{in}$: Input width and height
- $K$: 커널 사이즈(예: 5x5 -> $K=5$,  3x3 -> $K = 3$)
- $P$: 패딩
- $S$: Stride

- 대괄호 안의 식은 정수로 나오지 않으면 소수점 아래는 버림한다.

### Stride란?
- 아는 내용이니까 메모 생략
- Stride가 1일 때가 1보다 클 때보다 좋은 Locality(지역성, 입력 이미지들의 인접한 픽셀들 간의 정보가 유사하다는 특성)을 갖는다.
	- 왜냐하면 Stride가 클 수록 중간 픽셀들을 건너뛰면서 인접 픽셀 간의 연속성(local continuity)이 끊기게 되기 때문이다.
	- 따라서 출력 feature map은 입력보다 공간 해상도가 크게 감소하고, 세밀한 변화나 작은 물체 정보가 사라질 수 있다.

### Padding이란?
![[Pasted image 20251026204435.png|400]]
- 입력 이미지 주위에 extra pixels(일반적으로 zero 값을 가짐)을 추가하는 것이다.
- ==입력 이미지 사이즈와 같은 출력 피처맵을 갖기 위해서==나 ==테두리 부분의 정보 손실을 막기 위해 사용한다.==


## Convolution 자체의 의미
- 본래, Convolution은 "두 함수를 결합해 세 번째 함수를 만드는 연산"이다.
- 하나의 함수가 다른 함수에 어떻게 반응하거나 수정되는지를 보여준다.
- 즉, 한 함수를 다른 함수 위로 이동시키면서 두 함수의 유사도를 측정하는 과정이다.
- 겹치는 정도가 클 수록 출력값이 커지고, 유사하지 않을수록 작아진다.
- 이것이 CNN에서의 "Feature detection"의 수학적 기초이다.

### Step 1. Flip one function
- 함수 하나(보통 g)를 좌우 반전시킨다.
- 이렇게 해야 두 함수가 같은 방향으로 슬라이드되며 올바른 정렬 상태에서 비교할 수 있다.

### Step2. Shift the flipped function
- 그 다음 뒤집힌 g를 시간만큼 이동시킨다.
- 슬라이드를 하며 유사도를 계산할 수 있다.
- 즉 $t=0, 1, 2$,일 때마다 필터가 입력의 다른 부분을 탐색한다.

### Step3. Multiply and Accumulate
- 각 위치에서 두 함수의 곱을 계산하고, 모든 $\tau$에 대해 적분(또는 합산)한다.
- 이 값은 유사도를 의미하며, 이 값이 크면 해당 위치에 특정 패턴이 존재한다는 뜻이다.


- 즉 convolution은 단순 곱셈이 아니라 "패턴 매칭"연산이다.

### 왜 CNN에서는 연속형 대신 이산형을 사용하는가?
1. 입력 이미지가 이산적이다.
2. 계산 효율을 위함이다.
3. 필터가 학습된다.
	- 전통 신호처리에서는 g(t)가 사람이 만든 고정 필터였지만, CNN에서는 학습 가능한 파라미터이다.
	- 따라서 flip이 불필요하고, 실제 구현에서는 cross-correlation 연산이 쓰인다.


## Convolution Layers: C1, C3, C5
- 합성곱 계층은 지역 특징을 추출하고, 공유 가중치(Shared weights)를 이용해 파라미터 수를 줄인다.
- 각 필터는 입력의 작은 영역(local receptive field)만을 바라보며, 동일한 필터가 입력 전체를 슬라이드하면서 적용된다.

### C1 Layer
- 입력: 32x32x1 grayscale 이미지
- 필터: 6개, 크기 5x5
- stride = 1, padding = 0
- 출력: 6x28x28 feature map
- 활성화 함수: tanh
- C1은 입력 이미지의 가장 기초적인 저수준 특징(선, 단순 패턴)을 학습한다.
- 각 필터는 서로 다른 방향과 형태의 패턴에 반응하도록 학습된다.

### C3 Layer
- 입력: S2의 출력 (6x14x14)
- 필터: 16개, 크기 5x5
- stride = 1
- C3는 S2의 feature map 일부 조합에만 연결되는 grouped connection 구조를 가진다.
	- 이는 파라미터를 줄이고, feature diversity(특징 다양성)를 높이기 위한 설계이다.
	- 각 출력 맵은 서로 다른 입력 조합을 학습해, 더 복잡한 패턴(곡선,  교차선 등)을 인식한다.
- 출력: 16x10x10 feature map

- Grouped connection
	- 그룹1: 3개의 연속적인 입력 맵 - 6개 출력
	- 그룹2: 4개의 연속적인 입력 맵 - 6개 출력
	- 그룹3: 4개의 불연속적인 입력 맵 - 3개 출력
	- 그룹4: 6개 입력 맵 전부 - 1개 출력

- 파라미터 계산(Grouped connection 고려됨)
	- 그룹1: $(5*5*3+1)*6 = 456$
	- 그룹2: $(5*5*4+1)*6 = 606$
	- 그룹3: $(5*5*4+1)*3 = 303$
	- 그룹4: $(5*5*6+1)*1 = 151$
	- Total trainable parameters: ==1516==

### C5 Layer
- 입력: S4의 출력 (16x5x5)
- 필터: 120개, 크기 5x5
- stride = 1
- C5는 입력 크기(5x5)와 필터 크기(5x5)가 같으므로, 사실상 fully connected와 유사한 합성곱이다.
- 앞선 계층에서 추출된 지역 특징들을 종합하여 전역적(global) 특징으로 통합한다.
- 출력: 120차원의 feature vector(120 feature maps of size 1x1)

- 파라미터 계산:
	- 입력으로 5x5 피처 맵이 16개 들어온다.
	- 출력으로 하는 feature vector의 120의 원소는 각각 16개의 5x5 피처맵의 값을 모두 반영한다.(bias도 1개 있음)
	- 그러므로 각 feature vector의 원소를 만들기 위해 필요한 파라미터는 아래와 같다.
	$$5*5*16+1 = 401$$
	- 120개의 feature vector 원소가 출력이므로 $401 * 120 = 48,120$이다.
	


## Pooling Layers: S2, S4
- Pooling(Subsampling)은 공간 크기를 줄이고, 작은 위치 변화(translation)에 대해 강건한 표현을 만든다.
- LeNet-5에서는 Average Pooling을 사용했다.
- 이 단계에서 연속적인 값 대신 대표값(평균)을 취함으로써 데이터의 노이즈를 줄이고, 계산량을 감소시킨다.

### S2 Layer
- 입력: C1의 출력 (6x28x28)
- Pooling window: 2x2, stride = 2
- 출력: 6x14x14
- 각 2x2 영역의 평균을 취해 공간 크기를 절반으로 줄인다.
- 이 단계에서 얻는 효과는 차원 축소와 translation invariance의 강화이다.

- 파라미터 개수:
	- $(1+1)*6 = 12$

### S4 Layer
- 입력: C3의 출력(16x10x10)
- Pooling window: 2x2, stride = 2
- 출력: 16x5x5
- S2와 같은 방식으로 연산을 수행하여 feature map의 크기를 다시 절반으로 줄인다.
- 이 단계까지 진행하면 feature map은 이미지의 전체 구조를 더 압축적으로 표현한다.

- 파라미터 개수:
	- $(1+1)*16 = 32$

### S2, S4 Layer에서 파라미터가 필요한 이유
- 어차피 average pooling이어서 입력 피처맵의 2x2 범위들을 평균내는 역할을 하므로 파라미터가 필요없지 않을까 싶다.
- 현대 CNN에서는 위 생각이 정확히 맞고, 파라미터가 필요없다.
- ==하지만 LeNet-5에서는 "Trainable subsampling"을 사용한다.==

$$y = \tanh\!\Big(a \sum_{i\in R} x_i + b\Big)$$

- 여기서
    - R: 2×2 지역 영역 (입력 픽셀들의 집합)
    - a: 학습 가능한 **gain(스케일)**
    - b: 학습 가능한 **bias(편향)**

- 즉,
	1. 2×2 영역의 평균 또는 합을 구한 뒤,
	2. 그 결과에 “학습 가능한 스케일 a”를 곱하고,
	3. “학습 가능한 바이어스 b”를 더한 다음,
	4. 비선형 활성화 함수(tanh)를 적용한다.

## Fully Connected Layer: F6
- F6는 완전연결층으로, 앞선 합성곱 계층들이 만든 고수준 특징을 통합하고 분류기로 넘기기 위한 표현을 만든다.
- 입력: 120차원의 벡터 (C5의 출력)
- 뉴런 수: 84
- 활성화 함수: tanh
- F6는 각 입력 특징의 비선형 조합을 학습하여, 숫자 클래스 간의 구분을 명확히 하는 중간 표현을 만든다.
- 이후 F6의 출력은 최종 분류층(RBF layer)에 전달된다.

- 파라미터 수
	- 각 출력 뉴런당 120개의 가중치 + 1개의 편향이 존재한다.
	- $(120+1)*84 = 10,164$

## Output Layer
- 마지막 계층은 출력 뉴런 10개로 구성되어, 손글씨 숫자 10개 클래스를 분류하는 역할을 한다.
- 현대 CNN에서의 softmax대신, Radial Basis Function (RBF)를 사용했다.
- 입력: F6의 84개 뉴런 모두와 완전 연결
- 출력 뉴런: 10개(각각 0-9 클래스에 해당)
- 학습 방식: backpropagation으로 가중치를 업데이트

- 파라미터 수 계산:
	- Softmax layer는 선형 결합을 학습하지만, RBF layer는 "입력 벡터가 각 중심에 얼마나 가까운지"를 학습한다.
	- 그래서 입력 뉴런 84개에 대해 10개씩 파라미터를 가져서 총 840개의 파라미터를 가진다.


![[Pasted image 20251027012910.png]]
1. 당시에는 softmax 기반 확률 분류보다 패턴 매칭이 더 직관적인 방법으로 여겨졌다. RBF layer에서는 각 클래스 중심과 입력 벡터의 유클리드 거리를 기반으로 유사도를 계산하여 입력이 특정 클래스 중심에 가까울수록 출력값이 커지게 동작했다. 즉, softmax 대신 RBF를 사용한 이유는 거리 기반 분류를 통한 패턴 인식 효과를 얻기 위함이었다.

  
2. LeNet-5는 tanh 활성화 함수, Average pooling, RBF 출력층을 사용하는 반면, 현대 CNN은 ReLU, Max pooling, Softmax, Batch Normalization, Dropout 등을 사용한다. 또한 LeNet-5는 32x32 입력 크기와 데이터셋이 단순한 반면, 현대 CNN은 수백만 장의 컬러 이미지를 처리하도록 확장되었다. 즉, LeNet-5는 CNN의 기초 구조를 제시했지만 규모, 비선형성, 정규화 측면에서 현대 CNN과 다르다.


3. C3 layer는 S2의 모든 feature map과 완전히 연결되지 않고 일부 조합만 연결된다. 이렇게 하면 첫째로, 파라미터 수가 들어 과적합을 방지하고, 둘째로 입력 조합이 다양해져 feature 다양성이 증가한다. 즉, grouped connection은 효율성과 표현력 향상을 동시에 달성하기 위한 설계다.


4. LeNet-5는 현대 CNN의 기초를 마련한 최초의 end-to-end 학습 구조이다. convolution, pooling, shared weights, local receptive filed 개념을 처음으로 성공적으로 통합하여, 이후 AlexNet, VGG, ResNet 등 모든 CNN의 설계 기반이 되었다. 따라서 LeNet-5는 딥러닝 비전 분야의 출발점이자 역사적 표준 모델로 여전히 중요하다.