---
created: 2025-09-16
Question: AlexNet에서 ReLU 직후 LRN을 사용하는데 시너지가 있는거지? 어떤 뉴런은 ReLU에서 매우 큰 값을 유지할텐데 LRN이 suppresse 해주니까.
---
네, 정확하게 이해하셨어요 👍

  

AlexNet에서 **ReLU → LRN** 순서를 둔 건 의도적인 설계예요.

1. **ReLU의 특징**
    
    - 음수는 0으로 잘라내고, 양수는 그대로 두기 때문에 큰 값이 나오면 그대로 유지됩니다.
        
    - 따라서 어떤 뉴런은 매우 큰 활성값을 가질 수 있어요.
        
    - 이런 경우 특정 뉴런이 지나치게 우세해져서 다른 뉴런의 학습이 방해될 수 있습니다.
        
    
2. **LRN(Local Response Normalization)의 역할**
    
    - 주변 채널의 제곱합으로 나눠주기 때문에, 특정 뉴런의 값이 크면 인접 채널들의 값이 상대적으로 억제됩니다.
        
    - 즉, **ReLU로 커진 값을 LRN이 부분적으로 누르고 경쟁 구조를 만들어줌**.
        
    - 결과적으로 희소(sparse)하면서 대비가 뚜렷한 활성화 패턴을 얻게 됩니다.
        
    
3. **시너지 효과**
    
    - ReLU는 빠른 수렴과 gradient vanishing 방지를 도와줬고,
        
    - LRN은 ReLU의 “큰 값 폭주” 문제를 완화하며, 더 균형 잡힌 feature competition을 가능하게 했습니다.
        
    - 이 조합이 AlexNet 당시 성능 향상에 기여했죠.
        
    

---

👉 정리하면: **ReLU가 큰 값을 만들고, LRN이 그것을 억제해 채널 간 경쟁을 유도**했기 때문에 둘 사이에 시너지가 있었다고 볼 수 있습니다.

  

다만, 나중에는 **Batch Normalization**이 ReLU + LRN의 역할을 더 효과적으로 대체하면서 LRN은 사라지게 된 거예요.

  

혹시 원하시면, 제가 **ReLU만 썼을 때 vs ReLU+LRN 썼을 때 활성화 분포**를 그림으로 비교해서 보여드릴까요?

![[Activation Distribution after ReLU + LRN.png]]

위 그래프는 **ReLU만 적용했을 때**와 **ReLU 뒤에 LRN을 붙였을 때**의 활성화 분포를 비교한 예시예요.

- 왼쪽(ReLU): 큰 값들이 그대로 유지되어 꼬리가 길게 퍼져 있음.
    
- 오른쪽(ReLU+LRN): 큰 값일수록 억제되어 분포가 눌리고, 더 균형 잡힌 활성화가 됨.
    

  

👉 이렇게 ReLU의 장점(빠른 학습, gradient 소실 방지)은 살리면서, LRN이 큰 값을 suppress해 채널 간 경쟁을 유도한 것이 AlexNet에서의 시너지 효과였습니다.
