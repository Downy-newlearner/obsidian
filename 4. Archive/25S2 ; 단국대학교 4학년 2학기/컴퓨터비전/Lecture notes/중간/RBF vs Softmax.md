---
created: 2025-09-15
Question: LeNet-5에서 사용되는 RBF와 Softmax를 비교해줘
---
좋습니다. LeNet-5에서 사용된 **RBF(radial basis function) 출력층**과 현대 CNN에서 주로 쓰이는 **Softmax 출력층**을 비교해서 정리해드릴게요.

---

### **1. 기본 개념**

- RBF: 각 클래스별로 하나의 **프로토타입 벡터**를 두고, 입력 벡터가 그 중심과 얼마나 가까운지를 가우시안 함수로 계산. 즉, **거리 기반 유사도**를 출력.
    
- Softmax: 각 클래스에 대한 점수를 계산한 뒤 지수화하고 정규화하여 **확률 분포**를 출력. 즉, **확률 기반 분류**.
    

---

### **2. 출력 해석**

- RBF: “이 입력이 특정 클래스 중심에 얼마나 가까운가?”라는 해석.
    
- Softmax: “이 입력이 특정 클래스일 확률이 얼마인가?”라는 해석.
    

---

### **3. 장점과 단점**

- RBF:
    
    - 장점: 클래스 간 경계를 **거리로 직접 정의**할 수 있어 해석적이고 안정적.
        
    - 단점: 클래스 수가 많아지거나 데이터 분포가 복잡해지면 잘 동작하지 않고, 스케일이 커지면 성능 저하.
        
    
- Softmax:
    
    - 장점: 클래스 수가 많아도 잘 확장되고, 확률적으로 해석 가능. 학습 시 cross-entropy 손실과 자연스럽게 연결됨.
        
    - 단점: 항상 상대적인 확률만 제공하므로, 절대적인 “거리” 개념은 담기 어려움.
        
    

---

### **4. LeNet-5와 현대 CNN에서의 차이**

- LeNet-5(1998): 출력층으로 RBF를 사용 → 클래스별 중심과의 유사도를 계산하여 손글씨 인식.
    
- 현대 CNN: 거의 대부분 softmax를 사용 → 확률적 출력, cross-entropy 손실과 함께 학습.
    

---

👉 정리하면, LeNet-5의 RBF는 **거리 기반 접근**이고, 현대 CNN의 Softmax는 **확률 기반 접근**이에요.

LeNet-5 당시에는 softmax가 지금처럼 사실상의 표준은 아니었고, RBF가 안정적이고 직관적인 방법으로 채택된 거예요.

  

혹시 이걸 **비교 표**로 깔끔하게 정리해드릴까요?