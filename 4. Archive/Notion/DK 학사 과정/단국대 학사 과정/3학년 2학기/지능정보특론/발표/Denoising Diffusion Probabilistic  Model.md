핵심 문장, 키워드 위주로 뽑아보자.
#### 딥러닝&파이썬 용어 정리
|이름|설명|
|---|---|
|[[Conditional]]|“Conditional”이라는 용어가 붙은 이미지 생성형 모델 아키텍처는 특정 조건이나 입력에 따라 생성할 결과물이 결정된다는 의미이다.|
|[[Objective function, Cost function]]|목적 함수, 최적화 문제에서 변수가 최소화 또는 최대화해야 하는 함수이다. 일례로 경사 하강법의 대상이 목적 함수인 것이다.|
|[[브이랩]]|Virtual Lab 가상환경에서 실험을 수행할 수 있도록 도와주는 시스템|
|[[robust learning]]|다양한 환경이나 조건에서도 강력한 성능을 유지할 수 있도록 모델을 학습하는 방법. 데이터의 노이즈, 잡음, 이상치에 대한 강인성을 강조하며 이런 불확실성에서도 안정적이고 신뢰성 있는 예측 모델을 만드는 것이 목표인 학습 방법이다.|
|[[Noise Scheduler]]|Diffusion Process에서 노이즈가 추가되는 정도를 조절한다. 예시로 linuer noise scheduler는 시간이 지날 수록 더 많은 노이즈가 추가되도록 한다.|
|[[Markov chain]]|각 전이의 단계는 현재 상태에만 의존한다는 메모리리스 속성을 가진다. 즉 현재 데이터 포인트(상태)에서 노이즈를 추가하여 다음 데이터 포인트(다음 상태)가 생성된다. 이 과정은 여러 단계에 걸쳐 진행되며, 각 단계가 가우시안 전이로 이루어지게 된다.|
|[[Gaussian transition]]|Diffusion model에서 데이터의 변환 과정이 가우시안(정규) 분포를 따르는 것과 관련이 있습니다. 확산 모델은 노이즈를 점진적으로 추가하거나 제거하여 데이터를 생성합니다. 이 과정에서 각 단계는 가우시안 분포를 통해 정의된 전이 과정을 따릅니다.|
|[[Diffusion Model]]|주로 데이터를 생성하거나 모사하는 데 사용되는 수학적 또는 컴퓨터 과학적 모델을 의미한다. 이 모델은 확산의 원리를 기반으로 하여, 데이터를 점진적으로 "노이즈"와 함께 변화시키고, 그 과정을 역으로 수행하여 원래 데이터를 복원하거나 새로운 데이터를 생성하는 데 사용된다.|
|[[deep generative model]]||
|[[parameterized Markov chain]]|마르코프 체인의 전이 확률이 특정한 매개변수들에 의해 조정되고 결정된다는 것을 의미한다. 그리고 이 매개변수들은 모델의 훈련 과정에서 최적화된다.|
|[[variational inference(변분 추론)]]|이후 타임스텝으로 근사하기 위해 사용하는 베이지 추론 기법 중 하나로, 효율적이고 실용적이라는 특징이 있다. 후방 분포를 직접 계산하는 것이 아니라 그저 후방분포를 근사하는 간단한 분포를 설정한다는 아이디어를 통해 효율성과 실용성을 챙기는 것이다.|
|[[이미지가 가우시안 분포를 따른다는 것은]]|각 픽셀은 R, G, B 값으로 각각 0~255 사이의 값을 갖는다. 이 때 R, G, B 각각을 서로 독립적으로 보아 모든 픽셀의 R 값이 가우시안 분포를 따르고, G값, B값도 가우시안 분포를 따른다는 것이다.|
|[[결합 분포(joint distribution)]]|두 개 이상의 확률 변수의 분포를 나타내는 개념으로, 여러 확률 변수가 동시에 어떤 값을 가질 확률을 설명한다.|
  
  
## Introduction
**2️⃣**
- ==DDPM은 parameterized Markov chain(파라미터화된 마르코프 체인)이다.==
    - 이 parameterized Markov chain는 유한한 시간 이후에 데이터와 매칭되는 샘플을 만들어내는 변분 추론을 사용해 훈련된다.
- **역산 과정 학습**: DDPM에서 마르코프 체인의 전이는 데이터를 무작위로 노이즈를 추가하는 확산 과정을 반대로 하는 방향으로 학습되며, 이는 신호가 파괴될 때까지 진행됩니다.
- **가우시안 노이즈 활용**: 확산 과정이 소량의 가우시안 노이즈로 구성될 때, 샘플링 체인의 전이를 조건부 가우시안으로 설정하는 것이 가능하여 모델을 간소화할 수 있습니다.
- **신경망 파라미터화**: 조건부 가우시안을 활용함으로써 특히 간단한 신경망 파라미터화가 가능해져, 효율적인 학습과 데이터 샘플 생성을 돕습니다.
  
3️⃣
  
  
4️⃣
  
**핵심 요약 (쉬운 용어로)**:
- **고품질 샘플 생성**: 확산 모델은 실제로 높은 품질의 샘플을 만들 수 있습니다. 특히 특정한 방법을 사용하면, 잡음을 제거하는 과정과 샘플링 방식이 서로 연결된다는 것을 보여주었고, 이것이 연구의 중요한 기여로 평가받고 있습니다.
- **로그 우도와 코드 길이**: 이 모델의 로그 우도는 다른 모델에 비해 낮지만, 에너지 기반 모델보다는 더 나은 성과를 보입니다. 또한, 샘플을 저장하는 데 필요한 정보의 대부분이 잘 보이지 않는 이미지 세부 정보를 설명하는 데 사용된다는 사실을 발견했습니다.
- **샘플링 방법**: 확산 모델의 샘플을 생성하는 방법은 자기 회귀 디코딩 방식과 비슷합니다. 이는 기존 모델의 한계를 넘어서 새로운 가능성을 제공하는 방식입니다.
**전체적으로**, 이 연구는 확산 모델이 고품질 샘플을 만들 수 있다는 것을 보여주며, 이를 위해 효과적인 방법과 샘플링 절차를 개발하여 기존의 모델들을 확장하는 방향으로 기여하고 있습니다.
## Background
![[Source/image 40.png|image 40.png]]
![[Diffusion%EB%B0%9C%ED%91%9C%EC%9E%90%EB%A3%8C_page-0018.jpg]]
중요한 것은. reverse process도 가우시안으로 모델링할 수 있고, 이 때의 평균과 분산이 가장 궁금한 것이 된다.
  
  
![[Source/image 1 17.png|image 1 17.png]]
![[Diffusion%EB%B0%9C%ED%91%9C%EC%9E%90%EB%A3%8C_page-0012.jpg]]
forward process 관련 수식
  
이 식은 특정 시점 \(t\)의 상태 \(x_t\)가 이전 상태 \(x_{t-1}\)에 기반하여 정의된 정규 분포를 따른다는 것을 나타냅니다. 일반적으로 이러한 표현은 시계열 모델이나 상태 공간 모델에서 사용됩니다.
  
$\beta_t$﻿는 0에 가까운 0.0001같은 값이다. 그러므로 $\sqrt{1 - \beta_t}$﻿는 1에 가까운 값이다.
이는 이전 값을 아주 조금 감소시킨다는 의미를 갖는다.
  
이전 스텝에서 다음 스텝으로 갈 때 노이즈를 조금 조금씩 입힌다.
![[Source/image 2 16.png|image 2 16.png]]
어떻게 학습할 것인가? > variational upper bound를 이용(VAE에서 식을 전개하듯이)
1. $p_\theta$﻿에 대한 negative log likelihood를 수식적으로 정의하고
2. 이에 대한 ELBO를 구한다.
![[Source/image 3 13.png|image 3 13.png]]
최종적인 Loss Term이다.
이를 통해 단순히 이 세가지 term의 KLD 텀과 마지막 텀 하나를 가지고 로스를 최소화시킴으로서 평균 구하기를 목표로 하고 있다. 이는 Regression과 동일해지는 모습이다.
1. 첫 번째 텀:
    
    $p(x_T)$﻿는 가우시안 노이즈로 상정함
    
    q의 경우에도 실제 가우시안 커널을 q로 이야기함.
    
    즉 가우시안과 가우시안의 KLD를 구하는 것은 의미가 없어서 첫 번째 텀은 사용 안됨
    
2. 두 번째 텀($L_{t-1}$﻿):
    
    여기서 둘 사이의 KLD
    
    첫 q는 tractable posterior distribution으로 정의를 내려 이는 가우시안 분포가 된다.
    
    둘을 모두 Normal distribution으로 가정이 되고, 이 둘의 KLD를 가우시안 분포간의 KLD로 만들 수 있게 된다.
    
    ![[Source/image 4 8.png|image 4 8.png]]
    
    평균 사이의 차이, 분산을 나눠주는 수식으로 표현이 가능해진다.
    
    ![[Source/image 5 8.png|image 5 8.png]]
    
    ![[Source/image 6 6.png|image 6 6.png]]
    
    앞에서 정의한 대로 x_t$x_t$﻿$x_0$﻿를 통해 표현가능한데 조금조금 노이즈를 더하는 것을 한 번에 표현한다고 가정했을 때 이 식이 나오는 것이다.
    
    이제 뮤 틸다를 수식화하면 다음과 같다.
    
    ![[Source/image 7 5.png|image 7 5.png]]
    
    최종적으로 알고 싶은 것은 가우스 분포인 리버스 프로세스의 평균이다.(다음 사진)
    
    ![[Source/image 8 4.png|image 8 4.png]]
    
    나머지 값들은 동일한 값들이고 노이즈 네트워크 $\epsilon_\theta(x_t, t)$﻿를 통해서 평균을 예측할 수 있게 된다.
    
    결국 가우시안 커널의 평균을 알고싶다 → 노이즈 프레딕션을 해야한다. 로 바뀐 것이다.
    
    ![[Source/image 9 4.png|image 9 4.png]]
    
    노이즈를 예측하는 것 만으로 최종 로스를 계산할 수 있다.
    
    이 $L_{t-1}$﻿를 다음과 같이 표현할 수 있다.
    
    ![[Source/image 10 3.png|image 10 3.png]]
    
    그 다음 DDPM 논문에서 주장하는 대로 $\lambda_t$﻿를 1로 하여 $L_{simple}$﻿을 정의하자
    
    ![[Source/image 11 3.png|image 11 3.png]]
    
3. 세 번째 텀($L_0$﻿):
    
    고정된 값이므로 사용되지 않는다.
    
      
    
  
**요약: 훈련과 샘플링**
![[Source/image 12 3.png|image 12 3.png]]
최종적으로 로스 함수를 구하기 위해 가우시안 모델의 평균을 예측하고싶었고, 이를 위해 식을 쓰다보니 각 스텝 사이의 노이즈를 예측하는 모델을 만들면 된다는 결론이 나왔다.
매 스탭마다 가우시안 노이즈를 샘플링하고 이미지에 타임스텝에 맞게 노이즈를 더하고, 이를 통해 네트워크는 어떤 노이즈가 더해졌는지 예측하는 구조를 가지며 학습한다.
이는 평균, 분산을 기반으로 학습한 것이기 때문에 우리가 생각하는 평균을 기반으로 샘플링을 하는 모습이다.
  
  
영상 20:40까지 메모함
  
  
![[Source/image 13 3.png|image 13 3.png]]
![[Source/image 14 3.png|image 14 3.png]]
![[Source/image 15 3.png|image 15 3.png]]
  
  
## Diffusion models and denoising autoencoders
## Experiments
## Related Work
## Conclusion
  
  
## 강의3

> [!info] [ Diffusion ] 1. loss 설명  
> [Powered by Vrew]  
> [https://www.youtube.com/watch?v=RGlwzCWJubs](https://www.youtube.com/watch?v=RGlwzCWJubs)  
노이즈를 거는 q, 노이즈를 걷어내는 $p_\theta$﻿ 이 때 p에 $\theta$﻿를 붙여서 친절히 뉴럴 넷임을 알려줌
![[Source/image 16 3.png|image 16 3.png]]
그리고 뮤랑 시그마에 세타가 붙어있으니까 이 둘을 예측하는, 즉 이 둘이 출력인 것임을 알 수 있음
  
베이지안 룰 공식
![[Source/image 17 3.png|image 17 3.png]]
  
  
마르코프 체인 성질
![[Source/image 18 3.png|image 18 3.png]]
  
위 둘을 알고 시작해야한다.
  
  
  
  
  
  
  
$ x_{t-1} = \frac{1}{\sqrt{\alpha_{t}}} \left(x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}t}} \epsilon\theta(x_t, t)\right)$﻿
$\bar{\alpha}_t $