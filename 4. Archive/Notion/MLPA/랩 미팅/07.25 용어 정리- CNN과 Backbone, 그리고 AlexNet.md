## 1. Backbone 개념을 이해하기 위한 배경지식 키워드 목록
1. 신경망 (Neural Networks)
### 2. 합성곱 신경망 (Convolutional Neural Networks, CNNs)
**합성곱이란?**
- 먼저 합성곱이란 두 함수의 결합을 통해 새로운 함수를 만들어내는 연산을 의미한다.
- 합성곱은 주로 이미지 데이터에서 특징을 추출하는 방식으로 사용된다.
**합성곱연산**
- `입력 이미지`에 적용하는 연산이다. 무엇을 적용하냐면 여러 개의 필터(또는 커널)를 적용하여 합성곱 연산을 수행한다.
    
    **필터**: 작은 크기의 행렬, 이미지의 특정 패턴이나 특징을 인식하는 데 도움을 준다.(Edge 검출, 텍스처 감지)
    
**지역적 패턴 인식**
- CNN은 전체 이미지가 아닌 부분적으로 관련된 정보를 학습할 수 있다.
- 이는 이미지의 각 부분에 대해 독립적으로 필터를 적용함으로써 이루어진다.
**특징 추출**
- CNN에서 합성곱 연산을 통해 자동으로 유용한 특징을 추출할 수 있다.(CNN의 장점)
- 이는 여러 개 쌓이는 Convolutional 레이어에서, 각 레이어는 이전 레이어에서 추출된 특징의 조합을 학습하여 더 높은 수준의 특징을 생성한다.
**비선형성 얹기(출력 단계에서)**
- 합성곱 연산 후에 ReLU와 같은 활성화 함수를 적용하여 비선형성을 도입한다.
- 이는 CNN이 더욱 복잡한 패턴을 학습할 수 있게 해준다.
**풀링 레이어(출력 단계에서)**
- 합성곱 레이어의 출력에서 공간적 차원을 줄여 계산 비용을 줄이고, 과적합을 방지한다.
- Max pooling / Average pooling 을 이용하여 중요한 정보는 유지하면서, 해상도를 낮춘다.
**완전 연결 레이어**
- `최종 분류 결정`, `비선형성 추가`, `차원 축소`의 역할을 한다.
**전이 학습**
  
**Vanishing Gradients(기울기 소실) 문제**
![[Source/Untitled 5.png|Untitled 5.png]]
- 시그모이드의 특성상 시그모이드 함수의 도함수는 값이 커질수록 0에 근접한다. 이로 인해서 기울기 소실 문제가 발생한다.
  
  
  
## ==3. 특징 추출 (Feature Extraction)==

> [!info] [딥러닝] CNN - 특징 추출 과정  
> 기본적인 Neural Network 앞에 여러 계층의 Convolutional Layer을 붙인 형태위의 Convolution Layer 부분이 아래 Feature extraction 부분, 위의 Neural Network 부분이 아래 Classification 부분이  
> [https://velog.io/@yeonheedong/DL-CNN-특징-추출-과정](https://velog.io/@yeonheedong/DL-CNN-특징-추출-과정)  
특징 추출은 이미지 데이터에서 중요한 패턴이나 특성을 자동으로 인식하는 과정이다. CNN에서는 여러 개의 합성곱(Convolution) 레이어를 사용하여 입력 이미지의 고유한 특징을 추출한다. 각 합성곱 레이어는 필터를 적용하여 이미지의 특정 지역을 처리하고, 활성화 함수를 통해 비선형성을 추가한다. 특징 추출 이후, 풀링(Pooling) 레이어를 통해 중요한 정보만 남기고 공간적 차원을 줄이는 과정이 이루어진다.
1. 이미지넷 (ImageNet)
2. ==전이 학습 (Transfer Learning)==
3. 레이어 (Layers)
4. 모델 평가 지표 (Model Evaluation Metrics)
5. 데이터 전처리 (Data Preprocessing)
6. 모델 아키텍처 (Model Architectures)
7. 잔차 블록 (Residual Block)
8. ==잔차 연결 (Skip Connection)==
9. 다층 퍼셉트론 (Multilayer Perceptron, MLP)
10. 배치 정규화 (Batch Normalization)
### 21. 알렉스넷 (AlexNet)
**2012년 ILSVRC 대회 우승**
- 이미지넷에서 개최한 ILSVRC에서 2등 모델과 10% 이상의 압도적인 차이를 보이며 1위를 달성하였다.
- CNN의 창시자 LeCun의 LeNet5와 유사했고, GPU를 사용하였다.
**기존 머신러닝 기법에서 탈피**
- 기존 Object Recognition에서는 머신러닝 기법이 일반화 되어 있었다. (예: SVM)
- 머신러닝 사용시 small dataset은 좋은 성능을 냈으나, 성능이 좋은 일반화를 위해서는 larger dataset이 필요했다.
  
- AlexNet은 심층 신경망 모델을 사용하여 머신러닝 기법의 탈피를 성공했다.
- 원래 심층 신경망 모델은 Vanishing Gradient 문제때문에 사용치 않고 있었지만 AlexNet은 이 문제를 해결한 모델이다.
**Larger Dataset 사용**
- 학습시킬 때 적은 계산량만이 필요하여, Larger Dataset 사용이 가능했다.
**Dropout**
![[Untitled 1.png]]
- 과대적합을 방지하는 기법
- 노드를 확률적으로 사용하여 과대적합을 방지했다.
**ReLU 활성함수 사용**
- 학습 속도가 빠름(단순한 구조 때문에)
- 기울기 소실 문제를 어느정도 해결.
**AlexNet 이후 개선된 CNN 모델들**
**1. VGGNet (Visual Geometry Group Network)**
- **VGG-16, VGG-19**: 2014년 ILSVRC에서 소개된 VGGNet은 심층 컨볼루션 네트워크의 깊이가 이미지 인식 성능에 미치는 영향을 강조했습니다. VGGNet은 매우 단순하고 규칙적인 구조를 가지며, 동일한 크기의 필터를 연속으로 사용합니다.
    - **의의**: 네트워크 깊이가 중요한 성능 향상 요소임을 보여주었고, 이후 많은 연구에서 표준 아키텍처로 사용되었습니다.
**2. ResNet (Residual Network)**
- **ResNet-50, ResNet-101, ResNet-152**: 2015년 ILSVRC에서 소개된 ResNet은 잔차 연결(skip connections) 기법을 도입하여 매우 깊은 네트워크에서도 학습이 원활하게 이루어지도록 했습니다. 이로 인해 기울기 소실 문제(vanishing gradient problem)를 해결하게 되었으며, ResNet-152는 당시 가장 깊은 네트워크 중 하나였습니다.
    - **의의**: 잔차 연결을 통해 딥러닝 모델을 매우 깊게 쌓을 수 있게 하였고, 학습 및 추론의 성능을 크게 향상시켰습니다. ResNet은 나아가 다양한 응용 분야에서 표준으로 자리 잡았습니다.
**3. GoogLeNet (Inception Network)**
- **Inception v1, v2, v3**: 2014년 ILSVRC에서 소개된 GoogLeNet(Inception v1)은 Inception 모듈이라는 독특한 구조를 도입하여 다양한 크기의 필터를 병렬로 사용하여 특징을 추출했습니다. 이를 통해 네트워크의 성능과 효율성을 극대화했습니다.
    - **의의**: 여러 규모의 특징을 병렬로 추출하는 방법을 도입하여, 효율성과 성능을 동시에 높였습니다. Inception 아키텍처는 깊이와 너비를 모두 확장한 설계로 널리 연구되고 있습니다.
  
### ==22. ResNet==