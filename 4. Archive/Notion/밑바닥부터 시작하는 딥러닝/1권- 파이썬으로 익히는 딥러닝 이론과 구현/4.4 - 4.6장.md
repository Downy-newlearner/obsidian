## 기울기
![[Source/Untitled 14.png|Untitled 14.png]]
```Python
# 앞 절에서 x0, x1에 대한 편미분을 변수별로 따로 계산했음.
# x0, x1의 편미분을 동시에 계산하고 싶다면?
# x0 = 3, x1 = 4일 때 (x0, x1) 양쪽의 편미분을 묶어 벡터로 정리한 것을 기울기gradient라고 한다.
def numerical_gradient(f, x):
    h = 1e-4
    grad = np.zeros_like(x)  # x와 형상이 같은 배열을 생성
    for idx in range(x.size):
        tmp_val = x[idx]
        # f(x+h) 계산
        x[idx] = tmp_val + h
        fxh1 = f(x)
        # f(x-h) 계산
        x[idx] = tmp_val - h
        fxh2 = f(x)
        grad[idx] = (fxh1 - fxh2) / (2 * h)
        x[idx] = tmp_val  # 값 복원
    return grad
```
  
- 기울기 벡터의 그림은 다음과 같이 그려진다
    
    ![[Source/Untitled 1 8.png|Untitled 1 8.png]]
    
    - 기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향이다.
    - 이후 경사법에서는 ‘손실 함수’의 출력 값을 최대한 줄여나갈 것이므로 기울기는 매우 의미있는 개념이다.
    
      
    
## 경사법(경사하강법)
![[Source/Untitled 2 6.png|Untitled 2 6.png]]
- 기울기를 이용해 함수의 최솟값(또는 가능한 한 작은 값)을 찾으려는 것이 경사법이다.
- 경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동한다. 그런 다음 이동한 곳에서도 마찬가지로 기울기를 구하고, 또 그 기울어진 방향으로 나아가기를 반복한다.
![[Source/Untitled 3 6.png|Untitled 3 6.png]]
- ‘좋은 장소’를 잘 찾아가기 위해, 학습률을 잘 조정하면서 올바르게 학습하고 있는지를 확인하면서 진행해야한다.
![[Source/Untitled 4 5.png|Untitled 4 5.png]]
![[Source/Untitled 5 5.png|Untitled 5 5.png]]
![[Source/Untitled 6 5.png|Untitled 6 5.png]]
- 실제로 진정한 최솟값은 (0,0)이므로 경사법으로 거의 정확한 결과를 얻은 것이다.
    - x의 변화 히스토리
        
        > print(x_history)  
        > [[-3.00000000e+00 4.00000000e+00]  
        > [-2.40000000e+00 3.20000000e+00]  
        > [-1.92000000e+00 2.56000000e+00]  
        > [-1.53600000e+00 2.04800000e+00]  
        > [-1.22880000e+00 1.63840000e+00]  
        > [-9.83040000e-01 1.31072000e+00]  
        > [-7.86432000e-01 1.04857600e+00]  
        > [-6.29145600e-01 8.38860800e-01]  
        > [-5.03316480e-01 6.71088640e-01]  
        > [-4.02653184e-01 5.36870912e-01]  
        > [-3.22122547e-01 4.29496730e-01]  
        > [-2.57698038e-01 3.43597384e-01]  
        > [-2.06158430e-01 2.74877907e-01]  
        > [-1.64926744e-01 2.19902326e-01]  
        > [-1.31941395e-01 1.75921860e-01]  
        > [-1.05553116e-01 1.40737488e-01]  
        > [-8.44424930e-02 1.12589991e-01]  
        > [-6.75539944e-02 9.00719925e-02]  
        > [-5.40431955e-02 7.20575940e-02]  
        > [-4.32345564e-02 5.76460752e-02]  
        > [-3.45876451e-02 4.61168602e-02]  
        > [-2.76701161e-02 3.68934881e-02]  
        > [-2.21360929e-02 2.95147905e-02]  
        > [-1.77088743e-02 2.36118324e-02]  
        > [-1.41670994e-02 1.88894659e-02]  
        > [-1.13336796e-02 1.51115727e-02]  
        > [-9.06694365e-03 1.20892582e-02]  
        > [-7.25355492e-03 9.67140656e-03]  
        > [-5.80284393e-03 7.73712525e-03]  
        > [-4.64227515e-03 6.18970020e-03]  
        > [-3.71382012e-03 4.95176016e-03]  
        > [-2.97105609e-03 3.96140813e-03]  
        > [-2.37684488e-03 3.16912650e-03]  
        > [-1.90147590e-03 2.53530120e-03]  
        > [-1.52118072e-03 2.02824096e-03]  
        > [-1.21694458e-03 1.62259277e-03]  
        > [-9.73555661e-04 1.29807421e-03]  
        > [-7.78844529e-04 1.03845937e-03]  
        > [-6.23075623e-04 8.30767497e-04]  
        > [-4.98460498e-04 6.64613998e-04]  
        > [-3.98768399e-04 5.31691198e-04]  
        > [-3.19014719e-04 4.25352959e-04]  
        > [-2.55211775e-04 3.40282367e-04]  
        > [-2.04169420e-04 2.72225894e-04]  
        > [-1.63335536e-04 2.17780715e-04]  
        > [-1.30668429e-04 1.74224572e-04]  
        > [-1.04534743e-04 1.39379657e-04]  
        > [-8.36277945e-05 1.11503726e-04]  
        > [-6.69022356e-05 8.92029808e-05]  
        > [-5.35217885e-05 7.13623846e-05]  
        > [-4.28174308e-05 5.70899077e-05]  
        > [-3.42539446e-05 4.56719262e-05]  
        > [-2.74031557e-05 3.65375409e-05]  
        > [-2.19225246e-05 2.92300327e-05]  
        > [-1.75380196e-05 2.33840262e-05]  
        > [-1.40304157e-05 1.87072210e-05]  
        > [-1.12243326e-05 1.49657768e-05]  
        > [-8.97946606e-06 1.19726214e-05]  
        > [-7.18357285e-06 9.57809713e-06]  
        > [-5.74685828e-06 7.66247770e-06]  
        > [-4.59748662e-06 6.12998216e-06]  
        > [-3.67798930e-06 4.90398573e-06]  
        > [-2.94239144e-06 3.92318858e-06]  
        > [-2.35391315e-06 3.13855087e-06]  
        > [-1.88313052e-06 2.51084069e-06]  
        > [-1.50650442e-06 2.00867256e-06]  
        > [-1.20520353e-06 1.60693804e-06]  
        > [-9.64162827e-07 1.28555044e-06]  
        > [-7.71330261e-07 1.02844035e-06]  
        > [-6.17064209e-07 8.22752279e-07]  
        > [-4.93651367e-07 6.58201823e-07]  
        > [-3.94921094e-07 5.26561458e-07]  
        > [-3.15936875e-07 4.21249167e-07]  
        > [-2.52749500e-07 3.36999333e-07]  
        > [-2.02199600e-07 2.69599467e-07]  
        > [-1.61759680e-07 2.15679573e-07]  
        > [-1.29407744e-07 1.72543659e-07]  
        > [-1.03526195e-07 1.38034927e-07]  
        > [-8.28209562e-08 1.10427942e-07]  
        > [-6.62567649e-08 8.83423532e-08]  
        > [-5.30054119e-08 7.06738826e-08]  
        > [-4.24043296e-08 5.65391061e-08]  
        > [-3.39234636e-08 4.52312849e-08]  
        > [-2.71387709e-08 3.61850279e-08]  
        > [-2.17110167e-08 2.89480223e-08]  
        > [-1.73688134e-08 2.31584178e-08]  
        > [-1.38950507e-08 1.85267343e-08]  
        > [-1.11160406e-08 1.48213874e-08]  
        > [-8.89283245e-09 1.18571099e-08]  
        > [-7.11426596e-09 9.48568795e-09]  
        > [-5.69141277e-09 7.58855036e-09]  
        > [-4.55313022e-09 6.07084029e-09]  
        > [-3.64250417e-09 4.85667223e-09]  
        > [-2.91400334e-09 3.88533778e-09]  
        > [-2.33120267e-09 3.10827023e-09]  
        > [-1.86496214e-09 2.48661618e-09]  
        > [-1.49196971e-09 1.98929295e-09]  
        > [-1.19357577e-09 1.59143436e-09]  
        > [-9.54860614e-10 1.27314749e-09]  
        > [-7.63888491e-10 1.01851799e-09]]  
        
- 만약 학습률이 너무 크다면, 큰 값으로 발산한다.
- 학습률이 너무 작다면, 학습이 제대로 진행되지 않은 채 끝난다.
  
### 경사하강법 요약
기울기 구하기 → 기울기(벡터) 방향으로 학습률(lr)만큼 이동하기 → 이동한 곳에서 다시 기울기 구하고 학습률 만큼 이동하기를 반복
  
## 신경망에서의 기울기
```Python
#기울기 구하는 함수
def numerical_gradient(f, x):
    h = 1e-4
    grad = np.zeros_like(x)  # x와 형상이 같은 배열을 생성
    for idx in range(x.size):
        tmp_val = x[idx]
        # f(x+h) 계산
        x[idx] = tmp_val + h
        fxh1 = f(x)
        # f(x-h) 계산
        x[idx] = tmp_val - h
        fxh2 = f(x)
        grad[idx] = (fxh1 - fxh2) / (2 * h)
        x[idx] = tmp_val  # 값 복원
    return grad
```
  
```Python
# 4.4.2 신경망에서의 기울기
class simpleNet:
    """docstring for simpleNet"""
    def __init__(self):
        self.W = np.random.randn(2, 3)  # 정규분포로 초기화
    def predict(self, x):
        return np.dot(x, self.W)
    def loss(self, x, t):
        z = self.predict(x)
        y = softmax(z)
        loss = cross_entropy_error(y, t)
        return loss

net = simpleNet()
print(net.W)  # 가중치 매개변수(랜덤)
x = np.array([0.6, 0.9])
p = net.predict(x)
print(p)
print(np.argmax(p))  # 최댓값의 인덱스
t = np.array([0, 0, 1])  # 정답 레이블
print(net.loss(x, t))

def f(W):
    return net.loss(x, t)
#또는 이렇게 작성할 수도 있다.
f = lambda W: net.loss(x,t)
dW = numerical_gradient(f, net.W)
print(dW)
```
![[Source/Untitled 7 5.png|Untitled 7 5.png]]
위 W와 경사의 개념이 어려움 다시 정독해보자. (07.15)
  
## 학습 알고리즘 구현하기
### 2층 신경망 클래스 구현하기
```Python
# coding: utf-8
import sys
import os
import numpy as np
# 변경할 디렉토리를 설정합니다.
new_directory = "H:/내 드라이브/Home/deeplearning_from_scratch/ch4.신경망 학습"
# 작업 디렉토리를 변경합니다.
os.chdir(new_directory)
# 변경된 작업 디렉토리를 출력합니다.
print("변경된 작업 디렉토리:", os.getcwd())
sys.path.append(os.pardir)
from common.functions import *
from common.gradient import numerical_gradient

class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
        # 가중치 초기화
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)
    def predict(self, x):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
    
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)
        
        return y
        
    # x : 입력 데이터, t : 정답 레이블
    def loss(self, x, t):
        y = self.predict(x)
        return cross_entropy_error(y, t)
    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        t = np.argmax(t, axis=1)
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy
    # x : 입력 데이터, t : 정답 레이블
    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        return grads
    def gradient(self, x, t):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
        grads = {}
        batch_num = x.shape[0]
        # forward
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)
        # backward
        dy = (y - t) / batch_num  # ????
        grads['W2'] = np.dot(z1.T, dy)
        grads['b2'] = np.sum(dy, axis=0)
        da1 = np.dot(dy, W2.T)
        dz1 = sigmoid_grad(a1) * da1
        grads['W1'] = np.dot(x.T, dz1)
        grads['b1'] = np.sum(dz1, axis=0)
        return grads
```
  
## 정리
- 손실함수를 기준으로 그 값이 가장 작아지는 가중치 매개변수 값을 찾아내는 것이 신경망 학습의 목표이다.
- 가능한 한 손실 함수의 값을 찾는 수법으로 경사법을 사용한다.