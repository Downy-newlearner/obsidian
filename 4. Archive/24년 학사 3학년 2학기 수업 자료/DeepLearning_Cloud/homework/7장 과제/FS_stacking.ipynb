{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 소프트웨어학과 32204041 정다훈 7장 과제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   class   V2   V3   V4   V5   V6   V7   V8   V9  V10  ...  V492  V493  V494  \\\n",
      "0     -1  485  477  537  479  452  471  491  476  475  ...   477   481   477   \n",
      "1     -1  483  458  460  487  587  475  526  479  485  ...   463   478   487   \n",
      "2     -1  487  542  499  468  448  471  442  478  480  ...   487   481   492   \n",
      "3      1  480  491  510  485  495  472  417  474  502  ...   491   480   474   \n",
      "4      1  484  502  528  489  466  481  402  478  487  ...   488   479   452   \n",
      "\n",
      "   V495  V496  V497  V498  V499  V500  V501  \n",
      "0   485   511   485   481   479   475   496  \n",
      "1   338   513   486   483   492   510   517  \n",
      "2   650   506   501   480   489   499   498  \n",
      "3   572   454   469   475   482   494   461  \n",
      "4   435   486   508   481   504   495   511  \n",
      "\n",
      "[5 rows x 501 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('C:\\dankook\\DeepLearning_Cloud\\data\\madelon.csv')\n",
    "print(df.head())\n",
    "\n",
    "df_X = df.loc[:, df.columns != 'class']\n",
    "df_y = df['class']\n",
    "\n",
    "# df_y의 값이 1, -1로 되어 있어서 1, 0으로 변경\n",
    "df_y = df_y.replace(1, 1)\n",
    "df_y = df_y.replace(-1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V106', 'V476', 'V337', 'V65', 'V494', 'V339', 'V242', 'V443', 'V454', 'V379', 'V49', 'V473', 'V154', 'V412', 'V137', 'V434', 'V330', 'V205', 'V212', 'V348', 'V11', 'V57', 'V150', 'V282', 'V5', 'V495', 'V432', 'V287', 'V129', 'V200']\n",
      "0.5125\n",
      "['V106', 'V476', 'V337', 'V65', 'V494', 'V339', 'V242', 'V443', 'V454', 'V379', 'V49', 'V473', 'V154', 'V412', 'V137', 'V434', 'V330', 'V205', 'V212', 'V348', 'V11', 'V57', 'V150', 'V282', 'V5', 'V495', 'V432', 'V287', 'V129', 'V200', 'V47', 'V176', 'V297', 'V222', 'V482', 'V74', 'V86', 'V459', 'V247', 'V25', 'V120', 'V334', 'V286', 'V246', 'V413', 'V42', 'V194', 'V383', 'V415', 'V225']\n",
      "0.509\n",
      "['V106', 'V476', 'V337', 'V65', 'V494', 'V339', 'V242', 'V443', 'V454', 'V379', 'V49', 'V473', 'V154', 'V412', 'V137', 'V434', 'V330', 'V205', 'V212', 'V348', 'V11', 'V57', 'V150', 'V282', 'V5', 'V495', 'V432', 'V287', 'V129', 'V200', 'V47', 'V176', 'V297', 'V222', 'V482', 'V74', 'V86', 'V459', 'V247', 'V25', 'V120', 'V334', 'V286', 'V246', 'V413', 'V42', 'V194', 'V383', 'V415', 'V225', 'V431', 'V299', 'V165', 'V13', 'V463', 'V56', 'V112', 'V115', 'V295', 'V464', 'V420', 'V256', 'V278', 'V344', 'V45', 'V353', 'V458', 'V288', 'V378', 'V418']\n",
      "0.5175\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Feature selection using the filter method\n",
    "test = SelectKBest(score_func=chi2, k=df_X.shape[1])\n",
    "fit = test.fit(df_X, df_y)\n",
    "\n",
    "# Sort features by their scores\n",
    "f_order = np.argsort(-fit.scores_)  # sort index by decreasing order\n",
    "sorted_columns = df.columns[f_order]\n",
    "\n",
    "# Test classification accuracy by selected features using XGBoost\n",
    "model = XGBClassifier(eval_metric='logloss', random_state=1234)\n",
    "\n",
    "df_X_best = []\n",
    "temp = 0\n",
    "for i in [30,50,70]:\n",
    "    fs = sorted_columns[0:i]\n",
    "    df_X_selected = df_X[fs]\n",
    "    scores = cross_val_score(model, df_X_selected, df_y, cv=5)\n",
    "    print(fs.tolist())\n",
    "    print(np.round(scores.mean(), 4))\n",
    "    if temp < scores.mean():\n",
    "        temp = scores.mean()\n",
    "        df_X_best = df_X_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 70\n",
      "Selected Features: ['V473', 'V495', 'V47', 'V222', 'V42']\n",
      "Acc: 0.5635\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Forward Search\n",
    "######################################################################\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define the model\n",
    "model = XGBClassifier(eval_metric='logloss', random_state=1234)\n",
    "\n",
    "# Initialize Sequential Feature Selector\n",
    "sfs = SequentialFeatureSelector(model, direction='forward', n_features_to_select=5)\n",
    "\n",
    "# Fit the SFS model\n",
    "fit = sfs.fit(df_X_best, df_y)\n",
    "print(\"Num Features: %d\" % fit.n_features_in_)\n",
    "fs = df_X_best.columns[fit.support_].tolist()   # selected features\n",
    "print(\"Selected Features: %s\" % fs)\n",
    "\n",
    "# Evaluate model performance with selected features\n",
    "scores = cross_val_score(model, df_X[fs], df_y, cv=5)\n",
    "print(\"Acc: \" + str(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 5\n",
      "Selected Features: ['V330', 'V495', 'V459', 'V286', 'V45']\n",
      "Acc: 0.5465000000000001\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Backward elimination (Recursive Feature Elimination)\n",
    "######################################################################\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define the model\n",
    "model = XGBClassifier(eval_metric='logloss', random_state=1234)\n",
    "\n",
    "# Initialize Recursive Feature Elimination\n",
    "rfe = RFE(model, n_features_to_select=5)\n",
    "\n",
    "# Fit the RFE model\n",
    "fit = rfe.fit(df_X_best, df_y)\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "fs = df_X_best.columns[fit.support_].tolist()   # selected features\n",
    "print(\"Selected Features: %s\" % fs)\n",
    "#print(\"Feature Ranking: %s\" % fit.ranking_)\n",
    "\n",
    "# Evaluate model performance with selected features\n",
    "scores = cross_val_score(model, df_X[fs], df_y, cv=5)\n",
    "print(\"Acc: \" + str(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model_stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "#standard scaler import\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score1: 0.5174999999999998\n"
     ]
    }
   ],
   "source": [
    "# Base model: XGBoost\n",
    "base_model = XGBClassifier(eval_metric='logloss', random_state=1234)\n",
    "\n",
    "# Cross-validation score for the base model\n",
    "base_score = cross_val_score(base_model, df_X_best, df_y, cv=5)\n",
    "print(f'score1: {np.mean(base_score)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score2: 0.5155\n"
     ]
    }
   ],
   "source": [
    "# Define the estimators for the stacking classifier\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=10, random_state=1234)),\n",
    "    ('svm', SVC(probability=True, random_state=1234)),  # Support Vector Machine model\n",
    "    ('catboost', CatBoostClassifier(random_state=1234, verbose=0)),  # CatBoost model\n",
    "    ('lr', make_pipeline(StandardScaler(), LogisticRegression(max_iter=5000)))  # Logistic Regression with scaling\n",
    "]\n",
    "\n",
    "# Define the stacking model\n",
    "model_stacking = StackingClassifier(\n",
    "    estimators=estimators, \n",
    "    final_estimator=LogisticRegression(max_iter=1000)  # Using Logistic Regression as final estimator\n",
    ")\n",
    "\n",
    "# Cross-validation score for the stacking model\n",
    "model_stacking_score = cross_val_score(model_stacking, df_X_best, df_y, cv=5)\n",
    "print(f'score2: {np.mean(model_stacking_score)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepcl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
