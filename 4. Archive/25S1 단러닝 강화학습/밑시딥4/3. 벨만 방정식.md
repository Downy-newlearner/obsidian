## 시작하며(Q&A)

- Q: 강화학습에서 벨만방정식은 왜 쓰는거야?
	- A:벨만 방정식은 최적 정책을 찾기 위해 각 상태의 최적 가치를 계산하는 데 사용됩니다.

- Q: 밑시딥4의 '3. 벨만방정식'에 등장하는 방정식 3개의 차이를 표로 알려줘
	- A:

| 방정식 유형    | 영어명                                | 설명                                                      |
| --------- | ---------------------------------- | ------------------------------------------------------- |
| 벨만 방정식    | Bellman equation                   | 주어진 정책 하에서 각 상태의 가치를 계산하여 ==정책의 가치를 평가==함.              |
| 행동 가치 함수  | Action-value function (Q-function) | 상태-행동 쌍의 가치를 평가하며, 특정 상태에서 특정 행동을 선택할 때의 기대 가치 계산에 사용됨. |
| 벨만 최적 방정식 | Bellman optimality equation        | 최적 정책에서의 상태 가치를 계산하여 최적 행동을 도출하는 데 사용됨.                 |

- Q: 행동 가치 함수에 대해 설명해줘.
	- A: 행동 가치 함수(Q 함수)는 특정 상태에서 특정 행동을 수행했을 때의 기대되는 총 보상을 계산합니다. 이 함수는 강화학습 에이전트가 최적의 행동을 선택할 수 있도록 도와주는 중요한 도구로, 미래 보상까지 고려한 값을 제공하여 정책을 평가하고 개선하는 데 사용됩니다.

- Q: 벨만 최적 방정식에 대해 설명해줘.
	- A: 벨만 최적 방정식은 각 상태에서의 최적 가치를 계산하여 최적의 행동을 결정하는 데 사용되는 방정식입니다. 이 방정식은 최적 정책 하에서의 기대 보상을 구체화하며, 가치 반복이나 정책 반복과 같은 알고리즘에서 활용됩니다.
## 3장 목표
![[Pasted image 20250320204405.png|400]]
위와 같은 결정적이지 않고 확률적인 상황에서 '상태 가치 함수'를 구하자.


## 흐름
보상을 많이 얻는 것이 Reinforcement Learning의 목적이다. 
어떤 시간 $t$ 이후로 얻을 수 있는 보상의 총합이 ‘return’ $G_t$이다. 
상태 가치 함수는 return에 대한 기댓값이다.

따라서, _Reinforcement Learning의 목표_ 는 이러한 State Value Function를 최대화하는 policy를 찾는 것이다. 
_State Value Function_  $V(s)$ 는 특정 상태  $s$ 에서 시작하여 ==최적의 policy==를 따를 때 얻을 수 있는 미래 보상의 기댓값이다. 

수식으로 표현하면:

$V(s) = \mathbb{E}[G_t | S_t = s]$

여기서 $E$는 기대값을 의미하고, $G_t$ 는 시간 $t$ 이후로 얻을 수 있는 보상의 총합이다.

또한, _행동 가치 함수(Action Value Function) $Q(s, a)$_ 도 중요한 개념이다. 
이는 특정 상태 $s$에서 행동 $a$를 취했을 때의 기대 보상을 나타낸다:

$Q(s, a) = \mathbb{E}[G_t | S_t = s, A_t = a]$

$Q(s, a)$ 는 상태뿐만 아니라 행동까지 고려하기 때문에 더 구체적인 정보로, $V(s)$보다 더 좁은 개념이라고 볼 수 있다.
또한 Action Value Function는 State-Action Value Function이라고도 부른다.


Reinforcement Learning 에이전트는 이러한 가치 함수를 학습함으로써 최적의 policy를 도출하고, 각 상태에서 가장 보상이 높은 행동을 선택한다. 이를 통해 에이전트는 점점 더 나은 성능을 발휘할 수 있다.
일반적으로 많은 강화학습 알고리즘들은 ( Q )-learning이나 SARSA와 같은 방법을 통해 ( Q(s, a) )를 학습한다.

policy  $\pi(a|s)$ 는 주어진 상태  $s$에서 행동 $a$를 선택할 확률을 나타내며, 에이전트는 이 policy를 기준으로 행동을 결정한다. 최적 policy는 모든 상태에 대하여 최대 기대 보상을 제공하는 행동을 선택하게 된다.

그 결과 에이전트는 주어진 환경 내에서 장기적인 보상을 극대화할 수 있게 되는 것이다.


## 3.1.2 벨만 방정식 도출

수익(return): ![[Pasted image 20250320204830.png|300]]
- 수익 $G_t$는 시간 $t$이후로 얻을 수 있는 보상의 총합이다.
- 할인율 $\gamma$에 따라 더 나중에 받는 보상일수록 값이 기하급수적으로 감소한다.

수익 식 정리: ![[Pasted image 20250320204936.png|300]]

정리된 식을 상태 가치 함수의 수식에 대입 후 식정리:
![[Pasted image 20250320205032.png|350]]
- 상태 가치 함수는 수익에 대한 기댓값이다. 
- 마지막 식의 전개는 기댓값의 '선형성' 덕분에 성립된다.
