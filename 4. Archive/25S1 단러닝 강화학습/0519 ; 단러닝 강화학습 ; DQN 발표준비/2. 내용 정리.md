**8.2 DQN의 핵심 기술**
- Q러닝에서는 추정치를 사용하여 추정치를 갱신한다.
- 여기에 신경망까지 사용하면 불안정+불안정 -> 엄청난 불안정
- 그래서 '경험 재생' + '목표 신경망' 기술을 사용한다.

- DQN = '벨만 방정식 기반 TD 업데이트(Q-learning의 업데이트 방식)' + '신경망' + '경험 재생' + '목표 신경망' + 'Reward Clipping'
	-  Experience Replay: 순차적 데이터 상관성을 줄여 안정적이고 효율적인 학습을 가능하게 해주는 기법
	- Target Network: 목표값 변동을 완화해 학습의 안정성을 높여주는 기법
	- Reward Clipping: 다양한 환경에 일관된 학습을 가능하게 해주는 기법

