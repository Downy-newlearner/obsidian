
## 브리핑 자료
- Q 러닝은 벨반 방정식 $Q^*(s,a) = E_{s'\sim \mathcal{E}}[r + \gamma \max_{a'}Q^*(s',a')|s,a]$을 이용하여 Q 함수를 반복적으로 업데이트하여 학습한다.
- 이 때 계산된 Q 값은 "Q-table"에 저장되며, 이 테이블을 점진적으로 수정해 나가는 과정을 통해 에이전트의 행동을 학습시킨다.

- Q 러닝의 한계: 
	- State가 많은 경우 Q-table 구축이 어렵다.(환경의 상태 공간이 매우 크거나 연속적일 경우)
	- 순차적으로 발생하는 sample data 간의 높은 상관관계 때문에, 학습이 불안정해질 수 있다.
	- Q 함수가 업데이트되는 과정에서 예측값(Q(s,a))의 목표값((r + \gamma \max_{a'}Q(s',a')))이 계속해서 변하므로 학습이 불안정해질 수 있다.


DQN은 Q-learning의 한계를 딥러닝(CNN)으로 극복한 강화학습 기법으로, 고차원 입력(이미지 등)에서 직접 Q값을 예측한다.
한 번의 네트워크 연산으로 모든 행동의 Q값을 얻어 효율적이며, 다음과 같은 핵심 기술을 사용한다.
1. Experience Replay
    - 랜덤 샘플링으로 학습 안정성과 데이터 효율성을 향상시킨다.
    - Off-policy 학습은 필수적이다.
    
2. Target Network
    - 목표 네트워크와 학습 네트워크를 분리해 학습 안정성을 확보한다.
    
3. Reward Clipping
    - 보상 값을 -1, 0, +1로 제한해 다양한 환경에서 일관된 학습이 가능하다.
    
- Q. "Experience replay를 사용하기 위해서는 학습 중인 모델의 현재 파라미터가 과거에 데이터를 생성할 때 사용한 파라미터와 다를 수 있으므로 off-policy 학습이 필수적입니다." 라는 문장 설명이 필요하다.
- A. 경험이 기록된 당시의 모델과 현재 모델이 다를 수 있다는 이야기이다.
  

- Q. Q-learning에서 Q-table을 사용하는 방식의 주요 한계점은 무엇인가요?
- A. Q-table은 상태 공간과 행동 공간의 크기가 클 경우 구축하고 관리하는 데 많은 메모리가 필요하며, 모든 상태-행동 쌍을 방문하기 어려워 비효율적입니다.

  
- Q. DQN에서 심층 신경망(DNN)은 어떤 역할을 수행하나요?
- A. 심층 신경망은 복잡하고 고차원적인 상태 공간에서 Q 함수를 효과적으로 근사화하는 역할을 합니다. raw sensory input으로부터 직접 유용한 특징을 추출하고 각 행동의 Q 값을 예측합니다.

  
- Q. DQN에서 [[경험 재생(Experience Replay)]] 기법을 사용하는 주된 이유는 무엇인가요?
- A. 경험 재생은 에이전트 경험의 순차적인 상관관계를 줄이고 데이터를 효율적으로 재활용하여 학습의 안정성과 효율성을 높이기 위해 사용됩니다.

  
- Q. DQN에서 타깃 네트워크(Target Network)는 왜 사용되며, 메인 네트워크와 어떻게 다르게 업데이트되나요?
- A. 타깃 네트워크는 학습 목표 값 계산을 안정화하기 위해 사용됩니다. *메인 네트워크*는 경사 하강법으로 계속 업데이트되지만, *타깃 네트워크*는 메인 네트워크의 가중치를 일정 주기마다 복사하여 업데이트됩니다.

  
- Q. DQN에서 보상 클리핑(Reward Clipping)의 목적은 무엇인가요?
- A. 보상 클리핑은 게임마다 다른 보상 스케일을 통일하여 다양한 환경에서 학습을 더 쉽게 만들고 에러 도함수의 스케일을 제한하기 위해 사용됩니다.

  
- Q. DQN 아키텍처에서 CNN 층은 주로 어떤 역할을 담당하나요?
- A. CNN 층은 입력으로 들어오는 raw 이미지 픽셀 데이터에서 게임 상태를 이해하는 데 필요한 공간적 특징(예: 오브젝트 위치)을 추출하는 역할을 담당합니다.

  
- Q. DQN에서 손실 함수는 무엇이며, 어떤 값을 최소화하기 위해 사용되나요?
- A. 손실 함수는 예측 Q 값과 타깃 Q 값 사이의 차이(TD 오차)의 제곱 평균입니다. 이 손실 함수를 최소화함으로써 네트워크가 실제 Q 함수를 더 잘 근사하도록 가중치를 학습합니다.

  
- Q. $\epsilon$-탐욕 정책에서 $\epsilon$ 값은 학습이 진행됨에 따라 보통 어떻게 변화하며, 그 이유는 무엇인가요?
- A. $\epsilon$ 값은 학습이 진행됨에 따라 감소합니다. 초기에는 탐험을 많이 하다가 학습이 진행될수록 현재까지 학습된 최적의 행동(활용)을 더 자주 선택하기 위함입니다.

  
- Q. Double DQN은 어떤 문제를 해결하기 위해 제안되었나요?
- A. Double DQN은 기존 DQN이 Q 값을 과대평가하는 경향을 해결하기 위해 제안되었습니다. 이는 행동 선택과 행동 가치 평가에 동일한 네트워크를 사용하기 때문에 발생하는 문제입니다.

  
- Q. Prioritized Experience Replay는 기존 경험 재생의 어떤 단점을 보완하나요?
- A. Prioritized Experience Replay는 기존 경험 재생에서 모든 경험을 균일하게 샘플링하는 비효율성을 보완합니다. TD 오차가 큰, 즉 학습에 더 유용한 경험에 더 높은 우선순위를 부여하여 샘플링합니다.

## 답할 수 있어야 하는 핵심 질문들

1. 강화 학습의 기본적인 구성 요소는 무엇인가요? 특히 상태(State), 행동(Action), 보상(Reward), 정책(Policy)의 역할은 무엇이며, 가치 함수(Value Function), 특히 상태 가치 함수(State Value Function)와 행동 가치 함수(Action-Value Function, Q 함수)는 어떻게 정의되나요?

   강화 학습은 에이전트(agent)가 환경(environment)과 상호작용하며 학습하는 방식입니다. 이 과정에서 핵심적인 구성 요소들이 있습니다.

   - **상태(State, s):** 에이전트가 특정 시점에 처한 상황 또는 환경의 표현입니다. 복잡한 환경에서는 현재 상태만으로는 부족하여 과거 상태들의 시퀀스(Sequence)를 함께 고려하기도 합니다.
   - **행동(Action, a):** 에이전트가 주어진 상태에서 선택할 수 있는 동작들의 집합 또는 개별 동작입니다. 에이전트의 행동은 환경에 영향을 미칩니다.
   - **보상(Reward, r):** 에이전트가 특정 상태에서 행동을 수행한 결과로 환경으로부터 받는 스칼라 값입니다. 장기적인 보상은 할인율(γ)을 적용하여 평가합니다.
   - **정책(Policy, π 또는 μ):** 에이전트가 주어진 상태에서 어떤 행동을 선택할지를 결정하는 전략입니다.

   **가치 함수(Value Function)**는 특정 상태 또는 상태-행동 쌍의 미래에 얻을 것으로 예상되는 총 보상(Return)의 기댓값을 나타내는 함수입니다.

   - **상태 가치 함수(State Value Function, Vπ(s)):** 정책 π를 따를 때, 특정 상태 s에서 시작하여 앞으로 얻게 될 총 보상(할인된 누적 보상, Return G)의 기댓값입니다.
   - **행동 가치 함수(Action-Value Function, Qπ(s, a) 또는 Q 함수):** 정책 π를 따를 때, 특정 상태 s에서 행동 a를 선택한 이후 앞으로 얻게 될 총 보상(Return G)의 기댓값입니다.

2. Q 함수는 왜 중요한가요? Q 함수를 통해 최적 정책(Optimal Policy)을 어떻게 찾을 수 있나요?

   Q 함수는 강화 학습에서 매우 중요합니다. 만약 우리가 최적의 Q 함수(Optimal Q Function, Q*(s, a))를 알고 있다면, 에이전트는 복잡한 탐색이나 계산 없이도 최적 정책(Optimal Policy, π*(s))을 즉시 알 수 있습니다.

   최적 정책은 Q 함수를 통해 매우 간단하게 찾을 수 있습니다. 특정 상태 s에서 에이전트가 선택할 수 있는 여러 행동들(a') 중에서 가장 높은 Q*(s, a') 값을 가지는 행동을 선택하는 것이 바로 최적 정책이 됩니다. 즉, π*(s) = argmax_a Q*(s, a) 입니다.

3. Q-learning은 무엇이며, 상태나 행동의 수가 매우 많은 복잡한 환경에 적용할 때 어떤 한계가 있나요?

   Q-learning은 모델 프리(model-free), 오프 폴리시(off-policy) 시간차(Temporal Difference, TD) 학습 알고리즘입니다. Q-table에 저장된 Q 값을 점진적으로 수정해나가면서 에이전트의 행동을 학습시킵니다. 

   하지만 Q-learning은 복잡한 환경에 적용할 때 여러 한계를 가집니다:

   - **Q-table 구축의 어려움:** 상태나 행동의 수가 많아지면 Q-table의 크기가 커져 관리가 어려워집니다.
   - **순차적 데이터의 상관관계:** 경험 데이터는 시간적으로 상관관계가 있어 학습이 불안정해질 수 있습니다.
   - **변동하는 학습 목표:** 학습 목표 값이 계속 변하며 안정성을 해칠 수 있습니다.

4. Deep Q Network (DQN)는 이러한 Q-learning의 한계를 어떻게 극복하나요? 특히, 신경망(Neural Network)이 Q 함수를 근사하는 데 어떤 역할을 하며, 고차원 입력(예: 이미지)을 처리하기 위해 어떤 종류의 신경망(예: CNN)이 사용되나요?

   DQN은 Q-learning의 한계를 극복하기 위해 신경망(Neural Network)을 사용하는 방법입니다.

   - **Q 함수의 근사:** DQN은 심층 신경망을 Q 함수 근사기로 사용합니다. 이를 통해 Q-table 없이 Q 값을 효율적으로 처리할 수 있습니다.
   - **고차원 입력 처리 및 CNN의 역할:** 고차원 시각적 입력(raw pixels)을 처리하기 위해 DQN은 합성곱 신경망(CNN)을 사용합니다. CNN은 이미지 데이터를 입력받아 필요한 공간적 특징을 추출합니다.

   DQN에서 학습의 안정성과 효율성을 높이기 위해 사용되는 기법들은 다음과 같습니다:

   - **Experience Replay:** 에이전트의 경험을 리플레이 버퍼에 저장하여 무작위로 추출하여 학습합니다.
   - **Target Network:** 학습의 안정성을 위해 두 개의 신경망을 사용하여, 목표 네트워크를 일정 주기마다 업데이트합니다.

   이 외에도 보상 클리핑(Clipping Rewards)을 사용하여 스케일을 조정하고, ϵ-greedy 정책을 사용하여 탐험과 활용의 균형을 맞춥니다.



네, 좋습니다. 이어서 DQN 발표 준비를 위해 공부 단계에서 도출했던 핵심 질문들 중 남은 네 가지 질문에 대해 현재까지 제공된 자료들을 바탕으로 답변을 정리해 드리겠습니다.

5. DQN의 학습 과정과 손실 함수는 어떻게 정의되며, Target Network와 Experience Replay는 구체적으로 어떤 방식으로 학습 안정성에 기여하나요?

   DQN의 학습 과정은 기본적으로 Q-learning의 업데이트 규칙에 심층 신경망과 안정화 기법들을 결합한 형태입니다. 제공된 자료들을 종합하여 일반적인 학습 과정을 정리하면 다음과 같습니다:

   - **초기화:** 리플레이 메모리 버퍼(D)와 Q 함수를 근사하는 신경망 (main Q network, 가중치 θ) 및 학습 목표 값 계산에 사용할 목표 네트워크(target network, 가중치 θ')를 초기 가중치로 초기화합니다.
   - **에피소드 반복:** 설...

   (이하 내용은 정리된 구조로 전개하며 불필요한 중복 및 설명은 생략하거나 요약)

   **Target Network의 기여:** Target Network는 학습 안정성에 매우 중요한 기여를 합니다. Q-learning의 손실 함수에서 목표 값(y)은 다음 상태 s'의 Q 값에 기반하여 계산됩니다. 목표 네트워크의 가중치 θ'는 일정 주기로만 메인 네트워크의 가중치를 복사하여 업데이트하여 비교적 안정적인 학습 목표를 제공합니다.

   **Experience Replay의 기여:** Experience Replay는 학습 안정성 및 효율성에 크게 기여합니다. 에이전트의 경험을 리플레이 메모리에 저장하고, 학습 시 이 버퍼에서 무작위로 미니배치를 추출하여 사용함으로써 경험 데이터의 강한 상관관계를 끊고 학습을 안정화시킵니다. 또한 데이터의 재활용으로 데이터 효율성을 높입니다.

6. ϵ-greedy 탐험 전략은 무엇이며, 강화 학습에서 탐험(Exploration)과 활용(Exploitation)의 균형이 왜 중요한가요?

   ϵ-greedy 탐험 전략은 에이전트가 행동을 선택하는 가장 일반적인 전략 중 하나입니다. 에이전트는 각 타임스텝에서 두 가지 방식 중 하나로 행동을 선택합니다:

   - **탐험 (Exploration):** 확률 ϵ로 무작위 행동을 선택합니다.
   - **활용 (Exploitation):** Q 값이 가장 높은 행동을 선택합니다.

   탐험과 활용의 균형은 매우 중요합니다. 탐험을 통해 에이전트는 새로운 정보를 얻고 환경을 전반적으로 이해하는 데 필요한 데이터를 수집할 수 있으며, 활용을 통해 에이전트는 현재 학습된 정보를 바탕으로 보상을 최적화할 수 있습니다.

7. DQN의 한계는 무엇이며, 이를 개선하기 위한 Double DQN, Prioritized Experience Replay 등의 기법은 각각 어떤 아이디어에 기반하나요?

   DQN은 몇 가지 한계를 가지며, 그 중 하나는 Q 값의 과대평가 경향입니다. 이를 개선하기 위해 Double DQN과 Prioritized Experience Replay가 제안되었습니다:

   - **Double DQN:** 최대값 연산으로 인한 과대평가 문제를 해결하기 위해 행동 선택과 가치 평가를 분리하는 기법입니다.
   - **Prioritized Experience Replay:** 경험의 중요도에 따라 우선순위를 두고 샘플링하여 학습의 효율성을 높이는 방법입니다.

8. DQN은 어떤 환경에서 주로 활용되며, Atari 게임 적용 사례에서 얻을 수 있는 시사점은 무엇인가요?

   DQN은 주로 상태 공간이 매우 크거나 고차원적인 입력(예: 이미지)을 가지는 환경에서 활용됩니다. 대표적인 사례로 Google DeepMind가 Atari 게임에서 DQN을 성공적으로 적용한 연구가 있습니다.

   시사점:
   - 고차원 Raw Input 처리 능력을 입증했습니다.
   - 범용 알고리즘의 가능성을 보여주었습니다.
   - Deep Learning과 Reinforcement Learning의 효과적인 결합을 증명했습니다.
   - 인간 수준의 성능을 달성했습니다.

이 답변들이 남은 핵심 질문들에 대한 이해를 돕는 데 도움이 되었기를 바랍니다.


