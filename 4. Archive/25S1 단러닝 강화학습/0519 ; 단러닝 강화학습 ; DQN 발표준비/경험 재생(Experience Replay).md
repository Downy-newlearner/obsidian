---
created: 
tags: 
aliases: []
reference:
---

경험 재생은 강화학습에서 에이전트가 환경과 상호작용한 경험(experience) 또는 전환(transition)을 리플레이 메모리(replay memory) 또는 리플레이 버퍼(replay buffer)라는 버퍼에 저장하고, 학습 시 이 버퍼에서 샘플링된 데이터를 사용하는 방식입니다. 특히 DQN 알고리즘의 핵심 기법 중 하나로 사용됩니다.

저장되는 경험은 에이전트가 환경과 한 번의 상호작용을 통해 얻는 정보로, 일반적으로 상태(state), 행동(action), 보상(reward), 다음 상태(next state) 등의 정보로 구성됩니다. 이 버퍼는 제한된 용량을 가지며, 새로운 경험이 추가될 때 오래된 경험은 버려지는 방식(순환 버퍼 또는 대기열 형태)으로 관리될 수 있습니다.
DQN과 같은 알고리즘에서 에이전트의 상태가 변경된 즉시 훈련을 수행하는 대신, 일정 수의 경험 샘플이 버퍼에 쌓일 때까지 기다렸다가, 저장된 경험 샘플 중에서 무작위로 미니배치를 추출하여 신경망 학습에 사용합니다. 이러한 무작위 샘플링은 균일한 방식으로 이루어질 수 있습니다.

## 주요 목적 및 장점

- **순차적인 데이터의 상관관계 문제 해결**: 강화학습을 통해 얻는 데이터는 에이전트의 연속적인 상호작용으로 인해 시간적으로 강한 상관관계를 가집니다. 이는 학습의 안정성을 저해할 수 있습니다. 경험 재생은 무작위 샘플링을 통해 데이터 간의 상관관계를 줄이고 업데이트의 분산을 감소시켜, 학습을 안정화시키는 데 도움을 줍니다.

- **데이터 분포의 비고정성 문제 완화**: 에이전트가 새로운 행동을 학습함에 따라 데이터를 생성하는 정책(policy)이 변화하고, 이에 따라 데이터의 분포도 변하게 됩니다. 경험 재생은 과거의 여러 행동에 대한 경험을 샘플링함으로써 훈련 데이터의 분포를 smoothing하고, 행동 분포가 많은 이전 상태들로부터 평균화되도록 하여 학습의 안정성을 높입니다.

- **데이터 효율성 증대**: 한 번 얻은 경험 샘플을 리플레이 메모리에 저장하고 여러 번의 학습에 재사용할 수 있습니다. 이는 경험을 얻는 즉시 한 번만 사용하는 방식보다 데이터 사용 효율을 높입니다.

- **과적합 감소**: 상관관계가 높은 경험에 신경망이 과적합되는 것을 줄일 수 있습니다.

## Off-policy 학습 필요성

경험 재생을 효과적으로 사용하기 위해서는 현재 학습 중인 모델의 파라미터와 과거 데이터를 생성할 때 사용한 파라미터가 다를 수 있기 때문에 Off-policy 학습이 필수적입니다.
경험 재생은 DQN이 Raw pixel input과 같은 고차원 감각 정보(sensory input)로부터 의미 있는 특징을 직접 추출하고 효과적으로 학습할 수 있게 하는 중요한 요소입니다.
더 나아가, 단순히 균일하게 샘플링하는 대신 학습에 더 유용하다고 판단되는 경험(예: TD 오류가 높은 경험)에 우선순위를 부여하여 샘플링하는 Prioritized experience replay와 같은 개선된 기법들도 제안되었습니다.