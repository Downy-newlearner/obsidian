---
Lecture date: 2025-01-13
tags:
  - 한권으로끝내는실전LLM파인튜닝
Part of book:
  - 1. NLP의 과거와 오늘
reference: file:///C:/Users/user/Documents/%EC%B9%B4%EC%B9%B4%EC%98%A4%ED%86%A1%20%EB%B0%9B%EC%9D%80%20%ED%8C%8C%EC%9D%BC/%ED%95%9C%20%EA%B6%8C%EC%9C%BC%EB%A1%9C%20%EB%81%9D%EB%82%B4%EB%8A%94%20%EC%8B%A4%EC%A0%84%20LLM%20%ED%8C%8C%EC%9D%B8%ED%8A%9C%EB%8B%9D.pdf
---
![[Pasted image 20250113104839.png]]


## 1.4 퍼셉트론의 등장
![[Pasted image 20250113105633.png]]
- 퍼셉트론은 인공지능 학습의 첫걸음이 되었다.
- 퍼셉트론은 *연결주의 접근법*(전통적인 경험주의 방법론, empiricist tradition theorists)을 따른다.
	- 활성화된 뉴런들 사이의 새로운 경로를 통해 정보가 저장되며, 이는 뉴런 간의 연결 강도로 표현된다.
	- 특정 자극과 반응 사이의 확률적 관계를 학습한다.

- 이런 경험에 따라 변화하는 신경망을 구축하기 위해 *통계적인 방법*을 활용했다.
	- 이는 신경망의 확률적인 특성을 고려한 것이다.
	- 무작위로 연결된 신경망이 어떻게 신뢰성 있게 작동하는지를 설명한다.

- 활성화 함수를 사용했다.
	- 신경망의 효율을 높이고 자극들 간의 유사성을 처리하는데 중요한 역할을 했다.

- 자발적 조직화
	- 예를 들어, '밥을'이라는 입력이 들어오면 '짓다', '먹다', '시키다', '주문하다' 등이 활성화되괴, '깨다', '부수다'같은 무관한 단어들은 억제된다.
	- 이런 선택적 활성화 과정은 시스템이 자발적으로 조직화하는 능력의 기초가 된다.

- '선형적 분리'라는 개념을 발견하다.
	- 자발적 조직화를 증명하기 위해 퍼셉틀론 시스템에 두 가지 서로 다른 유형의 자극을 무작위로 주는 실험을 했는데, 퍼셉트론이 이를 스스로 구분했다.(선형적 분리)
	- 선형적 분리는 주어진 데이터를 직선이나 평면 등의 단순한 기하학적 형태로 명확하게 나눌 수 있는 경우를 의미한다.


## 1.4 로젠블렛이 제시한 퍼셉트론의 한계
1. 기존 원칙에서 벗어난 완전히 다른 원칙이 필요하다.
2. 퍼셉트론은 시간적 요소를 고려하지 않았다. 그러므로 시간에 따른 패턴 인식과 같은 복잡한 자극에 대응하는 능력이 부족하다.
3. '분류'는 잘 하지만, '관계 파악'은 못한다.
4. 선형적인 문제만 풀 수 있다.


## 1.5 역전파 알고리즘과 비선형 함수의 도입
![[Pasted image 20250113112549.png]]
- XOR 문제같은 비선형 문제는 퍼셉트론이 풀지 못한다.

- 그래서 다층 퍼셉트론을 제안함
- >> 순방향 뿐만 아니라 역방향으로 조절해야한다는 것을 깨달음
- >> 그러나 미분을 함에 따라서 아무리 많은 층을 쌓아도 결국 하나의 선형 변환으로 축소되거나 0이 되어 학습되지 않음
- >> *비선형 함수 도입*
- >> 인공신경망은 복잡한 데이터 패턴을 학습하고 표현할 수 있게 됨.


## 1.6 트랜스포머의 등장
### 1990년대와 2000년대 초반
- 차원의 저주
	- 데이터의 차원이 증가할 수록 알고리즘 성능이 급락함
- 해결책: 주성분 분석, t-SNE

### 2000년대 중반부
- 기계에게 인간의 언어를 이해시키기 위한 노력을 시작했다.
- 규칙 기반 시스템을 시도하다가, 점차 통계적 방법론으로 전환됐다.

### 2013년
- Word2Vec이라는 혁신적인 단어 임베딩 기술이 등장했다.
- 단어를 벡터로 임베딩해 연산이 가능하다.

### 2014년
- GloVe가 소개됐다.
- Word2Vec의 아이디어를 확장해 전체 말뭉치의 통계 정보를 활용해 더 풍부한 단어 표현을 만들어낸다.

- 또한 RNN이 주목을 받기 시작한다.

### 2015년
- Attention 매커니즘이 소개됐다.

### 2017년
- Attention을 기반으로 한 Transformer가 소개됐다.

### 2018년
- BERT, GPT와 같은 사전 훈련된 언어 모델이 등장했다.