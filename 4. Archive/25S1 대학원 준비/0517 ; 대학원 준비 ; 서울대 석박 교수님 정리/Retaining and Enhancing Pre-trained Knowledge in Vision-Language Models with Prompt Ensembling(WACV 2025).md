**요약**

CLIP 모델과 같은 비전-언어 모델은 사전 학습을 통해 특정 작업에 대한 별도 학습 없이도 대응할 수 있는 제로샷(Zero-shot) 학습 능력을 보여주며 큰 주목을 받았습니다. 하지만 이러한 모델에 새로운 도메인 지식을 통합하면서도 제로샷 성능을 유지하는 것은 여전히 어려운 과제입니다.

  

이를 해결하기 위해 본 논문에서는 **Groupwise Prompt Ensemble(GPE)** 이라는 새로운 프롬프트 앙상블 학습 기법을 제안합니다. GPE는 다음 세 가지 핵심 전략에 기반합니다:

1. **마스킹 어텐션을 활용한 프롬프트 그룹화**로 제로샷 성능을 유지하면서 적응력을 향상.
    
2. **보조 프롬프트(auxiliary prompts)** 를 통해 기존 표현을 해치지 않고 새로운 도메인 지식을 통합.
    
3. **원래의 지식과 새로운 지식을 통합하는 앙상블 전략**으로 데이터 분포 변화에 강건한 모델을 구현.
    

  

광범위한 실험을 통해 제안한 GPE는 다양한 시나리오에서 기존 모델을 능가하며 비전-언어 모델의 적응성과 효율성에서 새로운 기준을 제시합니다.