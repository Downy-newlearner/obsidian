# 🧐 Cross-Entropy loss 란 무엇인가?

## 형식적 답

Cross-Entropy Loss는 머신러닝과 특히 딥러닝에서 분류 문제를 다룰 때 매우 중요한 손실 함수(Loss Function)입니다. 이 손실 함수는 모델이 예측한 확률 분포와 실제 레이블(정답) 간의 차이를 측정하는 데 사용됩니다.

개념 설명:
Cross-Entropy는 두 확률 분포 사이의 거리를 측정하는 지표입니다. 모델이 예측한 확률 분포와 실제 레이블이 같을수록, 즉 예측이 정확할수록 Cross-Entropy 값은 낮아집니다. 반대로 예측이 틀릴수록 값은 커집니다. 이를 통해 모델의 성능을 평가하고, 학습 과정에서 이를 최소화하는 방향으로 가중치가 업데이트됩니다.
사용 이유:
Cross-Entropy Loss는 분류 문제에서 특히 효과적입니다. 그 이유는 이 함수가 예측 확률이 실제 레이블과 가까워지도록 모델을 학습시키는 데 매우 적합하기 때문입니다. Softmax와 같은 활성화 함수와 함께 사용되어, 다중 클래스 분류 문제에서도 잘 동작합니다.

간단히 말해, Cross-Entropy Loss는 모델이 실제 정답을 얼마나 잘 예측하는지 측정하는 지표로, 이를 최소화하는 것이 모델 학습의 목표입니다.

## AI 답변

Cross-Entropy Loss는 기계학습, 특히 분류(classification) 문제에서 널리 사용되는 손실 함수(loss function)입니다. 이 손실 함수는 모델의 예측값과 실제 타깃값 사이의 차이를 측정하여 모델을 최적화하는 데 사용됩니다.

Cross-Entropy Loss는 다음과 같이 정의됩니다:

```
L = -Σ(y * log(p))
```

여기서 y는 실제 타깃값(0 또는 1), p는 모델의 예측 확률입니다.

Cross-Entropy Loss는 다음과 같은 특징을 가지고 있습니다:

1. **확률 분포 측정**: 이 손실 함수는 모델 출력과 실제 타깃 분포 사이의 유사성을 측정합니다. 모델 출력이 실제 타깃 분포와 유사할수록 손실이 작아집니다.

2. **비대칭성**: 잘못 예측한 경우에 대해 불균형적인 패널티를 부과합니다. 예를 들어, 실제 값이 1일 때 예측값이 0.1이면 손실이 크지만, 실제 값이 0일 때 예측값이 0.1이면 손실이 작습니다.

3. **대수 함수 사용**: 대수 함수를 사용하여 작은 확률에 대해 더 큰 패널티를 부과합니다. 이를 통해 모델이 확실한 예측을 하도록 유도합니다.

Cross-Entropy Loss는 이진 분류(binary classification) 뿐만 아니라 다중 클래스 분류(multi-class classification)에서도 사용될 수 있습니다. 이 경우 소프트맥스(softmax) 함수와 함께 사용되어 모든 클래스에 대한 확률의 합이 1이 되도록 합니다.

모델 학습 과정에서 Cross-Entropy Loss를 최소화하는 방향으로 모델의 가중치(weights)가 업데이트되어 모델의 성능이 향상됩니다.

