# 🧐 Optimizer의 종류와 차이에 대해서 아는가?

## 정답 링크

https://velog.io/@jhlim2993/Deep-Learning-%EA%B8%B0%EC%B4%88-%EB%82%B4%EC%9A%A9-%EC%A0%95%EB%A6%AC

## 형식적 답

optimizer (옵티마이저): gradient descent의 방법으로 파라미터를 학습할 때, 파라미터 변경 폭이 불안정한 문제를 해결하기 위해 학습 속도와 운동량을 조정하는 것.

속도를 조정하는 방법 (learning rate를 조정하는 방법)
아다그라드 (Adagrad): 파라미터의 업데이트 횟수에 따라 learning rate (이하 lr)를 조정
- 많이 변화하지 않는 변수들의 lr는 크게 하고, 많이 변화하는 변수들의 lr는 작게함.
즉 많이 변화한 파라미터는 최적 값에 근접 했을 것이라는 가정하게 작은 크기로 이동하면서 세밀하게 값을 조정
반대로 적게 변화한 파라미터들은 lr를 크게 하여 빠르게 오차 값을 줄이고자 하는 방법

아다델타 (Adadelta): 아다그라드에서 G값이 커짐에 따라 학습이 멈추는 문제를 해결하기 위해 등장한 방법
- lr에 대한 하이퍼파라미터가 필요 없음.
RMSProp: 아다그라드의 G(i)값이 무한히 커지는 것을 방지하고자 제안된 방법


운동량을 조정하는 방법
모멘텀 (Momentum): 가중치를 수정하기 전에 이전 수정 방향을 참고하여 같은 방향으로 일정한 비율만 수정하는 방법.

네스로프 모멘텀: 모멘텀 값이 적용된 지점에서 기울기 값을 계산.


속도와 운동량에 대한 혼용 방법
아담 (Adam):모멘텀과 RMSProp의 장점을 결합한 경사하강법.
- 파라미터마다 다른 학습률을 추가하면서, 이 학습률을 모멘텀의 v(i) 텀에 응용


룩어해드 옵티마이저, 아담처럼 경사하강 하고 처음 방향으로 조금 가고 다시 아담으로 하는 것

## AI 답변

안녕하세요. Optimizer는 머신러닝 및 딥러닝 모델을 학습시킬 때 사용되는 중요한 요소입니다. 딥러닝 모델의 가중치를 업데이트하는 방법을 결정하며, 이는 모델의 성능에 큰 영향을 미칩니다. 대표적인 Optimizer의 종류와 차이점은 다음과 같습니다.

1. **SGD (Stochastic Gradient Descent)**
   - 가장 기본적인 Optimizer로, 경사하강법을 기반으로 합니다.
   - 학습률(Learning Rate)이 고정되어 있어 초기 설정이 중요합니다.
   - 간단하지만 꾸준한 업데이트가 어려울 수 있습니다.

2. **Momentum**
   - SGD에 모멘텀 개념을 더한 Optimizer입니다.
   - 기울기 방향으로 가중치를 움직이면서 최적해를 향해 가속됩니다.
   - 평평한 지역에서 속도를 유지하고, 곡률이 있는 지역에서 속도를 줄입니다.

3. **AdaGrad**
   - 각 가중치 매개변수에 대한 학습률을 적응적으로 조정합니다.
   - 자주 업데이트되는 가중치는 작은 학습률을, 덜 업데이트되는 가중치는 큰 학습률을 적용합니다.
   - 그러나 장기적으로 학습률이 너무 작아지는 문제가 있습니다.

4. **RMSProp**
   - AdaGrad의 문제점을 해결하기 위해 제안되었습니다.
   - 지수 가중 이동 평균을 사용하여 가중치 업데이트의 크기를 조절합니다.
   - 장기간 학습에서도 적절한 학습률을 유지할 수 있습니다.

5. **Adam**
   - Momentum과 RMSProp의 장점을 결합한 Optimizer입니다.
   - 모멘텀과 RMSProp의 이동 평균을 동시에 사용하여 효율적인 업데이트를 수행합니다.
   - 많은 딥러닝 모델에서 좋은 성능을 보이며 가장 많이 사용되는 Optimizer 중 하나입니다.

각 Optimizer는 장단점이 있으며, 모델의 특성과 데이터에 따라 적절한 Optimizer를 선택하는 것이 중요합니다. 또한, 하이퍼파라미터 튜닝을 통해 Optimizer의 성능을 최적화할 수 있습니다.

