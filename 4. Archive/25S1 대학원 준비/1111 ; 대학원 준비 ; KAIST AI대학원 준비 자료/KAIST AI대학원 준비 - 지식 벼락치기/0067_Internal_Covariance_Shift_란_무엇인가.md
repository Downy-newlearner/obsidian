# 🧐 Internal Covariance Shift 란 무엇인가?

## 정답 링크

https://eehoeskrap.tistory.com/430

## 형식적 답

배치단위로 해도 해결 못하는 문제, 학습과정에서 계층별로 입력 데이터 분포가 달라져서 발생한 현상 배치 데이터간 분포가 상이할수 잇음 
이걸 해결하기 위해 배치 노말라이제이션에 생김

Internal Covariance Shift는 딥러닝 모델에서 신경망의 각 층에 있는 뉴런들의 출력 분포가 학습 과정 중에 변하는 현상을 의미합니다. 이 용어는 딥러닝 연구자들, 특히 Batch Normalization을 제안한 논문에서 많이 언급됩니다.

Internal Covariance Shift의 개념:
신경망 학습 과정 중 변동:

신경망의 각 층은 이전 층의 출력(즉, 입력 데이터)을 입력으로 받습니다. 그러나, 이 출력이 학습 과정에서 파라미터 업데이트에 따라 계속 변화하면, 그 결과로 신경망의 각 층에서 입력 분포가 변화하게 됩니다.
이 변화는 학습을 어렵게 만듭니다. 왜냐하면 다음 층의 가중치와 편향이 이전 층의 출력 분포에 맞추어 학습되었는데, 이 분포가 지속적으로 변동하면 학습이 불안정해지기 때문입니다.
Covariance Shift와의 차이점:

Covariance Shift: 일반적인 의미에서 Covariance Shift는 훈련 데이터와 테스트 데이터의 분포가 다를 때 발생하는 문제입니다. 이 경우, 모델이 학습한 분포와 실제로 적용해야 하는 분포가 다르므로 성능이 떨어집니다.
Internal Covariance Shift: 이는 신경망 내부에서 발생하는 문제로, 층간 입력 분포의 변화로 인한 학습 불안정성입니다. 모델의 학습 중에 일어나는 변화이므로, 학습을 더 복잡하고 느리게 만들 수 있습니다.
Internal Covariance Shift의 문제점:
학습의 불안정성: Internal Covariance Shift가 발생하면, 신경망의 각 층은 이전 층의 출력이 일정하지 않기 때문에 일관된 패턴을 학습하기 어려워집니다.
학습 속도 저하: 각 층이 적응해야 하는 출력 분포가 계속 변하기 때문에, 학습 속도가 느려지고 더 많은 학습 시간이 필요할 수 있습니다.
기울기 소실 또는 폭발: 출력 분포의 변화로 인해 기울기 소실(vanishing gradient) 또는 기울기 폭발(exploding gradient)이 발생할 가능성이 높아집니다. 이는 신경망이 깊어질수록 더 심각해질 수 있습니다.
Internal Covariance Shift를 해결하는 방법:
Batch Normalization은 Internal Covariance Shift 문제를 완화하기 위해 도입된 기법입니다.

Batch Normalization:
정의: 각 미니배치에서의 출력 분포를 정규화하여, 분포가 일정하게 유지되도록 합니다. 이를 통해 각 층의 입력 분포가 안정되므로, Internal Covariance Shift를 크게 줄일 수 있습니다.
작동 방식: 신경망의 각 층의 출력을 미니배치 수준에서 정규화한 후, 학습 가능한 스케일(scale)과 시프트(shift) 파라미터를 적용합니다. 이 과정에서 층의 출력이 표준 정규 분포를 따르게 되어, 학습이 더 안정적이고 빠르게 진행됩니다.
효과: Batch Normalization은 신경망이 더 빠르고 안정적으로 학습하도록 도와줍니다. 이는 기울기 소실 문제를 완화하고, 더 높은 학습률을 사용할 수 있게 하며, 결과적으로 더 좋은 성능을 내는 모델을 만들 수 있습니다.
요약:
Internal Covariance Shift는 신경망의 학습 과정 중에 각 층의 출력 분포가 변화하는 현상으로, 학습의 안정성과 속도를 저해할 수 있습니다. 이 문제를 해결하기 위해 Batch Normalization 같은 기법이 도입되었으며, 이는 층의 출력 분포를 정규화하여 학습을 더 빠르고 안정적으로 만들어줍니다. Internal Covariance Shift를 관리하는 것은 신경망의 성능을 향상시키고, 학습을 최적화하는 데 중요한 역할을 합니다.ㅁ

## AI 답변

Internal Covariance Shift란 딥러닝 네트워크의 훈련 과정에서 발생하는 문제점 중 하나입니다. 이는 네트워크의 층(layer)이 깊어질수록 활성화 값의 분포가 입력 데이터의 분포와 점점 더 동떨어지게 되는 현상을 말합니다.

네트워크의 초기 층에서는 입력 데이터의 분포와 비슷한 활성화 값 분포를 가지지만, 층이 깊어질수록 활성화 값의 분포가 변하게 됩니다. 이렇게 되면 가중치 업데이트 시 기울기 소실(vanishing/exploding gradients) 문제가 발생하여 네트워크 훈련이 원활히 이뤄지지 않습니다.

이러한 Internal Covariance Shift 문제를 해결하기 위해 BatchNormalization 기법이 제안되었습니다. BatchNorm은 층 사이에 정규화 과정을 추가하여 활성화 값의 분포를 일정하게 유지시켜 줌으로써 기울기 소실 문제를 완화시켜 줍니다. 이렇게 함으로써 더 깊은 네트워크를 안정적으로 훈련시킬 수 있게 되었습니다.

