# 🧐 Regularization이란?

## 형식적 답

*Regularization(정규화)**은 머신러닝에서 모델이 **과적합(Overfitting)**되는 것을 방지하기 위해 사용하는 기법입니다. Regularization은 모델의 복잡도를 제한하여, 훈련 데이터에 지나치게 맞춰지지 않도록 하고, 새로운 데이터에 대해 더 일반화된 성능을 발휘하도록 돕습니다.

Regularization의 개념:
모델이 훈련 데이터의 노이즈나 세부 패턴을 과도하게 학습하는 것을 방지하기 위해, Regularization은 모델의 손실 함수에 패널티(penalty) 항을 추가합니다. 이 패널티 항은 모델의 가중치(w) 값에 의존하며, 가중치가 커질수록 손실 함수의 값이 커지게 만들어, 모델이 복잡해지지 않도록 제어합니다.

특징: 가중치가 0이 되도록 만들어, 일부 특징을 완전히 제거(희소성(sparsity) 촉진)하여 특성 선택(feature selection)에도 유용합니다.

L1 Regularization (Lasso Regularization):
L1 Regularization은 손실 함수에 가중치 절댓값의 합을 더하는 방식입니다.
특징: 가중치가 0이 되도록 만들어, 일부 특징을 완전히 제거(희소성(sparsity) 촉진)하여 특성 선택(feature selection)에도 유용합니다.

L2 Regularization (Ridge Regularization):
L2 Regularization은 손실 함수에 가중치 제곱의 합을 더하는 방식입니다.
특징: 가중치가 0에 가깝게 줄어들게 하여, 모델이 특정 특징에 과도하게 의존하지 않도록 합니다. 대부분의 가중치를 작게 유지하는 경향이 있습니다.

Elastic Net Regularization:

Elastic Net은 L1과 L2 Regularization을 결합한 방법입니다.

희소성 측면
L1 Regularization은 많은 가중치를 0으로 만들어, 모델이 사용하지 않는 특징을 제거합니다. 결과적으로, 희소(sparse)한 모델이 만들어지며, 이는 해석이 용이하고 계산 비용이 낮습니다.
L2 Regularization은 모든 가중치를 줄이지만, 대부분의 가중치가 0이 되지 않고 작은 값을 유지합니다. 모든 특징이 모델에 기여하게 되며, 특정 특징을 완전히 무시하지는 않습니다.

해석 가능성 (Interpretability)
L1 Regularization을 사용하면 중요한 특징만 남기고 나머지를 제거하기 때문에, 결과적으로 해석 가능한 모델을 얻을 수 있습니다. 이는 특히 특징 선택이 중요한 상황에서 유용합니다.
L2 Regularization은 모든 특징이 모델에 기여하기 때문에, 해석이 L1 Regularization보다 복잡할 수 있습니다.

회귀 계수의 안정성

L1 Regularization은 데이터가 다소 변동될 때, 가중치의 변동이 클 수 있습니다. 가중치가 0으로 수렴하기 때문에 민감한 반응을 보일 수 있습니다.
L2 Regularization은 가중치가 작은 값으로 유지되기 때문에, 데이터 변동에 대해 더 안정적인 반응을 보입니다. 이는 가중치들이 서로 유사하게 유지되도록 하여, 모델의 예측이 더 부드럽고 안정적입니다.

L1 Regularization

높은 차원의 데이터에서 중요하지 않은 특징을 제거하고 싶을 때 유용합니다.
예를 들어, 텍스트 데이터에서 특정 단어들이 중요하지 않다면, L1 Regularization을 통해 불필요한 단어들을 모델에서 제외할 수 있습니다.
L2 Regularization
모든 특징을 사용해야 하지만, 과도한 가중치 값을 억제하고 싶을 때 유용합니다.
예를 들어, 경제 지표들을 모두 사용해 예측 모델을 만들 때, 특정 지표에 너무 의존하지 않도록 하기 위해 L2 Regularization을 사용할 수 있습니다.

일라스틱넷은 중간

## AI 답변

Regularization은 머신러닝 모델의 과적합(Overfitting)을 방지하기 위한 기술입니다. 과적합은 모델이 훈련 데이터에만 지나치게 최적화되어 새로운 데이터에 대한 일반화 성능이 떨어지는 현상을 말합니다.

Regularization의 핵심 아이디어는 모델의 복잡도를 제한하여 과적합을 방지하는 것입니다. 이를 위해 모델의 가중치(weight)에 페널티를 부과하여 가중치 값이 너무 커지지 않도록 하는 방식을 사용합니다. 대표적인 Regularization 기법으로는 다음과 같은 것들이 있습니다.

1. L1 규제 (Lasso 규제)
   - 가중치의 절대값 합에 비례하는 페널티를 부과하여 일부 가중치를 0으로 만듭니다. 이를 통해 특징 선택(Feature Selection)의 효과도 얻을 수 있습니다.

2. L2 규제 (Ridge 규제)
   - 가중치의 제곱 합에 비례하는 페널티를 부과하여 모든 가중치 값을 작게 만듭니다.

3. 탄력망 규제 (Elastic Net Regularization)
   - L1과 L2 규제를 혼합한 방식입니다.

4. Dropout
   - 신경망 모델에서 은닉층의 일부 노드를 무작위로 제거하여 노드 간 과적합을 방지합니다.

5. 조기 종료 (Early Stopping)
   - 검증 데이터의 손실이 증가하기 시작하면 학습을 중단하는 방법입니다.

Regularization을 적절히 적용하면 모델의 일반화 성능을 향상시킬 수 있지만, 너무 강한 정도의 규제를 적용하면 과소적합(Underfitting)의 위험이 있습니다. 따라서 하이퍼파라미터 튜닝을 통해 최적의 규제 강도를 찾는 것이 중요합니다.

