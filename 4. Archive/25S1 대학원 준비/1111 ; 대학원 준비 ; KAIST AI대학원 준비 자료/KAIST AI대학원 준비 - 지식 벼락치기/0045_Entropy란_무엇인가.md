# 🧐 Entropy란 무엇인가?

## 정답 링크

https://hyunw.kim/blog/2017/10/14/Entropy.html

## 형식적 답

정보 필요시 필요한 최소량

정리하자면, entropy란 최적의 전략 하에서 그 사건을 예측하는 데에 필요한 질문 개수를 의미합니다. 다른 표현으로는 최적의 전략 하에서 필요한 질문개수에 대한 기댓값입니다. 따라서, 이 entropy가 감소한다는 것은 우리가 그 사건을 맞히기 위해서 필요한 질문의 개수가 줄어드는 것을 의미합니다. 질문의 개수가 줄어든다는 사실은 정보량도 줄어든다는 의미입니다.


필자는 통계학에서 말하는 정보량을 “깜놀도”라고 말하고 싶다. 깜짝 놀랄만한 정도를 줄여서 말이다.

즉, 통계학에서는 놀랄만한 내용일수록 정보량이 많다고 얘기한 것이다.
이 개념은 확률의 개념을 재해석 한 것으로도 볼 수 있는데, 다시 말해 확률이 낮은 사건일 수록 정보량은 높다. 거의 일어나지 않을 일이기 때문이다.

## AI 답변

엔트로피(Entropy)는 열역학 및 정보이론에서 중요한 개념입니다. 간단히 말하면 엔트로피는 무질서 또는 불확실성의 정도를 나타냅니다.

열역학에서 엔트로피는 에너지의 무질서한 정도를 나타내는 상태함수입니다. 엔트로피가 높을수록 무질서도가 높고 유용한 에너지를 얻기 어려워집니다. 이는 열역학 제2법칙에 따라 고립계의 엔트로피는 시간이 지날수록 증가한다는 것을 의미합니다.

정보이론에서 엔트로피는 무작위성이나 불확실성의 정도를 나타냅니다. 확률분포가 균등할수록 엔트로피가 높아집니다. 예를 들어 동전을 던질 때 앞면과 뒷면이 나올 확률이 같으면 엔트로피가 최대가 됩니다. 반대로 어떤 사건이 확실할수록 엔트로피는 0에 가깝습니다.

정보이론의 엔트로피 개념은 데이터 압축, 암호화, 기계학습 등 다양한 분야에 적용되고 있습니다. 효율적인 데이터 압축을 위해서는 신호의 엔트로피를 최소화해야 합니다. 또한 엔트로피는 정보량을 측정하는 지표로 사용되기도 합니다.

