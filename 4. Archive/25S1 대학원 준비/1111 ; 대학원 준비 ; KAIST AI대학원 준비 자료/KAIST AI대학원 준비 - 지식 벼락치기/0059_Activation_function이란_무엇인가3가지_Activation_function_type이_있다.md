# 🧐 Activation function이란 무엇인가?3가지 Activation function type이 있다.

## 정답 링크

https://velog.io/@jhlim2993/Deep-Learning-%EA%B8%B0%EC%B4%88-%EB%82%B4%EC%9A%A9-%EC%A0%95%EB%A6%AC

## 형식적 답

**Activation Function(활성화 함수)**는 인공 신경망(Artificial Neural Network)에서 뉴런의 출력을 결정하는 함수입니다. 뉴런의 입력값을 받아, 이 입력값이 특정 임계치를 넘을 때 비선형성을 추가하여 출력으로 전달합니다. 활성화 함수가 없다면, 신경망은 선형 모델이 되어버려 복잡한 비선형 문제를 해결할 수 없게 됩니다.

Activation Function의 역할:
비선형성 도입: 활성화 함수는 신경망에 비선형성을 추가하여, 복잡한 데이터 패턴을 학습하고, 복잡한 문제를 해결할 수 있게 합니다.
신경망의 출력 제한: 활성화 함수는 뉴런의 출력을 특정 범위로 제한하여, 신경망이 안정적으로 학습할 수 있도록 돕습니다.

Sigmoid Function (시그모이드 함수):

정의: Sigmoid 함수는 입력값을 0과 1 사이의 값으로 압축합니다.

특징: 주로 이진 분류에서 출력층에 사용됩니다. 그러나, 시그모이드 함수는 입력값이 매우 크거나 작을 때, 출력이 0 또는 1에 가까워져서 밴싱 그래디언트(vanishing gradient) 문제가 발생할 수 있습니다.
사용 사례: 이진 분류, 출력이 확률을 나타내야 하는 경우
ReLU (Rectified Linear Unit):

정의: ReLU 함수는 입력값이 0보다 크면 그대로 출력하고, 0보다 작으면 0을 출력합니다.
특징: ReLU는 현재 가장 널리 사용되는 활성화 함수로, 계산이 간단하고, Gradient Descent를 사용하는 학습 과정에서 빠르고 효과적으로 수렴할 수 있도록 합니다. 하지만, ReLU는 음수 입력에 대해 항상 0을 출력하므로, 뉴런이 죽어버리는 Dying ReLU 문제가 발생할 수 있습니다.
사용 사례: CNN(Convolutional Neural Networks)과 같은 깊은 신경망의 은닉층

Tanh (Hyperbolic Tangent Function):

정의: Tanh 함수는 입력값을 -1과 1 사이로 압축합니다. 시그모이드 함수와 유사하지만, 출력 범위가 넓습니다.

특징: Tanh 함수는 시그모이드보다 중심이 0에 가까워지므로, 평균적으로 더 나은 성능을 보이는 경우가 많습니다. 그러나 여전히 큰 입력값에서 밴싱 그래디언트 문제가 발생할 수 있습니다.
사용 사례: 이진 분류, RNN(Recurrent Neural Networks)에서 자주 사용
추가적인 Activation Function:
Leaky ReLU: ReLU의 변형으로, 음수 입력에 대해 작은 기울기를 유지하여 Dying ReLU 문제를 완화합니다.
Softmax: 다중 클래스 분류 문제에서 출력층에 사용되며, 각 클래스의 확률을 반환합니다.

Sigmoid: 출력이 0과 1 사이로 제한되며, 주로 이진 분류 문제에 사용됩니다.
ReLU: 간단하고 계산 효율이 높으며, 현재 가장 널리 사용되는 활성화 함수입니다.
Tanh: 출력이 -1과 1 사이로 제한되며, 시그모이드보다 중심이 0에 가까워 더 나은 성능을 보이는 경우가 많습니다.

## AI 답변

활성화 함수(Activation Function)란 신경망의 노드에서 가중치 합을 특정 임계값을 기준으로 출력 신호를 결정하는 함수입니다. 활성화 함수는 비선형성을 추가하여 복잡한 매핑을 수행할 수 있게 해주며, 주요 3가지 유형은 다음과 같습니다.

1. Sigmoid 함수
   - 출력 범위가 0에서 1 사이
   - 수식: sigmoid(x) = 1 / (1 + exp(-x))
   - 장점: 뉴런이 활성화되는 정도를 0과 1 사이 값으로 표현
   - 단점: 기울기 소실 문제로 인해 학습이 불안정해질 수 있음

2. Tanh(Hyperbolic Tangent) 함수
   - 출력 범위가 -1에서 1 사이 
   - 수식: tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))
   - 장점: 0을 기준으로 양수/음수 출력이 가능하여 데이터 표현력이 높음
   - 단점: 시그모이드와 마찬가지로 기울기 소실 문제 존재

3. ReLU(Rectified Linear Unit) 함수
   - 출력이 0 또는 입력값 그대로 
   - 수식: relu(x) = max(0, x)
   - 장점: 계산 효율이 높고 기울기 소실 문제가 적음
   - 단점: 입력이 음수일 경우 0으로 출력되어 정보 손실 발생 가능

활성화 함수는 신경망 모델의 성능에 큰 영향을 미치므로 문제에 적합한 함수를 선택하는 것이 중요합니다.

