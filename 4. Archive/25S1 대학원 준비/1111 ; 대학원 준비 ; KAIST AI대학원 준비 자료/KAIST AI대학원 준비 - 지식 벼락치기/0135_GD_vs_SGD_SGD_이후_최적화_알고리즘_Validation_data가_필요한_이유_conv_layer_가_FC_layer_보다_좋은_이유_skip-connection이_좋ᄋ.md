# GD vs SGD
SGD 이후 최적화 알고리즘
Validation data가 필요한 이유
conv layer 가 FC layer 보다 좋은 이유
skip-connection이 좋은 이유
AI vs Ml vs DL
DL이 더 좋으니까 ML은 쓰지 말까요?
eigen value, eigen vector
basis 란?
다변수 함수의 convecity?

## AI 답변

1. GD(Gradient Descent) vs SGD(Stochastic Gradient Descent):
- GD: 전체 데이터셋을 사용하여 경사를 계산하고 매개변수를 업데이트합니다. 계산 비용이 크고 병렬화가 어렵습니다.
- SGD: 데이터셋에서 무작위로 선택한 일부 데이터(미니배치)만 사용하여 경사를 계산하고 매개변수를 업데이트합니다. 계산 비용이 작고 병렬화가 가능합니다.

2. SGD 이후 최적화 알고리즘:
- Momentum, Nesterov Accelerated Gradient
- Adagrad, Adadelta, RMSprop
- Adam, Nadam
- 이들은 SGD의 단점을 보완하고 수렴 속도를 높이기 위해 고안되었습니다.

3. Validation data가 필요한 이유:
- 모델의 일반화 성능을 평가하기 위해 필요합니다.
- 학습 데이터에 과적합되지 않도록 하기 위해 필요합니다.
- 하이퍼파라미터 튜닝 시 모델 선택의 기준으로 사용됩니다.

4. Conv layer가 FC layer보다 좋은 이유:
- 공간적 지역성을 활용하여 효율적인 특징 추출이 가능합니다.
- 파라미터 수가 상대적으로 적어 과적합 위험이 낮습니다.
- 평행 이동, 회전 등에 강건한 특성이 있습니다.

5. Skip-connection이 좋은 이유:
- 기울기 소실/폭발 문제를 완화시킵니다.
- 네트워크 깊이 증가에 따른 성능 저하를 방지합니다.
- 다양한 수준의 특징을 결합하여 표현력을 높입니다.

6. AI vs ML vs DL:
- AI(인공지능): 인간의 지능을 모방하는 광범위한 개념입니다.
- ML(기계학습): 데이터로부터 패턴을 학습하는 AI의 한 분야입니다.
- DL(딥러닝): 인공신경망을 기반으로 하는 ML의 한 방법론입니다.

7. DL이 더 좋으니까 ML은 쓰지 말까요?
- 아닙니다. DL이 ML의 한 방법론일 뿐입니다.
- 문제와 데이터의 특성에 따라 적절한 ML 기법을 선택해야 합니다.
- 특정 문제에서는 DL 외의 다른 ML 기법이 더 효과적일 수 있습니다.

8. Eigen value, Eigen vector:
- 행렬 A에 대해 Av = λv를 만족하는 스칼라 λ와 벡터 v가 있을 때, λ를 고유값(Eigen value), v를 고유벡터(Eigen vector)라고 합니다.
- 고유값과 고유벡터는 선형 변환의 기하학적 특성을 나타냅니다.

9. Basis:
- 벡터 공간에서 다른 모든 벡터를 선형 결합으로 표현할 수 있는 선형 독립 벡터들의 집합을 기저(Basis)라고 합니다.
- 기저를 구성하는 벡터들의 선형 결합으로 해당 공간의 모든 벡터를 표현할 수 있습니다.

10. 다변수 함수의 Convexity:
- 함수의 정의역 내의 모든 점에서 함수의 그래프가 아래로 볼록한 형태를 가질 때, 그 함수를 볼록 함수(Convex function)라고 합니다.
- 볼록 함수는 최적화 문제에서 중요한 역할을 합니다. 전역 최적점이 유일하고, 기울기 기반 방법으로 최적점을 찾을 수 있습니다.

