# 🧐 Batch Normalization은 무엇이고 왜 하는가?

## 형식적 답

그라디언트 배니싱 익스플로딩 해결 하는 방법

Internal Covariance Shift는 딥러닝 모델에서 신경망의 각 층에 있는 뉴런들의 출력 분포가 학습 과정 중에 변하는 현상을 의미합니다. 이 용어는 딥러닝 연구자들, 특히 Batch Normalization을 제안한 논문에서 많이 언급됩니다.

각 배치별로 평균과 분산을 이용해서 정규화 하는 것, 신경망 안에 포함되서 하는것임
학습단계- 배치별로 평균과 분산 구함
추론할때는 - 트레이닝할때 배치에서 들어온애들 평균들로 평균 표준편차 세팅함
무조건 0은아니고 적당한 괜찮은 위치도 학습시키게 하는 것임 안그러면 선형적이 되니 무쓸모해져서 그럼

CNN에서도 활용함

## AI 답변

Batch Normalization은 딥러닝 모델 학습 과정에서 활성화 함수 앞에서 데이터를 정규화하는 기법입니다. 이를 통해 다음과 같은 이점을 얻을 수 있습니다.

1. 내부 공변량 이동(Internal Covariate Shift) 문제 완화
- 딥러닝 모델의 층이 깊어질수록 입력 분포가 계속 변하여 수렴이 어려워지는 문제가 있습니다. 
- Batch Normalization을 통해 각 층의 입력 데이터 분포를 정규화하여 이 문제를 줄일 수 있습니다.

2. 학습 속도 향상
- 데이터 분포가 정규화되면 경사 하강법 기반 최적화가 보다 효율적으로 이루어집니다.
- 이에 따라 동일한 epochdml에서도 더 빠른 수렴이 가능합니다.

3. 과적합 완화 효과
- Batch Normalization은 입력 데이터에 일종의 노이즈를 추가하는 효과가 있습니다.
- 이를 통해 모델의 일반화 성능이 향상되어 과적합 문제를 완화할 수 있습니다.

4. 초기값 선택에 대한 민감도 감소
- 가중치나 편향 초기값이 좋지 않으면 신경망 학습에 어려움이 있습니다.
- Batch Normalization을 적용하면 초기값에 크게 영향받지 않게 됩니다.

요약하면, Batch Normalization은 내부 공변량 이동 문제를 완화하고 학습을 가속화하며 과적합 위험을 낮춰주는 효과가 있어 딥러닝 모델 성능을 크게 높일 수 있습니다.

