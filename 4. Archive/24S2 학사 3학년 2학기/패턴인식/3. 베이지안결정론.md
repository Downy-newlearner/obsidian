---
Lecture date: 2024-11-04
tags:
---
# 10월 23일 강의
Q. 베이지안 이론은 왜 필요한가?(어디에 사용하는가?)
A. 
- *알고있는 정보*(사전 확률, likelihood, normalization constant)를 이용하여 *알고싶은 정보*(후험 확률)를 구하기 위해서 필요하다.
- 다시 말해, 모든 클래스에 대해서 아래 정보들을 알고 있다면, 새로운 데이터가 들어왔을 때 어떤 클래스에 속할지 예측(합리적인 결정, optimal decision)하는 것이 가능하다.
	1. 랜덤 샘플링을 했을 때, 해당 클래스가 나올 확률(사전 확률)
	2. 해당 클래스에서 랜덤 샘플링을 했을 때 x가 나올 확률(likelihood)


Q. 베이지안 이론을 한 문장으로 정의하면 무엇인가?
A. 베이지안 이론은 사건이 일어날 확률을 알기 위해, 이미 알고 있는 정보(사전 확률)와 새로운 데이터를 결합해 확률을 갱신하는 방법이다.


![[베이지안결정론_page-0001.jpg]]

![[베이지안결정론_page-0002.jpg]]
데이터, 클래스, 그 안의 확률값들
이것을 데이터의 확률분포라고 한다.
PDF를 계산하는 것이 분석의 끝이다.
p(x)는 하나의 함수 형태인데 이것을 추정하기 위해서 우리가 잘 알고있는 함수를 이용하여 추정하는 것이다.(parametric method)
근데 근사할 수 있는 적당한 함수가 없는 경우에는 nonparametric method를 사용한다.

PDF를 구하면 확률이론에 의해서 분류기(classifier)를 만들 수 있다.
확률값을 기반으로 해서 분류기를 만든 것을 베이지안 classifier라고 한다.

*알고 있는 정보(학습에 이용하는 정보)*
1. 다이아몬드의 반짝임 정도 데이터들 $p(x|C_1)$
2. 큐빅의 반짝임 정도 데이터들 $p(x|C_2)$
3. 다이아몬드인지, 큐빅인지 모르는 무언가의 반짝임 정도 $x_{new}$

*알고 싶은 정보(인식할 정보)*
그 무언가가 다이아몬드일 확률 $p(C_1|x_{new})$
그 무언가가 큐빅일 확률 $p(C_2|x_{new})$
최종적으로 그 무언가가 무엇인지에 대한 인식 결과 $y(x_{new})$
	최종 인식 결과는 확률적으로 적절한 판단의 결과이다.(다이아와 큐빅일 확률 중 높은 것을 선택한다.)

![[베이지안결정론_page-0003.jpg]]
C1에서 x가 나올 확률, C2에서 x가 나올 확률, C3에서 x가 나올 확률을 학습한다.
새로운 $x_{new}$가 들어오면 $x_{new}$에서 C1, C2, C3가 나올 확률을 구할 수 있다.


## Bayesian Decision Theory란?
![[베이지안결정론_page-0004.jpg]]



![[베이지안결정론_page-0005.jpg]]
**베이지안 이론에서의 용어**
1. *사전 확률(Prior probability)*: $P(C_i)$

2. *Likelihood* : $P(x|C_i)$
	어떤 클래스 $C_i$에서 샘플을 수집할 때, 임의의 값 x가 나타날 확률 분포이다.

3. *후험 확률(Posterior probability): $P(C_i|x)$*
	x가 주어졌을 때 x가 Ci에 속할 확률
	베이지안 이론을 통해 얻고자 하는 '결과'

4. *정규화 상수(Normalization constant): $P(x)$*
	- $P(x)$는 결정(Decision)을 내리는데 영향을 끼치지 않는다.
		왜냐하면 결정($x$가 주어졌을 때, $x$가 어느 클래스에 속하는지에 대한 최선의 선택)은 ![[Pasted image 20241207140713.png]]
		의 상대적인 비교이고, 각 클래스마다 분모에 $P(x)$는 공통으로 가지고 있기 때문이다.
	- 정규화 상수라는 이름은 정확히 말하면 "확률을 만들기 위한 정규화 상수"이다.
		위 이유로 인해서, $x$가 어느 클래스에 속하는지 결정하기 위한 클래스간의 비교는 $P(C_i)P(x|C_i)$ 에서 발생하지만, $\Sigma P(C_i)P(x|C_i)$은 1이 아닌 $P(x)$이다.
		확률은 총 합이 1이 되어야만하므로, $P(C_i)P(x|C_i)$를 '정규화 상수'를 나누어 정규화를 해야만 우리가 집중하는 '후험 확률'이 확률의 조건을 비로소 만족할 수 있다.
	- 우도(likelihood)를 계산할 때 필요한 값이라는 관점도 괜찮다.


![[베이지안결정론_page-0006.jpg]]
[[Likelihood ratio R]]을 알아보자. ^43ddc1

**베이지안 결정**
클래스가 i, j, 두 개만 존재하는 경우에 Likelihood ratio R을 이용해 결정을 내릴 수 있다.
R이 1보다 크냐 작냐를 따지면 된다.

사실 말이 Likelihood ratio지, 후험 확률의 비이고  이는 곧 $P(C_i)P(x|C_i)$와 $P(C_j)P(x|C_j)$의 비이다.이다.

Likelihood ratio를 쓰는 경우는 클래스가 두 개인 경우이므로, 전제인 Total probability theory에 의해서 두 클래스에 대한 후험 확률의 합은 1이다.($\because$두 클래스를 합친 것이 전체이다.)
그러므로 R을 구하면 아래 두 식을 확보한 셈이다.
	1. $\frac{P(A|x)}{P(B|x)} = R$
	2. $P(A|x) + P(B|x) = 1$
두 식을 연립해서 두 후험 확률을 모두 구할 수 있다.
![[Pasted image 20241207142827.png]]

![[베이지안결정론_page-0007.jpg]]


## Continuous Densities
![[베이지안결정론_page-0008.jpg]]



### ACT 예제 1: a single feature
![[베이지안결정론_page-0009.jpg]]
학생의 성적을 보고 5년 안에 졸업을 할 수 있을지, 없을지 예측하는 것이 예제의 주제이다.

*주어진 정보*
과거 학생들 중 5년 안에 졸업한 학생들의 ACT 점수 분포
	가우시안 분포를 따름
	평균: 26
	분산: 2
$\rightarrow P(x|G) = N(26,2)$


과거 학생들 중 5년 안에 졸업하지 못한 학생들의 ACT 점수 분포
	가우시안 분포를 따름
	평균: 22
	분산: 3
$\rightarrow P(x|\bar{G}) = N(22,3)$

어떤 학생의 ACT 점수가 22점이라면 이 학생은 5년 안에 졸업할 수 있을까?

*알고싶은 정보*
$P(G|x=22)$

*주어진 정보를 통해 얻을 수 있는 정보*
$P(x=22|G)$
$P(x=22|\bar{G})$



![[베이지안결정론_page-0010.jpg]]



## Multiple Features
![[베이지안결정론_page-0011.jpg]]
멀티 피처가 되면서 싱글 피처와 달라지는 점은 x가 벡터가 되는 것이다.

### ACT 예제 2: multiple features(bivariate normal density)
![[베이지안결정론_page-0012.jpg]]

![[베이지안결정론_page-0013.jpg]]
중간고사 범위
***
# 11월 4일 강의
### 복습 북마크
여기 복습해야함
태블릿 필기 따라서 복습하기
또한, 저 시그마 행렬이 단일 변수 가우시안 분포에서 다중 변수 가우시안 분포가 될 때 생긴 것인데, 어떻게 생겨났고 작용하는지 알아내기

많은 경우에 피처가 늘어나면 성능이 늘어난다.
d개의 피처에 대한 노멀 density의 경우로 연습을 해보자.


- [x] 1차 학습: 2024-11-09
- [x] 1차 복습: 2024-12-07
- [ ] 2차 복습:
![[베이지안결정론_page-0014.jpg]]
[[d 차원 벡터]]는 d차원 위의 한 점을 가리킨다.
피처가 추가가되면 대부분의 경우의 분별력이 높아지는 경향이 있다.
피처가 d개까지 늘어나면 어떻게 되는지 확인한다.(Normal density의 예시로 확인한다.)

이 예제는 $\rho_{xy}$가 0이 아니므로, x와 y는 correlation이 있다.
만약 $\rho_{xy} = 0$이라면 x와 y는 correlation이 없는 것이고, 이 때, $\sum_G = \begin{bmatrix} \sigma_x^2 & 0 \\ 0 & \sigma_y^2 \end{bmatrix}$이 되고 이는 Diagonal Matrix라고 불린다.

Normal density에 가장 큰 연산은 [[det (행렬식, Determinant)]]와 역행렬 구하는 연산이다. 
Diagonal Matrix는 이를 쉽게 구할 수 있으므로, 계산량에서 매우 큰 이점이 존재한다.

참고로 x, y가 독립이면 서로 uncorrelated하다.
현실적으로 x, y가 독립인 경우가 거의 존재하지 않지만, correlation이 크지 않다면 독립이라고 가정해서 연산량에서 이득을 보는 쪽을 선택하곤 한다.

## Conditionally Independent Features
![[베이지안결정론_page-0015.jpg]]
![[Pasted image 20241109152440.png]]
d개의 피처가 존재하고, 각 피처가 가질 수 있는 값이 V개 그리고 k개의 클래스가 존재한다면 경우의 수는 $kV^d$이다.
하지만 각 피처가 독립이라는 전제를 세운다면 경우의 수는 $kVd$개로 대폭 감소한다.
이렇게 연산량의 차이가 존재한다.


![[베이지안결정론_page-0016.jpg]]
독립을 가정해서 연산하는 것의 단점은, 오류에 의한 Bias가 존재할 수 있다는 점이다.
그러나 계산상의 이득이 크다면 어느정도 bias는 감수할 수도 있다.

주의해야할 것은, Uncorrelated = independent가 아니라는 것이다.


![[베이지안결정론_page-0017.jpg]]
 *(a)*
 클래스 A와 B 안에서는 각각 x와 y가 서로 독립이다.
 하지만 전체로서는 x와 y는 독립이 아니다.(positively correlative)
	 $\because$전체에는 A와 B가 포함되어있기 때문이다.

*(b)*
전체로 봤을 때는 x, y는 서로 독립이지만, 클래스 안에서 봤을 때는 독립이 아니다
A안에서는 x가 커짐에 따라 y가 가질 수 있는 값의 범위가 작아진다.
반대로, B안에서는 x가 커짐에 따라 y가 가질 수 있는 값의 범위가 커진다.
하지만 전체(A, B 모두)를 봤을 때는 x, y는 서로 독립이다.


### ACT 예제 3: multiple features
예제 교훈: 피처가 늘어날 수록 더욱 정확한 예측(decision)이 가능하다.

![[베이지안결정론_page-0018.jpg]]
앞의 예제에서는 피처가 ACT 점수, 내신 점수 이렇게 두 개 였다.
이 예제에서는 에세이 점수라는 피처를 추가한다.([1~10]의 값을 가진다.)


![[베이지안결정론_page-0019.jpg]]
feature가 한 개 일 때는 졸업할 수 있는 확률을 44%로, 두 개 일 때는 10%, 이제 세 개 일 때 7%라고 예측했다.


### Detecting HIV with ELISA 예제
테스트를 2번 하는 경우이다.
각 테스트는 독립적이다.
![[베이지안결정론_page-0020.jpg]]
이전에는 한 번의 검사였고 이번에는 검사를 두 번 진행헀다.
이 각 두번의 검사는 서로 독립적이다.
두 검사가 서로 독립적이라서 이렇게 확률이 크게 뛸 수 있었다.


### x, y, z의 값에 따른 클래스 예측(decision) 예제
$\vec{X} = (0,1,4)$일 때 A, B, C 클래스 중 어느 클래스일지 예측하는 문제이다.
x, y, z는 서로 독립이므로 어렵지 않게 구할 수 있다.
x, y, z가 독립인 경우의 경우의 수와, 독립이 아닐 때 경우의 수를 계산하는 연습을 해야한다.
![[베이지안결정론_page-0021.jpg]]
샘플 19, 클래스 3개, 피처 3개
피처
	x는 0 or 1
	y는 -1 or 0 or 1
	z는 3 or 4
![[베이지안결정론_page-0022.jpg]]

## Decision Regions
![[베이지안결정론_page-0023.jpg]]
**번역**:
- **결정 영역**에 특정 특성 값 $x$를 가진 샘플이 포함되어 있으면, 해당 샘플을 그 영역에 할당된 클래스로 분류한다.
    - **결정 경계**: 결정 영역 사이의 경계
        - 최적의 결정 경계는 **특성 공간**을 결정 영역 $R_1, \ldots, R_k$로 분리한다.
        - 두 클래스 $A$와 $B$ 사이의 최적 결정 경계 계산:
            - $P(A|x) - P(B|x) \Rightarrow P(A)p(x|A) - P(B)p(x|B)$
            - x인데 A에 속해있을 확률 = A의 확률 \* A에서 x가 있을 확률
**설명**:
- **결정 영역**은 특정 클래스에 해당하는 특성 값의 범위로, 샘플의 특성 값이 어느 결정 영역에 속하는지에 따라 해당 클래스로 분류한다.
- **결정 경계**는 두 결정 영역 간의 경계를 나타내며, 클래스 간의 분리 기준 역할을 한다.
- **최적의 결정 경계**는 특성 공간을 여러 결정 영역으로 나누어, 샘플이 속할 클래스를 최적화된 방식으로 결정한다.
- **두 클래스 $A$와 $B$ 사이의 결정 경계**는 조건부 확률 $P(A|x)$와 $P(B|x)$를 통해 계산하며, 이는 주어진 $x$에 대해 $A$와 $B$ 중 어느 클래스에 속할 가능성이 높은지 판단하기 위해 사용된다.

**요약**:  
결정 경계는 특성 공간에서 각 클래스에 해당하는 영역을 분리하는 경계로, 샘플을 적절한 클래스로 분류하기 위해 사용된다.


Decision region은 피처 벨류 x를 가지고있는데 x를 가지고 있는 샘플이 decision region안에 있다면 이는 그 region안에 있다고 분류된다.
그리고 decision region을 결정하는 경계선을 decision boundary라고 한다.
sample space를 decision region들로 나누는데 이 때 오류가 가장 적게 하는 decision boundary를 optimal dicision boundary이다.

각 클래스에 속할 확률이 같게 되는 지점을 dicision boundary라고 한다.


### geiger 예측기를 통한 방사성 광물 예측 예제
![[베이지안결정론_page-0024.jpg]]
*문제*
geiger(가이거) 예측기에서 방사성 광물을 예측한다. (광물 A 또는 광물 B)
1초 동안 몇 번의 핵 분열을 하는지를 바탕으로 A 또는 B를 예측한다.
광물이 핵분열을 일으키는 상황은 푸아송 RV로 모델링할 수 있다.

A인지 B인지 모르는 광물을 가이거 예측기로 예측해봤더니 1초에 5번 핵분열을 일으킨다.
A에 대한 prior probability는 0.4로 알고있는 상황이다.

이 광물이 A일 확률은?

*풀이*
A의 Decision region이 전체 중 0.4, B는 0.6이므로 $P(A) = 0.4, P(B) = 0.6$이다.
$P(n=5|A), P(n=5|B)$는 푸아송RV의 모델로 표현할 수 있다. 

[[Likelihood ratio R]]을 두 푸아송 RV로 표현한 확률의 비로 구한 후 $P(A|n), P(B|n)$을 구할 수 있다.
	R을 굳이 사용하는 이유는 비율인 R을 구하는 과정에서 연산이 쉽게 해결돼서인 것 같다.

*참고*
[[#^43ddc1|Likelihood Ratio R]]


![[베이지안결정론_page-0025.jpg]]
DB는 후행확률이 같은 지점인데, 이 슬라이드는 후행 확률의 한 인수인 $P(C_i)$가 같은  경우이다.
	그래서 figure를 보면 $P(x|C_i)$만 비교한다.

$P(C_i|x,y) = P(C_j|x,y)$를 만족시키는 라인을 그리면 Decision Boundary이다.



![[베이지안결정론_page-0026.jpg]]
위의 예시는 $P(C_1) = P(C_2)$인 경우였고 지금 예시는 다른 경우다.
그렇다면 $P(C_1)과 P(C_2)$에 가중치가 곱해진 꼴로 생각하면 된다.


![[베이지안결정론_page-0027.jpg]]



![[베이지안결정론_page-0028.jpg]]
노말 density의 decision boundary의 모양을 봤더니 규칙적인 현상을 발견했다.
	cov 매트릭스가 어떻게 생겼냐에 따라서 $P(x|C_i)$의 모양이 달라지는데 만약 피처들이 서로 독립이라면  cov 매트릭스는 Diagonal matrix 형태를 가진다.
	![[Pasted image 20241109172054.png]]
	파란 분포 경우에는 가로방향으로 퍼져있는 것과 세로 방향으로 퍼져있는 정도가 같다.
	이는 두 피처에 대한 분산이 같다는 것이다.
	$\begin{bmatrix}\sigma_1^2 & 0 \\ 0 & \sigma_2^2\end{bmatrix} = \begin{bmatrix}\sigma_1^2 & 0 \\ 0 & \sigma_1^2\end{bmatrix} = \sigma_1\begin{bmatrix}1 & 0 \\ 0 & 1\end{bmatrix}$이다.
	
그래서 이 경우에는 decision boundary가 어떻게?
모든 클래스에 대한 cov가 동일하다면?
cov가 특수한 경우가 아닌 각각 독립적인 형태를 띈다면 어떻게 될까?
위 경우들을 다음 시간에 배운다.

# 11월 11일 강의
*복습*
디시전 바운더리를 얘기하면서 각 클래스들의 PDF가 노멀 density 형태일 때, 이를 규정하는 파라미터는 민과 covariance니까. *covariance에 따라서 두 개의 노멀 덴시티의 계형 유형이 나뉠 수 있다.*

1. 하나는 모든 클래스의 cov matrix가 identity matrix의 상수배와 동일하다면 디시전 바운더리가 어떤 특성을 보인다.

2. 또 하나는 identity matrix의 상수배의 형태는 아니지만 모든 클래스에 대한 cov matrix가 동일한 경우(이는 상관관계가 비슷하다는 거지 피쳐 값 자체가 동일하다는 의미는 아니다.) 어떤 디시전 바운더리가 특정 양상을 띈다.

3. 세 번는 조건 없이 일반적인 경우에는(cov matrix가 서로 다른 경우) 어떤지 오늘 알아볼 것이다.

위 경우들은 모두 노멀 덴시티의 경우이다.(다루기 쉬움)
노멀 덴시티를 가지는 분포들이 상당히 많이 있기 때문에 적용이 쉽게 가능하다.
노멀 덴시티를 띄지 않더라도, 노멀 덴시티 여러개를 가지고 불특정한 분포를 모델링할 수 있다.(가우시안 믹스쳐 분포)


## 가우시안 분포로 DB 구하기
이 예제는 사전확률이 같은 경우이다.
![[베이지안결정론_page-0027.jpg]]


#### 1. 모든 분포의 Covariance matrix가 동일하면서 Identity matrix의 상수배(공분산이 모두 diagonal하다.)인 경우(또한 서로 Independent)
mean과 가까운 클래스로 데이터가 decision된다.
![[베이지안결정론_page-0028.jpg]]
디시전 바운더리는 C1과 C2가 같아지는 지점을 이어 만든 구역이다.
	피겨의 두 분포를 각각 C1, C2라고 함(Class1, 2)

*이 경우에서 보장되는 것*
	$\Sigma_1 = \Sigma_2$
	$\Sigma_1^{-1} = \Sigma_2^{-1}$

*헷갈리면 안되는 것*
	Covariance matrix가 서로 같다는 것은 $C_1(x,y)$에서의 $x$와 $y$의 분포가 $C_2(x,y)$의  $x$와 $y$의 분포와 같다는 의미일 뿐, $C_1 = C_2$인 것은 아니다.

#### 2. 모든 클래스의 Covariance matrix가 같은 경우
![[베이지안결정론_page-0029.jpg]]
앞선 첫 번째 경우는 서로 독립을 가정한 경우라서 분포가 기울어지지 않았지만 이 경우는 Covariance를 가지는 경우이므로 분포가 기울어졌다.
	다만 같은 Covariance를 가진다는 가정이므로 같은 기울기로 기울었다.

*이 경우에서 보장되는 것*
	$\Sigma_1 = \Sigma_2$

*헷갈리면 안되는 것*
	$\Sigma_1^{-1} \neq \Sigma_2^{-1}$ 
		두 Covariance matrix($\Sigma$)는 Identify matrix의 상수배가 아니므로 공분산의 역행렬은 서로 다를 수 있다.

데이터 포인트가 속하는 Region을 판별할 때, 단순히 $\mu$와의 거리로 비교하면 오류가 발생할 수 있다.
	$\therefore$ "다른 measure의 거리"를 사용해야한다.

우리가 보통 사용하는 '거리'는 유클리드 거리인데 이 유클리드 거리만 '거리'인 것이 아니다.
예를 들어 맨해튼 거리가 존재한다.
	![[Pasted image 20241129163835.png|150]]

현재 경우에는 [[Mahalanobis distance]]를 사용한다.


#### 3. Covariance matrix에 어떠한 제한이 없는 경우(일반적인 경우)
![[베이지안결정론_page-0030.jpg]]
위의 첫 번째, 두 번째 경우와는 다르게 $l_i(x)$에서 소거되는 부분이 없으므로 Decision boundary 구하면 이차식으로 정리된다.
	즉 곡선 형태의 Decision boundary가 발생한다.



### 가우시안 분포 DB 구하는 예제
bivariate normal classes
각 분포에서 두 변수 x, y는 서로 독립이다.
![[베이지안결정론_page-0031.jpg]]

![[베이지안결정론_page-0032.jpg]]

![[베이지안결정론_page-0033.jpg]]
한 번 D를 만들어놓으면 D의 부호만 보고 classification이 가능하므로 편리하다


### ACT 예제
![[베이지안결정론_page-0034.jpg]]

![[베이지안결정론_page-0035.jpg]]



### ACT 예제 2: the same covariance matrix
두 클래스의 covariance matrix가 같을 때와 다를 때 contour가 어떻게 형성되는지 확인하기 위한 예제이다.
![[베이지안결정론_page-0036.jpg]]
figure 1은 앞의 예제고, figure 2가 지금 상황이다.
cov matrix가 같아서 DB 식 정리중에 이차식은 모두 소거되어 직선 DB가 나온다.

### Normal density class 한 쌍에 대한 DB 형태의 경우의 수 총정리
![[베이지안결정론_page-0037.jpg]]
DB는 맥시멈으로 나올 수 있는 항이 x^2, y^2, xy, x, y, 상수항




## d-dimensional Decision Boundaries in Matrix Notation
![[베이지안결정론_page-0038.jpg]]

![[베이지안결정론_page-0039.jpg]]

![[베이지안결정론_page-0040.jpg]]



### 같은 cov matrices를 갖는 경우 DB 구하는 예제
이전 슬라이드에서의의 A가 소거되므로 초평면으로 DB가 형성된다.

![[베이지안결정론_page-0041.jpg]]

## Unequal Costs of Errors
보상과 패널티(로스)를 반영하여 total risk를 가장 적게 하는 Decision을 한다.
![[베이지안결정론_page-0042.jpg]]

### Loss를 반영하여 Decision하기 예제 1
![[베이지안결정론_page-0043.jpg]]


![[베이지안결정론_page-0044.jpg]]


![[베이지안결정론_page-0045.jpg]]


### Loss를 반영하여 Decision하기 예제 2
![[베이지안결정론_page-0046.jpg]]

![[베이지안결정론_page-0047.jpg]]

## Estimation of Error Rates
![[베이지안결정론_page-0048.jpg]]

![[베이지안결정론_page-0049.jpg]]

![[베이지안결정론_page-0050.jpg]]

![[베이지안결정론_page-0051.jpg]]

## Simple Counting
![[베이지안결정론_page-0052.jpg]]
Simple counting은 그저 $P(\epsilon)$을 추정하는 방법일 뿐이다.
$P(\epsilon)$을 정확히 구하기 위해서는 ![[Pasted image 20241209141456.png|90]] 를 정확히 알아야한다.
또한 이 적분도 어렵기 때문에 정확히 구하기가 어려운 경우가 많다.

*장점*
PDF를 몰라도 사용할 수 있는 방법이다.





**Summary**
베이지안 클래시파이어를 살펴봤다
이는 기본적으로 각 클래스마다의 PDF 정보를 바탕으로, 확률적으로 Decision을 내리는 것이다.
conditional probability density를 자세히 아는 것이 어려워 estimation을 한다.
	1. 파라메트릭 방법
	2. 논파라메트릭 방법

파라메트릭 방법은 원래 true PDF가 있는데, 이를 어떤 잘 알려진 함수를 이용하여 근사하는 것이다. 주로 Normal Density를 이용한다.
Normal Density를 규정하는 값이 mean, cov matrix라는 파라미터이므로, 파라메트릭 방법이라고 부른다.
추정을 마치고, 베이지안 룰을 이용해 클래시파이한다.
분류의 기준이 되는 선들을 DB라고 하고, DB가 세워졌다는 것은 분류를 할 수 있는 판별식이 만들어졌다. 즉 분류가 가능하다는 것이다.

뒤 쪽에서는 맞고 틀리고만 반영하는 것이 아니라, 코스트까지 반영을해서 decision을 내릴 수 있다는 것을 알아봤다.
만들어진 모델이 있으면 error rate는 어떻게 구하는가 까지 알아봤다.