
seq2seq는 컨텍스트 벡터와 인코더&디코더 구조를 처음 도입했다.

어텐션

==임베딩은 어떤 방식으로 시키냐?(12페이지?)
임베딩 후 유사도 검사하는 방법==


## 인코더
쿼리 키 벨류
검색어 검색 - 결과 검색 - 결과의 실제값

인코더에서 포지션 정보가 포함된 임베딩 벡터를 copy하여 세 번 값을 넣어주고, 그것들이 각각 쿼리 키 벨류의 역할을 한다.

행렬 메트릭스는 유사도 검사의 결과이다.
$QK^T$
	행렬 메트릭스를 구하는 과정에서 차원이 커질수록 값이 커지므로 $sqrt(d_k)$를 나누어 스케일링을 진행한다.

유사도 검사 -> 스케일링 -> 소프트 맥스

멀티 헤드 어텐션
	헤드를 쪼개는 이유는?
		==문장에 어떤 부분에 집중하냐를 기준으로 헤드를 쪼갠다.
		예를 들어 문장 성분에 집중, 명사에 집중, 동사에 집중 하는 등 다른 성분에 집중하는 헤드를 여러 개 둔다.

ADD&Norm은 잔차연결이다.

Feed Forward는 ReLU이다.

여기까지 인코딩을 N번 반복한 후 디코딩 과정으로 넘어간다.
이걸 어떻게 N번 수행?? 
처음 인풋이 문장이 나뉘는데 결과는 어떻게 나오길래 이게 되지?
	입력과 출력 형식이 같아야 가능한 것 아닌가?

==인코더에서 수행하는 마스크는 무엇인가?==


==N번 반복한 이유 찾아보기==


## 디코더
두 번쨰 어텐션에서는 인코더 아웃풋(Key, value)과 디코더의 아웃풋(첫 어텐션 결과)(Query)를 이용한다.
	==인코더의 아웃풋이 동일하게 키와 밸류로서 들어가는 것인가? 아니면 애초에 다른 아웃풋 2개가 존재하는 것인가?==


소스 마스킹은 문장 전체에 대한 마스킹

타겟 마스킹은 다음 토큰을 예측하는 데에 쓰이는 마스킹

패딩은 길이를 맞춰줌.. -> ==무엇의 길이?==

인코딩은 다른 관계들과 관계를 다 알아야하지만, 디코딩에서는 미래 단어가 현재 단어에 영향을 주면 안된다.(번역기의 역할에 모순)