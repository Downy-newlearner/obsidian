![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0001.jpg]]

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0002.jpg]]

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0003.jpg]]

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0004.jpg]]
클래스별로 다른 가중치때문에, 오히려 데이터 수가 적은(가중치가 큰) 데이터에 오버피팅 되는 경우가 많이 발생한다는 단점이 있다.

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0005.jpg]]
[[Focal Loss]]
confidence높 -> 가중치 줄임
confidence낮(긴가민가) -> 가중치 늘림

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0006.jpg]]

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0007.jpg]]
CNN 아키텍처
	Representation
		feature extraction을 통해 특징을 잘 표현한 feature map을 만든다.
	Classifier
		feature map을 flatten하여 올바른 dicision boundary를 만든다.
두 가지 과정이 있기 때문에, CNN 성능이 좋거나 나쁜 이유가 둘 중에 무엇인지 알기가 힘들다.
그래서 실험은 feature extraction의 4가지 방법과, Classifying의 4가지 방법 총 16가지 방법의 성능 체크를 통해 무엇이 어떻게 영향을 끼치는지 확인한다. 

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0008.jpg]]
focal loss는 hyperparameter가 있는데, 이를 항상 조절하면서 학습할 수 없다.
대신 다양한 샘플링 전략을 사용한다.
	C1, C2, C3가 있다고 가정하자.
		C3 데이터 수가 가장 적음
		데이터 수가 많은 클래스가 샘플링에서 계속 배치에 포함됨
		그래서 트리 구조를 이용해서 잘 섞이게 하는 전략도 있지만 가장 쉬운 전략은 데이터 수가 적은 클래스의 데이터를 계속 중복해서 배치에 추가시키는 것이다.


![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0009.jpg]]

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0010.jpg]]
$p_j$는 class j에서 data가 추출될 확률이다.

## Sampling Strategy 4가지(Representation에서의 Dat)
- Instance-balanced sampling
	- 각 sample은 속한 class와 관계없이 추출될 확률이 동일하다.(q=1)
	- 그러므로, class j에 속하는 data가 sample로 추출될 확률은 해당 class의 data 개수 $n_j$에 비례한다.
	- 예를 들어 $C_1$에 데이터 10개, $C_2$에 데이터 5개가 있다면 각 샘플링마다 $C_1$의 데이터가 뽑힐 확률은 10/15이고, $C_2$는 5/15이다.

- Class-balanced sampling
	- 클래스 별로 같은 수의 sample을 뽑는다.(q=0)
	- 위의 예시에서는 각 클래스 별로 7개의 데이터를 뽑는다고 하면 $C_1$에서는 10개 중에 7개가 뽑히고, $C_2$에서는 5개 내에서 중복이 되더라도 샘플링하는 것이다.(일반적으로 복원 추출, $C_1$도 마찬가지로 중복이 나올 수 있음)
	- 즉, 데이터 개수가 적은 class에서는 같은 데이터라도 계속 뽑힐 수 있다.

- Square-root Sampling
	- q=1/2
	- 위 두 방법의 중간 방법이다.

- Progressively-balanced Sampling
	- 에폭에 영향을 받게 하여 샘플링하는 방법이다.
	- 초반 Epoch에서는 instanced-balanced sampling을 하다가, 점차 학습이 진행되면서 class-balanced sampling으로 변해가는 방법이다.

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0011.jpg]]

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0012.jpg]]

## Classification Method
![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0013.jpg]]
*$\tau$-normalized classifier*
샘플이 많을 수록 norm이 적어짐을 알았다
-> 학습을 하지 않고 정규화를 잘 하면 되지 않을까 라는 아이디어로 시작(다음 슬라이드에 계속)
![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0014.jpg]]
[[L2-Norm]]
no scaling?
![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0015.jpg]]

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0016.jpg]]

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0017.jpg]]

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0018.jpg]]
그 말인 즉슨
![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0019.jpg]]

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0020.jpg]]

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0021.jpg]]

*Question*
1. 만약 fe학습을 하게 되면 로스는?
	흔히 하듯 학습진행, classifier 버리고 ?만 프리징
Feature Extraction와 classifier?

2. 복잡한 로스, 메모리 유닛 사용 방법론 vs 논문에서 제시한 방법론
	-> 도메인 adaptation에서 사용하는 것과 비슷?

기존의 방법들을 잘 조합해서 성능을 향상시킨 걸 보여주는 케이스이다.