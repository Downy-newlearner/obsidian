![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0001.jpg]]

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0002.jpg]]

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0003.jpg]]

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0004.jpg]]
클래스별로 다른 가중치때문에, 오히려 데이터 수가 적은(가중치가 큰) 데이터에 오버피팅 되는 경우가 많이 발생한다는 단점이 있다.

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0005.jpg]]
[[Focal Loss]]
confidence높 -> 가중치 줄임
confidence낮(긴가민가) -> 가중치 늘림

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0006.jpg]]

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0007.jpg]]
CNN 아키텍처
	Representation
		feature extraction을 통해 특징을 잘 표현한 feature map을 만든다.
	Classifier
		feature map을 flatten하여 올바른 dicision boundary를 만든다.
두 가지 과정이 있기 때문에, CNN 성능이 좋거나 나쁜 이유가 둘 중에 무엇인지 알기가 힘들다.
그래서 실험은 feature extraction의 4가지 방법과, Classifying의 4가지 방법 총 16가지 방법의 성능 체크를 통해 무엇이 어떻게 영향을 끼치는지 확인한다. 

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0008.jpg]]
focal loss는 hyperparameter가 있는데, 이를 항상 조절하면서 학습할 수 없다.
대신 다양한 샘플링 전략을 사용한다.
	C1, C2, C3가 있다고 가정하자.
		C3 데이터 수가 가장 적음
		데이터 수가 많은 클래스가 샘플링에서 계속 배치에 포함됨
		그래서 트리 구조를 이용해서 잘 섞이게 하는 전략도 있지만 가장 쉬운 전략은 데이터 수가 적은 클래스의 데이터를 계속 중복해서 배치에 추가시키는 것이다.


![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0009.jpg]]

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0010.jpg]]
$p_j$는 class j에서 data가 추출될 확률이다.

## Sampling Strategy 4가지
- Instance-balanced sampling
	- 각 sample은 속한 class와 관계없이 추출될 확률이 동일하다.(q=1_)
![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0011.jpg]]

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0012.jpg]]

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0013.jpg]]
*$\tau$-normalized classifier*
샘플이 많을 수록 norm이 적어짐을 알았다
-> 학습을 하지 않고 정규화를 잘 하면 되지 않을까 라는 아이디어로 시작(다음 슬라이드에 계속)
![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0014.jpg]]
[[L2-Norm]]
no scaling?
![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0015.jpg]]

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0016.jpg]]

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0017.jpg]]

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0018.jpg]]
그 말인 즉슨
![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0019.jpg]]

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0020.jpg]]

![[세미나-이정호_Decoupling representation and classifier for long-tailed recognition_page-0021.jpg]]

*Question*
1. 만약 fe학습을 하게 되면 로스는?
	흔히 하듯 학습진행, classifier 버리고 ?만 프리징
Feature Extraction와 classifier?

2. 복잡한 로스, 메모리 유닛 사용 방법론 vs 논문에서 제시한 방법론
	-> 도메인 adaptation에서 사용하는 것과 비슷?

기존의 방법들을 잘 조합해서 성능을 향상시킨 걸 보여주는 케이스이다.