ResNet(Residual Network)은 딥러닝에서 매우 인기 있는 신경망 아키텍처로, 깊은 네트워크의 학습 문제를 해결하기 위해 "[[잔차 연결]](residual connection)"을 도입한 것이 특징입니다. 

깊이가 깊어질수록 신경망이 학습하기 어려워지는데, 이를 해결하기 위해 ResNet은 입력 값을 일부 출력과 더하는 "[[스킵 연결]](skip connection)"을 통해 모델이 더 쉽게 학습할 수 있도록 합니다. 

이 구조 덕분에 ResNet은 매우 깊은 네트워크에서도 효율적으로 학습을 수행할 수 있으며, 이미지 인식, 분류, 객체 탐지 등의 컴퓨터 비전 과제에서 좋은 성능을 보입니다.

## ResNet의 성능이 좋은 이유
ResNet의 성능이 좋은 이유는 다음과 같은 몇 가지 핵심적인 특징 덕분입니다:

1. **잔차 학습**: ResNet의 주요 아이디어는 층을 통해 학습하려는 것을 원래 출력과의 차이, 즉 "잔차(residual)"로 정의하는 것입니다. 이 방식으로 신경망은 잔차만 학습하게 되어 더 깊은 네트워크에서 효과적으로 학습할 수 있습니다. 깊이가 깊을수록 학습이 어려워지는 문제를 해결하여, 매우 깊은 네트워크에서도 성능이 안정적으로 유지됩니다.

2. **스킵 연결**: 스킵 연결은 입력을 중간 층을 건너뛰어 다음 층으로 직접 전달해주는 구조로, 기울기 소실 문제를 완화하고 정보 손실을 최소화합니다. 이를 통해 더 많은 층을 추가해도 네트워크가 오히려 학습 성능을 유지하거나 개선할 수 있습니다. 이러한 연결은 특히 깊은 층을 효과적으로 학습하도록 돕습니다.

3. **효율적인 파라미터 사용**: 잔차 연결 덕분에 ResNet은 네트워크를 깊게 쌓아도 불필요하게 많은 파라미터를 사용하지 않고도 높은 성능을 낼 수 있습니다. 이는 일반적인 깊은 네트워크보다 더 효율적이며, 학습 속도도 상대적으로 빠릅니다.

4. **일반화 성능**: ResNet은 다양한 데이터셋에서 좋은 성능을 보이며, 전이 학습(transfer learning)에도 강합니다. 이는 모델이 깊은 구조에서 잘 학습된 특징을 유지하면서도 새로운 데이터셋에 적응하기 용이하게 만들어 줍니다.

이러한 이유들 덕분에 ResNet은 이미지 인식, 분류, 탐지 등의 컴퓨터 비전 문제에서 탁월한 성능을 발휘하며, 현재 딥러닝에서 가장 널리 쓰이는 아키텍처 중 하나입니다.

## 사용 예시
[[딥페이크 디텍터- ResNet50]]
(공부하면서 추가 예정)