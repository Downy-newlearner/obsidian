[[1권- 파이썬으로 익히는 딥러닝 이론과 구현]]
  
#### 밑바닥부터 시작하는 딥러닝
|책 이름|챕터/날짜|이름|설명|
|---|---|---|---|
|1권|1|[[아나콘다]]|데이터 분석에 중점을 둔 배포판|
|1권|1|[[공백문자]]|들여쓰기에 탭을 쓰는 경우도 있지만 4개의 공백문자를 권장한다.|
|1권|1|[[넘파이의 산술연산]]|원소 수가 같은 같은 차원의 넘파이 배열에 대해서 연산을 수행하면, 연산은 원소별로 수행된다.(element-wise) 또는 넘파이 배열과 수치 하나(스칼라 값)의 조합으로 된 산술 연산도 수행할 수 있다.(브로드캐스트)|
|1권|1|[[넘파이 - shape]]|배열의 형상, N차원 배열에서 각 차원의 크기|
|1권|1|[[배포판]]|사용자가 설치를 한 번에 수행할 수 있도록 필요한 라이브러리 등을 하나로 정리해둔 것|
|1권|2|[[퍼셉트론]]|다수의 신호를 입력으로 받아 하나의 신호를 출력한다. 출력은 1 또는 0이다. 인공 뉴런, 단순 퍼셉트론이라고도 부른다.|
|1권|3|[[reshape()]]|원하는 형상을 인수로 지정하여 넘파이 배열의 형상을 바꿀 수 있다.|
|1권|3|[[활성화 함수]]|activation function. 입력 신호의 총합을 출력 신호로 변환하는 함수. 입력 신호의 총합이 활성화를 일으키는지를 정하는 역할을 한다.|
|1권|3|[[배치]]|하나로 묶은 입력 데이터. 지폐 다발처럼 묶여있는 것을 상상하면 된다. 배치 처리는 컴퓨터로 계산할 때 큰 이점을 준다. 왜냐하면 수치 계산 라이브러리 대부분이 큰 배열을 효율적으로 처리할 수 있도록 고도화되어있기 때문이다. 또한 느린 IO를 통한 데이터 읽기 횟수가 줄어 효율이 높다.|
|1권|3|[[비선형 함수]]|말그대로 선형이 아닌 함수|
|1권|3|[[데이터 백색화]]|data whitening, 전체 데이터를 균일하게 분포시키는 것|
|1권|3|[[다층 퍼셉트론]]|신경망을 가리킨다.|
|1권|3|[[Image.fromarray()]]|넘파이로 저장된 이미지 데이터를 PIL용 데이터 객체로 변환한다.|
|1권|3|[[A.shape]]|배열의 형상을 나타내는 인스턴스 변수|
|1권|3|[[np.dot(A, B)]]|행렬 A, B의 곱을 리턴|
|1권|3|[[ReLU 함수]]|Rectified Linear Unit, 입력이 0을 넘으면 그 입력을 그대로 출력하고, 0 이하이면 0을 출력한다.|
|1권|3|[[단순 퍼셉트론]]|단층 네트워크에서 계단 함수(임계값을 경계로 출력이 바뀌는 함수)를 활성화 함수로 사용한 모델을 가리킨다.|
|1권|3|[[np.ndim()]]|배열의 차원 수 확인|
|1권|3|[[pickle]]|프로그램 실행 중에 특정 객체를 파일로 저장하는 기능이다. 저장해둔 pickle 파일을 로드하면 실행 당시의 객체를 즉시 복원할 수 있다. 그래서 MNIST 데이터셋의 load_mnist() 함수에서도 2번째 이후 읽기에서 pickle을 통해 데이터를 순식간에 준비할 수 있다.|
|1권|3|[[신경망]]|여러 층으로 구성되고 시그모이드 함수 등의 매끈한 활성화 함수를 사용하는 네트워크|
|1권|3|[[전처리]]|신경망의 입력 데이터에 특정 변환을 가하는 것|
|1권|3|[[정규화]]|데이터를 특정 범위로 변환하는 처리|
|1권|3|[[선형 함수]]|함수의 출력이 입력의 상수배만큼 변하는 함수|
|1권|3|[[PIL]]|파이썬 이미지 라이브러리|
|1권|4|[[손실 함수]]|신경망이 학습할 수 있도록 해주는 지표. 손실함수의 결괏값을 가장 작게 만드는 가중치 매개변수를 찾는 것이 학습의 목표이다.|
|1권|4|[[고원]]|복잡하고 찌그러진 모양의 함수에서 발생할 수 있는, 학습이 진행되지 않는 정체기|
|1권|4|[[학습]]|훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것|
|1권|4|[[중앙 차분]]|central difference, 이 방법은 h값이 작을 경우에도 비교적 높은 정확도를 유지합니다.|
|1권|4|[[에폭]]|epoch, 하나의 단위로, 1에폭은 학습에서 훈련 데이터를 모두 소진했을 때의 횟수에 해당한다. 예컨데, 훈련 데이터 10000개를 100개의 미니배치로 학습할 경우, 확률적 경사 하강법을 100회 반복하면 모든 훈련 데이터를 ‘소진’한 게 된다. 이 경우 100회가 1에폭이 된다.|
|1권|4|[[안장점]]|Saddle point. 어느 방향에서 보면 극댓값이고, 다른 방향에서 보면 극솟값이 되는 점.|
|1권|4|[[np.random.choice(train_size, batch_size)]]|미니 배치를 만들 때 사용되는 넘파이 함수. train 데이터에서 batch 사이즈만큼 데이터를 뽑는다.|
|1권|4|[[np.zeros_like()]]|형상이 같고 원소가 모두 0인 배열을 만든다.|
|1권|4|[[원-핫 인코딩]]|한 원소만 1로 하고 그 외는 0으로 나타내는 표기법|
|1권|4|[[경사법]]|gradient method, 기계학습을 최적화하는 데 쓰는 방법이다. 특히 신경망 학습에서 많이 사용한다.|
|1권|4|[[오차역전파법]]|기울기 계산을 고속으로 수행할 수 있다. 수치 계산을 사용할 때와 거의 같은 결과를 훨씬 빠르게 얻을 수 있다.|
|1권|4|[[특징]]|feature, 입력 데이터에서 본질적인 데이터(중요한 데이터)를 정확하게 추출할 수 있도록 설계된 변환기를 가리킨다. SIFT, SURF, HOG 등의 특징이 존재한다. 적절한 특징을 설계해서 사용해야 좋은 결과를 얻을 수 있다.|
|1권|4|[[전진 차분]]|forward difference, \[ f'(x) \approx \frac{f(x + h) - f(x)}{h} \]|
|1권|4|[[수치 미분]]|numerical differentiation, 함수를 수학적으로 미분하는 대신, 함수의 값들로부터 차분 방정식을 사용하여 근사적으로 도함수를 구하는 기법입니다. 이 방법은 특히 함수의 명확한 표현이 없거나 복잡한 경우에 유용합니다.|
|1권|5|[[기울기 확인]]|구현이 복잡한 오차역전파법에 버그가 숨어있지 않은지 구현이 쉬운 수치 미분을 통해 확인하는 방법|
|1권|5|[[연쇄법칙]]|미분에서 사용되는 기본적인 법칙으로, 합성함수의 미분을 계산하는 방법이다. 딥러닝에서는 오차역전파법(backpropagation)에서 가중치 업데이트를 위해 사용됩니다. 각 층의 출력이 다음 층의 입력으로 들어가기 때문에, 이 과정을 통해 전체 네트워크의 가중치에 대한 손실함수의 기울기를 효율적으로 계산할 수 있습니다.|
|1권|5|[[활성화 함수 계층]]||
|1권|5|[[Fully Connected Layer]]|신경망의 한 계층으로, 이전 층의 모든 뉴런이 현재 층의 각 뉴런과 연결되어 있는 구조를 지닌다. 이 계층은 입력 데이터의 모든 특성을 고려하여 가중치와 편향을 학습하여 출력값을 생성한다. FC Layer에서는 각 뉴런이 모든 입력값에 대해 선형 결합을 수행한 후 활성화 함수를 적용하여 비선형성을 추가한다.|
|1권|5|[[Affine-Softmax 계층]]|Affine/Softmax 계층은 주로 분류 문제를 해결하기 위해 신경망의 마지막에서 사용되는 두 가지 계층이다. 페이지 참고|
|1권|5|[[오차역전파법 2]]|backward propagation of errors, 오차를 역으로 전파하는 방법. 수치 미분보다 빠르게 기울기를 구한다.|
|1권|5|[[Sigmoid 함수]]|주로 이진 분류 문제에서 사용되는 활성화 함수 $\sigma(x) = \frac{1}{1 + e^{-x}}$|
|1권|5|[[활성화 함수와 손실 함수의 차이]]|페이지 참고|
|1권|5|[[순전파]]|계산을 왼쪽에서 오른쪽으로 진행하는 단계|
|1권|5|[[softmax 함수]]|다중 클래스 분류 문제에서 사용되는 활성화 함수로, 입력된 값을 확률 분포로 변환하는 역할을 한다.|
|1권|5|[[Dropout]]|Fully Connected Layer의 반대 개념. 학습 과정에서 일부 뉴런을 임의로 끄는 방식으로, 과적합을 방지하기 위해 사용된다.|
|1권|5|[[Affine]]|신경망에서 입력을 선형 변환한 후 편향을 추가하는 계층을 의미한다. 이 계층은 입력 데이터를 변형하여 다음 계층으로 전달하고, 주로 Fully Connected Layer(완전 연결 층)에서 사용된다. Affine 계층은 가중치와 편향을 학습하여 입력 데이터의 특징을 강조하는 역할을 한다.|
|1권|5|[[국소적]]|자신과 직접 관계된 작은 범위. 계산 그래프의 특징은 ‘국소적 계산’을 전파함으로써 최종 결과를 얻는다.|
|1권|6|[[AdaGrad]]|매개변수별로 학습률을 조정하여 적용하는 최적화 기법|
|1권|6|[[Adam]]|모멘텀 + AdaGrad, 둘의 장점을 합친 최적화 기법|
|1권|6|[[비등방성 함수]]|anisotropy function, 방향에 따라 기울기가 달라지는 함수. 확률적 경사 하강법을 이 함수에 적용하면 비효율적인 움직임을 보이므로, 다른 기법을 사용해야한다.|
|1권|6|[[모멘텀]]|일정한 방향을 갖는 변수에 대해서는 가속을 붙이고, 일정하지 않는(오락가락) 변수에 대해서는 가속을 하지 않아 비등방성 함수에 유리한 최적화 기법|
|1권|6|[[최적화]]|학습을 진행하며 매개변수의 최적값을 찾는 것.|