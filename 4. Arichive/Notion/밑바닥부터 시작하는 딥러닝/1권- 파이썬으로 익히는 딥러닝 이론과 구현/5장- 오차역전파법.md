## //Sigmoid, Softmax 차이
### 1. Sigmoid
![[Source/Untitled 2.png|Untitled 2.png]]
- 활성화 함수
**특징**
- 출력값이 항상 0~1 사이에 위치하는 함수이므로, 확률로 해석하기 적합하다.
- 비선형성을 추가하여 복잡한 패턴을 신경망에 학습시킬 수 있도록 돕는다.
- 모든 점에서 미분 가능하여, 역전파 알고리즘 적용에 적합하다.
  
**단점**
- 기울기 소실 문제
    - 입력값이 너무 크거나 작으면 미분계수가 0에 가까워져서 학습 속도가 느려질 수 있다.
- 출력 비대칭
    - 출력 값이 0.5를 중심으로 수렴하므로, 예측 이탈 발생 가능성이 있다.
### 2. Softmax
- 마찬가지로 활성화 함수
- 하지만 일반적으로 마지막 노드에서만 사용된다. (다중 클래스 분류 문제에서)
- 그 이유는, Softmax가 입력 값들을 확률로 변환하여 여러 클래스 중 하나를 선택하게 하는 함수이기 때문이다.
$\sigma(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}$
  
  
  
## 계산 그래프
![[Source/Untitled 15.png|Untitled 15.png]]
  
### 계산 그래프의 이점
1. 국소적 계산
2. 중간 계산 결과를 모두 보관할 수 있음
3. 계산 그래프에서 역전파를 통해 ‘미분’을 효율적으로 계산할 수 있음
  
## 계산 그래프의 역전파
![[Source/Untitled 1 9.png|Untitled 1 9.png]]
  
## 연쇄법칙
- 국소적 미분을 전달하는 원리
  
![[Source/Untitled 2 7.png|Untitled 2 7.png]]
![[Source/Untitled 3 7.png|Untitled 3 7.png]]
$\frac{\partial z}{\partial z}\frac{\partial z}{\partial t}\frac{\partial t}{\partial x}=\frac{\partial z}{\partial x}$﻿ 연쇄법칙 덕분에 훨씬 빠르게 계산할 수 있다.`
  
## 덧셈 노드의 역전파
![[Source/Untitled 4 6.png|Untitled 4 6.png]]
- 덧셈에 대해서는 미분하면 1이므로 역전파는 입력 값을 그대로 흘려보낸다.
  
## 활성화 함수 계층 구현하기
### Relu 계층
![[Source/Untitled 5 6.png|Untitled 5 6.png]]
```Python
class Relu:
    def __init__(self):
        self.mask = None
    def forward(self, x):
        self.mask = (x <= 0) #멤버 변수 mask의 값 세팅
        out = x.copy()
        out[self.mask] = 0
        return out
    def backward(self, dout):
        dout[self.mask] = 0
        dx = dout
        return dx
```
![[Source/Untitled 6 6.png|Untitled 6 6.png]]
![[Source/Untitled 7 6.png|Untitled 7 6.png]]
  
  
### Sigmoid 계층
![[Source/Untitled 8 4.png|Untitled 8 4.png]]
![[Source/Untitled 9 3.png|Untitled 9 3.png]]
  
![[Source/Untitled 10 2.png|Untitled 10 2.png]]
  
## Affine/Softmax 계층 구현하기
### Affine 계층
  
### Softmax-with-Loss 계층
![[Source/Untitled 11 2.png|Untitled 11 2.png]]
![[Source/Untitled 12 2.png|Untitled 12 2.png]]