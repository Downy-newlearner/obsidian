![[Source/Untitled 13.png|Untitled 13.png]]
## 데이터 주도 학습
- 딥러닝은 데이터에서 ‘특징’을 스스로 추출하고 특징의 패턴을 스스로 학습하여 결과를 내놓는다.
- 그래서 딥러닝을 end-to-end machine learning이라고도 한다.(종단간 기계학습)
- 범용 능력을 제대로 판단하기 위해 train data와 test data를 구별한다.
  
## 손실함수
- 신경망 학습에서는 현재의 상태를 ‘하나의 지표’로 표현한다.
- 그리고 그 지표를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색한다.
- 이 지표를 ‘손실함수’라고 부른다.
- 보통 평균 제곱 오차와 교차 엔트로피 오차를 사용한다.
  
### 평균 제곱 오차
![[Source/Untitled 1 7.png|Untitled 1 7.png]]
- $y_k$﻿는 신경망의 출력, $t_k$﻿는 정답 레이블이다.
![[Source/Untitled 2 5.png|Untitled 2 5.png]]
- 정답 레이블은 원 핫 인코딩을 사용하여 정답에 해당하는 인덱스만 1, 나머지는 0이다.
- 오차를 제곱하기 때문에, 큰 오차에 더 큰 패널티를 적용한다.
- 회귀에서 많이 사용한다.
### 교차 엔트로피 오차
![[Source/Untitled 3 5.png|Untitled 3 5.png]]
- $y_k$﻿는 신경망의 출력, $t_k$﻿는 정답 레이블이다.
- 정답 레이블이 아니라면 $t_k =0$﻿인 원 핫 인코딩을 사용하기 때문에, 정답 레이블에 대한 신경망의 출력만 반영된다.
  
### 미니배치 학습
- 기계학습은 훈련 데이터에 대한 손실 함수의 값을 구하고, 그 값을 최대한 줄여주는 매개변수를 찾아낸다. → 모든 훈련 데이터를 대상으로 손실 함수 값을 구해야한다.
- 손실함수의 합은 어떻게 구할까?
    
    ![[Source/Untitled 4 4.png|Untitled 4 4.png]]
    
    교차엔트로피 값
    
      
    
![[Source/Untitled 5 4.png|Untitled 5 4.png]]
  
### 미니배치 생성하기
```Python
# 무작위 10개 추출
train_size = x_train.shape[0]
batch_size = 10
batch_mask = np.random.choice(train_size, batch_size)
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]
```
  
### batch용 ‘교차 엔트로피 오차’ 구현하기
1. 원 핫 인코딩으로 정답이 주어진 경우
    
    ```Python
    # 4.2.4 (배치용) 교차 엔트로피 오차 구현하기
    def cross_entropy_error(y, t):
        if y.ndim == 1:
            t = t.reshape(1, t.size)
            y = y.reshape(1, y.size)
    
        batch_size = y.shape[0]
        return -np.sum(t * np.log(y + 1e-7)) / batch_size
    ```
    
- y는 신경망의 출력, t는 정답 레이블이다.
  
1. 숫자 레이블로 정답이 주어진 경우
    
    ```Python
    # 4.2.4 (배치용) 교차 엔트로피 오차 구현하기
    def cross_entropy_error(y, t):
        if y.ndim == 1:
            t = t.reshape(1, t.size)
            y = y.reshape(1, y.size)
    
        batch_size = y.shape[0]
        return -np.sum(t * np.log(y[np.arange(batch_size), t])) / batch_size
    ```
    
      
    
### 손실함수를 사용하는 이유
- 손실 함수를 지표로 사용하기 때문이다. ← 최적의 매개변수를 탐색할 때 손실 함수의 값을 가능한 작게 하는 매개변수 값을 찾는다. ← 이 때 미분을 사용하여 가중치 매개변수의 값을 아주 조금 변화시켰을 때, 손실 함수가 어떻게 변하는지 확인한다.(미분값이 음수인 방향으로 매개변수를 조정한다. 만약 가중치 매개변수값을 바꿨는데 손실함수의 변화가 없으면, 즉 미분값이 0이면 그 가중치 매개변수의 갱신은 거기서 멈춘다.)
- 만약 정확도를 지표로 삼는다면 가중치 매개변수를 조금 바꾼다고해서 정확도의 변화는 거의 없을 확률이 높아서 대부분의 가중치 매개변수의 미분값은 0이 될 것이다. 또는 변화가 있더라도 그 값이 불연속적으로 갑자기 변화한다. 그래서 정확도를 지표로 사용하지 않는다. (계단 함수를 활성화 함수로 사용하지 않는 이유)
- 이는 활성화 함수로 계단 함수가 아닌 시그모이드 함수를 사용하는 것과 같은 맥락이다.
    
    |   |   |   |
    |---|---|---|
    ||미분값(접선의 기울기)|활성화함수로 사용한다면|
    |계단함수|대부분 0|매개변수의 작은 변화가 주는 파장을 말살|
    |시그모이드 함수|어느 장소라도 0이 되지 않음|매개변수의 작은 변화가 주는 파장을 말살하지 않아서, 신경망이 올바르게 학습할 수 있음|
    
      
    
## 수치 미분
![[Source/Untitled 6 4.png|Untitled 6 4.png]]
- 미분에서 사용되는 ‘극한’의 개념을 컴퓨터로 구현하는데에는 주의해야하는 2가지 이슈가 있다.
    1. 반올림 오차 문제
        1. 컴퓨터는 작은 값(소수점 8자리 이하)을 생략하여 오차가 생긴다.
            
            → h의 값을 적당히 작은 값($10^{-4}$﻿)을 이용하면 오차를 피할 수 있다.
            
    2. 함숫값의 차이(차분) 문제
        1. ‘극한’을 ‘적당히 작은 값’으로 표현할 수 밖에 없는 한계때문에 발생하는 문제이다.
        2. 위의 계산 식은 (x+h)와 x 사이의 기울기에 해당하게 된다.(전방 차분))
            
            →이 오차를 줄이기 위해 (x+h)와 (x-h)일 때의 함수 f의 차분을 계산하는 방법을 사용한다. (중앙 차분)
            
              
            
![[Source/Untitled 7 4.png|Untitled 7 4.png]]