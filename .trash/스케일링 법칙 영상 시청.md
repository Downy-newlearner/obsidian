
Scaling Laws for Autoregressive Generative Modeling

L1 Loss($1-P_i$)
크로스 엔트로피 Loss($-lnP_i$) 
![[Pasted image 20250828223303.png]]
L1 loss가 완만히 증가하는 것과 달리 크로스 엔트로피 loss가 정답이 아닐 때 훨씬 많은 패널티를 부여함을 알 수 있다.

'다음 단어'는 보통 하나가 아니다. 즉 정답은 하나가 아니므로 로스값이 0이 되는 것은 현실적으로 불가능하다.

