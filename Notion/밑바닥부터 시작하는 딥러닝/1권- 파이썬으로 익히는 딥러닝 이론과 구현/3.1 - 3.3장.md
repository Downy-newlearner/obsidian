## 활성화 함수의 처리 과정
![[Source/Untitled 11.png|Untitled 11.png]]
- 활성화 처리 과정을 표현하기 위해 위 그림과 같이 그린다.
- 활성화 함수는 퍼셉트론에서 신경망으로 가는 길잡이이다.
- 단 퍼셉트론과 다층 퍼셉트론은 층의 개수 차이와, 활성화 함수의 차이가 존재한다.
  
## 계단 함수 구현하기
![[Source/Untitled 1 5.png|Untitled 1 5.png]]
- 위 함수는 이해하기 쉽지만 함수의 인자로 numpy 배열이 들어가지 못한다.
- 아래의 함수로 고쳐보자.
![[Source/Untitled 2 3.png|Untitled 2 3.png]]
- 넘파이의 특성을 활용해 간단히 구현해냈다.
![[Source/Untitled 3 3.png|Untitled 3 3.png]]
  
![[Source/Untitled 4 2.png|Untitled 4 2.png]]
  
```Python
import numpy as np
import matplotlib.pyplot as plt
def step_function(x):
    return np.array(x > 0, dtype=int) \#https://numpy.org/devdocs/release/1.20.0-notes.html
x = np.arange(-5.0, 5.0, 0.1)
y = step_function(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)  # y축의 범위 지정
plt.show()
```
  
## 시그모이드 함수 구현하기
```Python
def sigmoid(x):
    return 1 / (1 + np.exp(-x)) #브로드캐스트를 통해 편리하게 연산된다.
x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()
```
  
## 시그모이드 함수와 계단 함수 비교하기
![[Source/Untitled 5 2.png|Untitled 5 2.png]]
![[Source/Untitled 6 2.png|Untitled 6 2.png]]
  
### 차이점
- 시그모이드 함수는 부드러운 곡선이며 입력에 따라 출력이 연속적으로 변한다. (계단 함수는 불연속적)
- 이 시그모이드의 매끈함이 신경망 학습에서 매우 중요한 역할을 하게 된다.
  
### 공통점
- 입력이 작을 때 출력은 0에 가깝고(혹은 0이고), 입력이 커지면 출력이 1에 가까워지는(혹은 1이 되는) 구조이다.
- 즉 입력이 중요하면 큰 값을, 중요치 않으면 작은 값을 출력한다.
  
## 비선형함수를 사용해야하는 이유
![[Source/Untitled 7 2.png|Untitled 7 2.png]]
  
## ReLU 함수
- Rectified(정류된) Linear Unit
- 렐루 함수
![[Source/Untitled 8 2.png|Untitled 8 2.png]]