### 주간보고서
[https://docs.google.com/document/d/1cn5I8VP3Xo3kgxNitgrTg8yRY4qZDzTl2KXw9GixMiA/edit#heading=h.wwbe8zvdegn6](https://docs.google.com/document/d/1cn5I8VP3Xo3kgxNitgrTg8yRY4qZDzTl2KXw9GixMiA/edit#heading=h.wwbe8zvdegn6)
### 논문 오퍼
[[논문 준비 참고 자료]]
### VFSS
[[VFSS 피드백]]
  
---
#### 랩 미팅
|이름|태그|
|---|---|
|[[06.27 랩 미팅]]||
|[[07.04 - 이고은 선배- Conditional Diffusion Model]]||
|[[07.11 왕준기 선배- Long-Tailed Problem]]||
|[[07.25 양주열 선배- Masked-attention Mask Transformer for Universal Image Segmentation]]||
|[[07.25 용어 정리- CNN과 Backbone, 그리고 AlexNet]]||
|[[08.01 태민우 선배- NeRF + 3DGS]]||
|[[08.16]]||
|[[08.22 박준영 선배]]||
|[[08.28 김세영 선배]]||
|[[09.13 이정호 선배]]||
|[[09.20 표세훈 선배]]||
|[[10.04 정다훈 발표]]||
|[[10.15 청각장애인을 위한 배리어프리 ICT기술 세미나]]||
  
  
#### 딥러닝&파이썬 용어 정리
|책 이름|이름|설명|
|---|---|---|
|세미나|[[제프리 힌튼]]|영국의 컴퓨터 과학자이다. 1986년의 다층 퍼셉트론과 (오차)역전파 [알고리즘](https://namu.wiki/w/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98)을 증명, 2006년의 심층신뢰 신경망 발표로 [딥러닝](https://namu.wiki/w/%EB%94%A5%EB%9F%AC%EB%8B%9D)을 [인공신경망](https://namu.wiki/w/%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D) 방법론의 대세로 굳히고 [GPU](https://namu.wiki/w/GPU)를 통한 병렬연산을 업계에 대중화시킨 선구자이다.|
|세미나|[[ResNet]]|Backbone 아키텍처 중 하나이다.|
|세미나|[[Attention 메커니즘]]|기존 인코더-디코더 구조에서 정보 손실이 발생할 수 있는 문제를 해결한 개선안.(자세한 내용은 설명 참고)|
|세미나|[[인코더-디코더 구조]]|설명 참고|
|세미나|[[Operation Study]]|Operations Study는 특정 시스템이나 프로세스의 효율성을 평가하고 향상시키기 위한 연구. 시뮬레이션, 최적화 기법, 보고서 작성을 포함하며 주로 논문 Operation Study 해봤어? 라고 물으면 수학적 근거가 부족한 논문 등에서 실제로 실험을 해봤냐는 의미로 쓰인다.|
|세미나|[[Context Vector]]|자연어 처리 모델의 구조 중 하나인 인코더-디코더 구조에서 인코더가 입력 시퀀스를 처리하여 생성하는 고정된 길이의 벡터이다. 이 벡터는 입력 시퀀스의 중요한 정보를 요약하여 디코더에 전달합니다.|
|세미나|[[Convolution]]|CNN에서 C. 합성곱이라는 뜻이다.|
|세미나|[[Backbone]]|신경망에서 주로 특징 추출을 담당하는 기본 네트워크 구조이다. 특징 추출과 전이 학습의 역할을 한다.|
|세미나|[[Interpolate]]|주어진 데이터 포인트들 사이의 빈 공간을 채워서 연속적인 값을 추정하는 과정이다.|
|세미나|[[Domain generalization]]|머신러닝 및 컴퓨터 비전 분야에서의 기법으로, 모델이 특정 학습 도메인에서 학습한 후, 보지 못한 새로운 도메인에서도 잘 일반화할 수 있도록 하는 접근입니다.|
|세미나|[[그래픽스 파이프라인]]|3D 장면을 렌더링하여 2D 이미지로 변환하는 과정에서 거치는 일련의 단계를 의미한다.|
|세미나|[[Adaptive Density Control(ADC)]]|3D 그래픽스와 비전 관련 분야에서 사용하는 기술로, 장면의 밀도를 동적으로 조절하여 렌더링 품질과 효율성을 개선하는 방법|
|세미나|[[Fast Differentiable Rasterization]]|컴퓨터 그래픽스와 인공지능 분야에서 사용되는 기법으로, 그래픽스 파이프라인의 라스터화 단계에서 미분 가능성을 제공하는 방법이다.|
|세미나|[[Volume density]]|파티클의 투명도. Ray가 오브젝트를 통과하는 중 파티클의 위치하는데 그 순간 파티클의 투명도를 의미한다. 예를 들어 통과하는 중인 파티클은 오브잭트 내부라서 투명도가 낮다.|
|세미나|[[SfM]]|Structure from Motion, 여러 장의 2D 이미지의 정보를 통해 전체적인 3D 구조와 카메라 pose를 추정하는 기법|
|세미나|[[NeRF]]|Neural Radiance Fields는 3D 장면을 효율적으로 표현하고 렌더링하기 위한 딥러닝 기술이다. NeRF는 주로 2D 이미지 시퀀스를 입력으로 받아들여, 장면의 3D 구조와 재질을 학습하여 새로운 시점에서 이미지를 생성하는 능력을 가지고 있다.|
|세미나|[[Plenoptic]]|진정한 의미의 홀로그램이 완성되기 전에 사람들이 홀로그램처럼 콘텐츠를 감상할 수 있도록 입체 영상을 촬영, 처리, 영사하는 기술이다.|
|세미나|[[Intrinsic dimension]]|데이터나 공간의 본질적인 차원 수를 나타내는 개념입니다. 이는 데이터가 본래 지니고 있는 정보의 구조를 반영하며, 데이터가 놓여 있는 고차원 공간에서 얼마나 적은 차원으로 이 데이터를 설명할 수 있는지를 나타냅니다.|
|세미나|[[lightweight decomposition model]]|가벼운 분할 분석 모델|
|독학|[[Latent space]]|한국어로는 해공간, 잠재공간 이라고 부른다. 이미지들이 Convolution Layers를 거쳐 나온 feature vector들의 공간을 Latent space라고 부른다. 좋은 latent space가 형성되었다면(개, 고양이 이미지들의 feature vector들이 섞이지 않고 경계가 명확함) 뒤에 연결된 fully connected layer들은 클래스들을 쉽게 구분할 것이다.|
|독학|[[Fully Connected Layers]]|feature vector가 입력으로 들어가고 예측값이 출력으로 나온다. 예를 들어 강아지와 고양이 이미지를 구별하는 모델이라면 예측값은 강아지 또는 고양이가 될 것이다.|
|독학|[[Convolution Layers = Feature Extractor = Encoder]]|이들은 서로 같은 의미이다. 이들은 입력으로부터 유용한 feature를 뽑아내는 역할을 한다. 뽑아낸 정보를 feature vector라고 부른다.|
|독학|[[차원 축소]]|n차원 벡터는 있는 그대로서 시각화하기가 불가능한데, 이런 n차원 벡터들을 2차원으로 시각화하는 방법이다.|
|독학|[[Feature Vector]]|이미지 하나를 하나의 n차원 벡터로 표현하곤 하는데, 이 n차원 벡터를 Feature Vector라고 부른다.(Encoder의 출력이 feature vector이다. 입력은 이미지이다.) feature vector는 fully connected layers의 입력으로 들어간다.|
|독학|[[representation]]|딥러닝에서 representation은 말 그대로 ‘표현’을 의미한다. 딥러닝 모델이 이미지를 표현하는, 즉 representation하는 방법으로는 Feature Vector가 있다.|
|독학|[[cifar100 데이터셋]]|컴퓨터 비전 분야에서 널리 사용되는 데이터셋 중 하나이다. 비행기 사진들이 포함되어있는 데이터셋이다.|
|독학|[[dinov2]]|이미지 인식, 객체 탐지, 이미지 분류 등 다양한 작업에서 뛰어난 성능을 발휘하는 Meta의 딥러닝 모델이다.|
|독학|[[clipvitL14]]|Clip Vit L14. CLIP(Contrastive Language–Image Pretraining)은 텍스트와 이미지를 동시에 학습하여 텍스트 설명과 이미지 간의 연관성을 이해할 수 있도록 설계된 모델이다. Vit는 비전 트랜스포머를 의미한다. L14는 레이어 수가 14개임을 의미한다. clip, vit는 각각 딥러닝 모델을 의미한다.|
|세미나|[[ROI Pooling]]||
|세미나|[[DSA-LSTM]]||
|세미나|[[RPN]]|Region Proposal Network|
|세미나|[[sliding window]]||
|세미나|[[anchor box & bouding box]]|Detection에서 사용되는 기본 틀이 Anchor box이고 실제 디텍션해낸 결과가 바운딩 박스이다.|
|독학|[[결합 분포(joint distribution)]]|두 개 이상의 확률 변수의 분포를 나타내는 개념으로, 여러 확률 변수가 동시에 어떤 값을 가질 확률을 설명한다.|
|독학|[[이미지가 가우시안 분포를 따른다는 것은]]|각 픽셀은 R, G, B 값으로 각각 0~255 사이의 값을 갖는다. 이 때 R, G, B 각각을 서로 독립적으로 보아 모든 픽셀의 R 값이 가우시안 분포를 따르고, G값, B값도 가우시안 분포를 따른다는 것이다.|
|독학|[[variational inference(변분 추론)]]|이후 타임스텝으로 근사하기 위해 사용하는 베이지 추론 기법 중 하나로, 효율적이고 실용적이라는 특징이 있다. 후방 분포를 직접 계산하는 것이 아니라 그저 후방분포를 근사하는 간단한 분포를 설정한다는 아이디어를 통해 효율성과 실용성을 챙기는 것이다.|
|독학|[[parameterized Markov chain]]|마르코프 체인의 전이 확률이 특정한 매개변수들에 의해 조정되고 결정된다는 것을 의미한다. 그리고 이 매개변수들은 모델의 훈련 과정에서 최적화된다.|
|독학|[[Conditional]]|“Conditional”이라는 용어가 붙은 이미지 생성형 모델 아키텍처는 특정 조건이나 입력에 따라 생성할 결과물이 결정된다는 의미이다.|
|독학|[[Objective function, Cost function]]|목적 함수, 최적화 문제에서 변수가 최소화 또는 최대화해야 하는 함수이다. 일례로 경사 하강법의 대상이 목적 함수인 것이다.|
|독학|[[브이랩]]|Virtual Lab 가상환경에서 실험을 수행할 수 있도록 도와주는 시스템|
|독학|[[robust learning]]|다양한 환경이나 조건에서도 강력한 성능을 유지할 수 있도록 모델을 학습하는 방법. 데이터의 노이즈, 잡음, 이상치에 대한 강인성을 강조하며 이런 불확실성에서도 안정적이고 신뢰성 있는 예측 모델을 만드는 것이 목표인 학습 방법이다.|
|독학|[[Noise Scheduler]]|Diffusion Process에서 노이즈가 추가되는 정도를 조절한다. 예시로 linuer noise scheduler는 시간이 지날 수록 더 많은 노이즈가 추가되도록 한다.|
|독학|[[Markov chain]]|각 전이의 단계는 현재 상태에만 의존한다는 메모리리스 속성을 가진다. 즉 현재 데이터 포인트(상태)에서 노이즈를 추가하여 다음 데이터 포인트(다음 상태)가 생성된다. 이 과정은 여러 단계에 걸쳐 진행되며, 각 단계가 가우시안 전이로 이루어지게 된다.|
|독학|[[Gaussian transition]]|Diffusion model에서 데이터의 변환 과정이 가우시안(정규) 분포를 따르는 것과 관련이 있습니다. 확산 모델은 노이즈를 점진적으로 추가하거나 제거하여 데이터를 생성합니다. 이 과정에서 각 단계는 가우시안 분포를 통해 정의된 전이 과정을 따릅니다.|
|독학|[[Diffusion Model]]|주로 데이터를 생성하거나 모사하는 데 사용되는 수학적 또는 컴퓨터 과학적 모델을 의미한다. 이 모델은 확산의 원리를 기반으로 하여, 데이터를 점진적으로 "노이즈"와 함께 변화시키고, 그 과정을 역으로 수행하여 원래 데이터를 복원하거나 새로운 데이터를 생성하는 데 사용된다.|
|독학|[[deep generative model]]||
|세미나|[[knowledge distillation]]||
|세미나|[[카메라 Calibration]]||
|세미나|[[HRnet]]||
|세미나|[[$psi$(프사이)]]||
|세미나|[[Multi-view NRSfM]]||
|세미나|[[LSTM]]||
|세미나|[[GRU]]||
|세미나|[[4D Correleation Volumes]]||
|세미나|[[RAFT]]||
|세미나|[[Triangulation]]||
|세미나|[[Landmark detection]]||
|독학|[[모델 샘플링]]|딥러닝 등의 모델을 다룰 때, 샘플링은 주어진 확률 분포에서 값을 추출하는 과정이다.|
|독학|[[데이터 샘플링]]|전체 데이터 세트에서 샘플(일부) 데이터를 선택하는 과정이다.|
|독학|[[checkpoint]]|학습된 모델을 저장하는 기능이다. 체크포인트는 모델의 일관성을 유지하고, 실험의 재현성을 보장하는 데 도움을 준다.|
|독학|[[LoRA]]|Low-Rank Adaptation. 모델을 세부 조정하기 위한 학습 기법이다.|
|세미나|[[MBW]]||