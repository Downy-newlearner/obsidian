---
책 이름: 세미나
설명: 기존 인코더-디코더 구조에서 정보 손실이 발생할 수 있는 문제를 해결한 개선안.(자세한 내용은 설명 참고)
챕터/날짜: "07.25"
---
## 뜻💫

> [!info] 15-01 어텐션 메커니즘 (Attention Mechanism)  
> 앞서 배운 seq2seq 모델은 **인코더**에서 입력 시퀀스를 컨텍스트 벡터라는 하나의 고정된 크기의 벡터 표현으로 압축하고, **디코더**는 이 컨텍스트 벡터를 통해서 출력 …  
> [https://wikidocs.net/22893](https://wikidocs.net/22893)  
어텐션의 기본 아이디어는 디코더에서 출력 단어를 예측하는 매 시점(time step)마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고한다는 점입니다. 단, 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라, 해당 시점에서 예측해야할 단어와 연관이 있는 입력 단어 부분을 좀 더 집중(attention)해서 보게 됩니다.
## 예시📝