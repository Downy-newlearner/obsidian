![[강화학습_3-1_(250317)_page-0001.jpg]]
- markov process -> markov reward process -> markov decision process(지금 챕터에서 추가되는 빌드업)

- "State value가 높은 곳으로 가는 것이 좋다"라는 결론이 났고, 어떻게 높은 곳으로 갈 수 있는지는 Markov decision process를 통해 알 수 있다.
![[강화학습_3-1_(250317)_page-0002.jpg]]
- A: action space
- ![[Pasted image 20250317111911.png|150]]: state s에서 action을 수행한다 그 다음 다음 state로 가는 것이다. 이것이 transition probability('$p_{ss'}^a$, s에서 a를 해서 s'가 된다'라고 이해하면 된다.)

- 표준화 된 순서를 익히자!(Transition probability의 타이밍을 잘 알자)
	- MRP에서 reward function은 s에서 s'로 이동하며 도착하기 전에 리워드를 받고난 후 도착한다.
	- MDP에서 reward function은 action을 먼저 취하고 reward를 받고 도착한다.


![[강화학습_3-1_(250317)_page-0003.jpg]]

![[강화학습_3-1_(250317)_page-0004.jpg]]
- ![[Pasted image 20250317112331.png|150]]: transition probability는 액션을 먼저 취한 후 확인하는 것이다.
![[강화학습_3-1_(250317)_page-0005.jpg]]
- Policy는 주어져있는 정보(tuple)은 아니다.
	- MDP를 구성하는 정보가 아니라는 의미이다.

- s에서 a를 할 확률을 policy($\pi$)라고 한다.

- state space
- reward wfunc
- $\gamma$
- ... 이런 것들은 주어져 있는 것이고, policy는 만들어가야하는 것이다.

- policy를 만든다는 것은
	- action을 할 확률을 정의해야한다는 것이다.
	- 즉 Agent의 behavior를 정의하는 것이다.

- ![[Pasted image 20250317112926.png|300]]: time dependent한 것이 보통 현실이지만 그렇게 계산은 너무 복잡하기 때문에 불가능하다.

- 만약 시간의 흐름을 반영하는 것이 중요하다고 하면, state space에 time t를 포함하면 된다. 그렇다면 state space가 굉장히 많아질 것이다.


![[강화학습_3-1_(250317)_page-0006.jpg]]
- 원래 tran prob은 액션에 따라 결정이 났다.
- 그런데 action이 일어날 확률은 policy에 따르기 때문에 tran prob이 ![[Pasted image 20250317113418.png|200]]
처럼, Reward process는 ![[Pasted image 20250317113447.png|200]]처럼 정의 된다.

![[강화학습_3-1_(250317)_page-0007.jpg]]
- $\pi$에 따라 transition prob이 달라지므로 위 수식에서 $\pi$에 따른 state value function이 정의된다.
	- Policy에 대해 


![[강화학습_3-1_(250317)_page-0008.jpg]]

![[강화학습_3-1_(250317)_page-0009.jpg]]

![[강화학습_3-1_(250317)_page-0010.jpg]]

![[강화학습_3-1_(250317)_page-0011.jpg]]

![[강화학습_3-1_(250317)_page-0012.jpg]]

![[강화학습_3-1_(250317)_page-0013.jpg]]

![[강화학습_3-1_(250317)_page-0014.jpg]]

![[강화학습_3-1_(250317)_page-0015.jpg]]

![[강화학습_3-1_(250317)_page-0016.jpg]]

![[강화학습_3-1_(250317)_page-0017.jpg]]

![[강화학습_3-1_(250317)_page-0018.jpg]]

![[강화학습_3-1_(250317)_page-0019.jpg]]

![[강화학습_3-1_(250317)_page-0020.jpg]]

![[강화학습_3-1_(250317)_page-0021.jpg]]

![[강화학습_3-1_(250317)_page-0022.jpg]]

![[강화학습_3-1_(250317)_page-0023.jpg]]

![[강화학습_3-1_(250317)_page-0024.jpg]]

![[강화학습_3-1_(250317)_page-0025.jpg]]

![[강화학습_3-1_(250317)_page-0026.jpg]]

![[강화학습_3-1_(250317)_page-0027.jpg]]