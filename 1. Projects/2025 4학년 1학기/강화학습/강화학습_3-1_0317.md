![[강화학습_3-1_(250317)_page-0001.jpg]]
- markov process -> markov reward process -> markov decision process(지금 챕터에서 추가되는 빌드업)

- "State value가 높은 곳으로 가는 것이 좋다"라는 결론이 났고, 어떻게 높은 곳으로 갈 수 있는지는 Markov decision process를 통해 알 수 있다.
![[강화학습_3-1_(250317)_page-0002.jpg]]
- A: action space
- ![[Pasted image 20250317111911.png|150]]: state s에서 action을 수행한다 그 다음 다음 state로 가는 것이다. 이것이 transition probability('$p_{ss'}^a$, s에서 a를 해서 s'가 된다'라고 이해하면 된다.)

- 표준화 된 순서를 익히자!(Transition probability의 타이밍을 잘 알자)
	- MRP에서 reward function은 s에서 s'로 이동하며 도착하기 전에 리워드를 받고난 후 도착한다.
	- MDP에서 reward function은 action을 먼저 취하고 reward를 받고 도착한다.


![[강화학습_3-1_(250317)_page-0003.jpg]]

![[강화학습_3-1_(250317)_page-0004.jpg]]
- ![[Pasted image 20250317112331.png|150]]: transition probability는 액션을 먼저 취한 후 확인하는 것이다.
![[강화학습_3-1_(250317)_page-0005.jpg]]
- Policy는 주어져있는 정보(tuple)은 아니다.
	- MDP를 구성하는 정보가 아니라는 의미이다.

- s에서 a를 할 확률을 policy($\pi$)라고 한다.

- state space
- reward wfunc
- $\gamma$
- ... 이런 것들은 주어져 있는 것이고, policy는 만들어가야하는 것이다.

- policy를 만든다는 것은
	- action을 할 확률을 정의해야한다는 것이다.
	- 즉 Agent의 behavior를 정의하는 것이다.

- ![[Pasted image 20250317112926.png|300]]: time dependent한 것이 보통 현실이지만 그렇게 계산은 너무 복잡하기 때문에 불가능하다.

- 만약 시간의 흐름을 반영하는 것이 중요하다고 하면, state space에 time t를 포함하면 된다. 그렇다면 state space가 굉장히 많아질 것이다.


![[강화학습_3-1_(250317)_page-0006.jpg]]
- 원래 tran prob은 액션에 따라 결정이 났다.
- 그런데 action이 일어날 확률은 policy에 따르기 때문에 tran prob이 ![[Pasted image 20250317113418.png|200]]
처럼, Reward process는 ![[Pasted image 20250317113447.png|200]]처럼 정의 된다.

![[강화학습_3-1_(250317)_page-0007.jpg]]
- $\pi$에 따라 transition prob이 달라지므로 위 수식에서 $\pi$에 따른 state value function이 정의된다.
	- State value function은 Policy에 대해 dependent하다.


![[강화학습_3-1_(250317)_page-0008.jpg]]
- 이전에는 state value func까지 배웠다.
- State-action value function은 state value func이라고 보통 부른다.

- state s와 action a에 따른 기댓값이 state action value func이다.
![[강화학습_3-1_(250317)_page-0009.jpg]]

![[강화학습_3-1_(250317)_page-0010.jpg]]
- 이후 슬라이드부터는 이 수식들을 증명한다.

![[강화학습_3-1_(250317)_page-0011.jpg]]
- q 러닝 할 때 q가 이 q이다.

![[Pasted image 20250318132644.png|400]]
- S에서 각 액션을 할 확률이 25%씩이라고 하면 $v_\pi(s)$는 
	- 0.25*($q_\pi(s, a_0) + q_\pi(s, a_1) + q_\pi(s, a_2) + q_\pi(s, a_3)$)이다.


![[강화학습_3-1_(250317)_page-0012.jpg]]
q = immediately reward + transition prob * state value의 sum에 $\gamma$를 곱한 값

![[강화학습_3-1_(250317)_page-0013.jpg]]
- 방금 두 슬라이드의 수식을 연립해서 재귀적인 구조의 식을 만들었다.

![[강화학습_3-1_(250317)_page-0014.jpg]]
- 비로소 state func과 q func에 대한 재귀적인 수식을 만들었다.
	- state func의 재귀적 수식: ???
	- q func의 재귀적 수식: ???     - 채워놓아라  -0318 다훈


![[강화학습_3-1_(250317)_page-0015.jpg]]

![[강화학습_3-1_(250317)_page-0016.jpg]]
- 벨만 방정식은 itera
![[강화학습_3-1_(250317)_page-0017.jpg]]

![[강화학습_3-1_(250317)_page-0018.jpg]]

![[강화학습_3-1_(250317)_page-0019.jpg]]

![[강화학습_3-1_(250317)_page-0020.jpg]]

![[강화학습_3-1_(250317)_page-0021.jpg]]

![[강화학습_3-1_(250317)_page-0022.jpg]]

![[강화학습_3-1_(250317)_page-0023.jpg]]

![[강화학습_3-1_(250317)_page-0024.jpg]]

![[강화학습_3-1_(250317)_page-0025.jpg]]

![[강화학습_3-1_(250317)_page-0026.jpg]]

![[강화학습_3-1_(250317)_page-0027.jpg]]