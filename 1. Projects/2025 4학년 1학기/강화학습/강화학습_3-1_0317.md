![[강화학습_3-1_(250317)_page-0001.jpg]]
- markov process -> markov reward process -> markov decision process(지금 챕터에서 추가되는 빌드업)

- "State value가 높은 곳으로 가는 것이 좋다"라는 결론이 났고, 어떻게 높은 곳으로 갈 수 있는지는 Markov decision process를 통해 알 수 있다.
![[강화학습_3-1_(250317)_page-0002.jpg]]
- A: action space
- ![[Pasted image 20250317111911.png|150]]: state s에서 action을 수행한다 그 다음 다음 state로 가는 것이다. 이것이 transition probability('$p_{ss'}^a$, s에서 a를 해서 s'가 된다'라고 이해하면 된다.)

- 표준화 된 순서를 익히자!(Transition probability의 타이밍을 잘 알자)
	- MRP에서 reward function은 s에서 s'로 이동하며 도착하기 전에 리워드를 받고난 후 도착한다.
	- MDP에서 reward function은 action을 먼저 취하고 reward를 받고 도착한다.


![[강화학습_3-1_(250317)_page-0003.jpg]]

![[강화학습_3-1_(250317)_page-0004.jpg]]
- ![[Pasted image 20250317112331.png|150]]: transition probability는 액션을 먼저 취한 후 확인하는 것이다.
![[강화학습_3-1_(250317)_page-0005.jpg]]

![[강화학습_3-1_(250317)_page-0006.jpg]]

![[강화학습_3-1_(250317)_page-0007.jpg]]

![[강화학습_3-1_(250317)_page-0008.jpg]]

![[강화학습_3-1_(250317)_page-0009.jpg]]

![[강화학습_3-1_(250317)_page-0010.jpg]]

![[강화학습_3-1_(250317)_page-0011.jpg]]

![[강화학습_3-1_(250317)_page-0012.jpg]]

![[강화학습_3-1_(250317)_page-0013.jpg]]

![[강화학습_3-1_(250317)_page-0014.jpg]]

![[강화학습_3-1_(250317)_page-0015.jpg]]

![[강화학습_3-1_(250317)_page-0016.jpg]]

![[강화학습_3-1_(250317)_page-0017.jpg]]

![[강화학습_3-1_(250317)_page-0018.jpg]]

![[강화학습_3-1_(250317)_page-0019.jpg]]

![[강화학습_3-1_(250317)_page-0020.jpg]]

![[강화학습_3-1_(250317)_page-0021.jpg]]

![[강화학습_3-1_(250317)_page-0022.jpg]]

![[강화학습_3-1_(250317)_page-0023.jpg]]

![[강화학습_3-1_(250317)_page-0024.jpg]]

![[강화학습_3-1_(250317)_page-0025.jpg]]

![[강화학습_3-1_(250317)_page-0026.jpg]]

![[강화학습_3-1_(250317)_page-0027.jpg]]