![[강화학습_3-1_(250317)_page-0001.jpg]]
- markov process -> markov reward process -> markov decision process(지금 챕터에서 추가되는 빌드업)

- "State value가 높은 곳으로 가는 것이 좋다"라는 결론이 났고, 어떻게 높은 곳으로 갈 수 있는지는 Markov decision process를 통해 알 수 있다.
![[강화학습_3-1_(250317)_page-0002.jpg]]
- A: action space
- ![[Pasted image 20250317111911.png|150]]: state s에서 action을 수행한다 그 다음 다음 state로 가는 것이다. 이것이 transition probability('$p_{ss'}^a$, s에서 a를 해서 s'가 된다'라고 이해하면 된다.)

- 표준화 된 순서를 익히자!(Transition probability의 타이밍을 잘 알자)
	- MRP에서 reward function은 s에서 s'로 이동하며 도착하기 전에 리워드를 받고난 후 도착한다.
	- MDP에서 reward function은 action을 먼저 취하고 reward를 받고 도착한다.


![[강화학습_3-1_(250317)_page-0003.jpg]]

![[강화학습_3-1_(250317)_page-0004.jpg]]
- ![[Pasted image 20250317112331.png|150]]: transition probability는 액션을 먼저 취한 후 확인하는 것이다.
![[강화학습_3-1_(250317)_page-0005.jpg]]
- Policy는 주어져있는 정보(tuple)은 아니다.
	- MDP를 구성하는 정보가 아니라는 의미이다.

- s에서 a를 할 확률을 policy($\pi$)라고 한다.

- state space
- reward wfunc
- $\gamma$
- ... 이런 것들은 주어져 있는 것이고, policy는 만들어가야하는 것이다.

- policy를 만든다는 것은
	- action을 할 확률을 정의해야한다는 것이다.
	- 즉 Agent의 behavior를 정의하는 것이다.

- ![[Pasted image 20250317112926.png|300]]: time dependent한 것이 보통 현실이지만 그렇게 계산은 너무 복잡하기 때문에 불가능하다.

- 만약 시간의 흐름을 반영하는 것이 중요하다고 하면, state space에 time t를 포함하면 된다. 그렇다면 state space가 굉장히 많아질 것이다.


![[강화학습_3-1_(250317)_page-0006.jpg]]
- 원래 tran prob은 액션에 따라 결정이 났다.
- 그런데 action이 일어날 확률은 policy에 따르기 때문에 tran prob이 ![[Pasted image 20250317113418.png|200]]
처럼, Reward process는 ![[Pasted image 20250317113447.png|200]]처럼 정의 된다.

![[강화학습_3-1_(250317)_page-0007.jpg]]
- $\pi$에 따라 transition prob이 달라지므로 위 수식에서 $\pi$에 따른 state value function이 정의된다.
	- State value function은 Policy에 대해 dependent하다.


![[강화학습_3-1_(250317)_page-0008.jpg]]
- 이전에는 state value func까지 배웠다.
- State-action value function은 state value func이라고 보통 부른다.

- state s와 action a에 따른 기댓값이 state action value func이다.
![[강화학습_3-1_(250317)_page-0009.jpg]]

![[강화학습_3-1_(250317)_page-0010.jpg]]
- 이후 슬라이드부터는 이 수식들을 증명한다.

![[강화학습_3-1_(250317)_page-0011.jpg]]
- q 러닝 할 때 q가 이 q이다.

![[Pasted image 20250318132644.png|400]]
- S에서 각 액션을 할 확률이 25%씩이라고 하면 $v_\pi(s)$는 
	- 0.25*($q_\pi(s, a_0) + q_\pi(s, a_1) + q_\pi(s, a_2) + q_\pi(s, a_3)$)이다.


![[강화학습_3-1_(250317)_page-0012.jpg]]
q = immediately reward + transition prob * state value의 sum에 $\gamma$를 곱한 값

![[강화학습_3-1_(250317)_page-0013.jpg]]
- 방금 두 슬라이드의 수식을 연립해서 재귀적인 구조의 식을 만들었다.
- immediately reward의 평균값과 state value function의 평균값.

![[강화학습_3-1_(250317)_page-0014.jpg]]
- 비로소 state func과 q func에 대한 재귀적인 수식을 만들었다.
	- state func의 재귀적 수식: ???
	- q func의 재귀적 수식: ???     - 채워놓아라  -0318 다훈


![[강화학습_3-1_(250317)_page-0015.jpg]]

![[강화학습_3-1_(250317)_page-0016.jpg]]
- 벨만 방정식은 iteration을 통해 개선하는 과정을 가질 수 밖에 없다.

![[강화학습_3-1_(250317)_page-0017.jpg]]
- $v(s)$의 합이 최대가 되도록 하는 policy를 optimal policy라고 하고, optimal policy가 적용된 functino을 optimal value function이라고 한다.


![[강화학습_3-1_(250317)_page-0018.jpg]]

![[강화학습_3-1_(250317)_page-0019.jpg]]

![[강화학습_3-1_(250317)_page-0020.jpg]]

![[강화학습_3-1_(250317)_page-0021.jpg]]
- q func에 벌써 star가 붙어있는데 MAX를 시킨다?
- argmax func은 단순히 이렇게 생가가해야한다.
	- q star 들 중에 최댓값을 가지는 걸 선택한다.
	- 이전 예시에서는 4개의 q func은 이미 스타가 붙어있는데 그 4개 중에 star, 즉 star의 star를 뽑는 과정인 것이다.
		![[Pasted image 20250318135756.png|300]]
	- 최적화된 q 값은 내가 선택할 수 있는 action의 수와 같다. 
	- 그 중에서 최대값을 뽑는게 argmax가 동작하는 방식이다.

![[강화학습_3-1_(250317)_page-0022.jpg]]

![[강화학습_3-1_(250317)_page-0023.jpg]]
- optimized policy에서 $v_*(s)$는 가장 큰 q값이 리턴된다.
- argmax는 q 값이 가장 크도록하는 a를 리턴하는 것이고, max는 가장 큰 q값 자체를 리턴한다.

![[강화학습_3-1_(250317)_page-0024.jpg]]

![[강화학습_3-1_(250317)_page-0025.jpg]]

![[강화학습_3-1_(250317)_page-0026.jpg]]

![[강화학습_3-1_(250317)_page-0027.jpg]]