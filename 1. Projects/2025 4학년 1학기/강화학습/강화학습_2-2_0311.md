![[강화학습_2-2_(250311)_page-0001.jpg]]

![[강화학습_2-2_(250311)_page-0002.jpg]]
- 포괄적인 표현을 위해 Expectation을 붙여놓은 것이다
- state s에서 다음 state로 넘어갈 때 얻는 reward가 $R_s$이다.

![[강화학습_2-2_(250311)_page-0003.jpg]]
- 여기선 Sleep이 Terminal state이다.
- C2 -> C3 이동에서 시점이 t -> t+1 이라고 한다면, Reward가 적용되는 시점은 t+1이다.
- Episode에 상관없이 모두 똑같이 Terminal state로 가며 종료되지만 Reward는 Episode에 따라 다르다.

![[강화학습_2-2_(250311)_page-0004.jpg]]
- Return == immediately reward ??
- 지금 당장의 보상이 중요하다면 discount factor인 $\gamma$를 $\gamma=0$에 가깝게, 미래의 보상도 중요하다면 $\gamma=1$에 가깝게 두면 된다.

![[강화학습_2-2_(250311)_page-0005.jpg]]

![[강화학습_2-2_(250311)_page-0006.jpg]]
- 기댓값을 쓰는 이유는 리턴값이 에피소드마다 달라지는데 그것들에 대한 평균값임을 표현하기 위해서이다.

![[강화학습_2-2_(250311)_page-0007.jpg]]
- 평균값이 state value이다.

![[강화학습_2-2_(250311)_page-0008.jpg]]
- $\gamma = 0$으로 하면 state value는 immediately reward가 된다.
![[강화학습_2-2_(250311)_page-0009.jpg]]
- 만약 $\gamma$가 0이 아니면 immediately reward와 다르다.
![[강화학습_2-2_(250311)_page-0010.jpg]]

![[강화학습_2-2_(250311)_page-0011.jpg]]
- 결과 수식덕분에 이전에 balance equation을 연립할 일이 없어지고 간단해졌다.

![[강화학습_2-2_(250311)_page-0012.jpg]]

![[강화학습_2-2_(250311)_page-0013.jpg]]

![[강화학습_2-2_(250311)_page-0014.jpg]]

![[강화학습_2-2_(250311)_page-0015.jpg]]