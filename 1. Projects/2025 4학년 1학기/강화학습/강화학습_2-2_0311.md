![[강화학습_2-2_(250311)_page-0001.jpg]]

![[강화학습_2-2_(250311)_page-0002.jpg]]
- 포괄적인 표현을 위해 Expectation을 붙여놓은 것이다
- state s에서 다음 state로 넘어갈 때 얻는 reward가 $R_s$이다.

![[강화학습_2-2_(250311)_page-0003.jpg]]
- 여기선 Sleep이 Terminal state이다.
- C2 -> C3 이동에서 시점이 t -> t+1 이라고 한다면, Reward가 적용되는 시점은 t+1이다.
- Episode에 상관없이 모두 똑같이 Terminal state로 가며 종료되지만 Reward는 Episode에 따라 다르다.

![[강화학습_2-2_(250311)_page-0004.jpg]]
- Return == immediately reward ??
- 지금 당장의 보상이 중요하다면 discount factor인 $\gamma$를 $\gamma=0$에 가깝게, 미래의 보상도 중요하다면 $\gamma=1$에 가깝게 두면 된다.

![[강화학습_2-2_(250311)_page-0005.jpg]]

![[강화학습_2-2_(250311)_page-0006.jpg]]
- 기댓값을 쓰는 이유는 리턴값이 에피소드마다 달라지는데 그것들에 대한 평균값임을 표현하기 위해서이다.
- State value

![[강화학습_2-2_(250311)_page-0007.jpg]]
- 평균값이 state value이다.

![[강화학습_2-2_(250311)_page-0008.jpg]]
- $\gamma = 0$으로 하면 state value는 immediately reward가 된다.
![[강화학습_2-2_(250311)_page-0009.jpg]]
- 만약 $\gamma$가 0이 아니면 immediately reward와 다르다.
![[강화학습_2-2_(250311)_page-0010.jpg]]


## Bellman Equation
![[강화학습_2-2_(250311)_page-0011.jpg]]
- 결과 수식덕분에 이전에 balance equation을 연립할 일이 없어지고 간단해졌다.

![[강화학습_2-2_(250311)_page-0012.jpg]]
- 모든 s'에 대해 sumation
	- 모든 s'이란?
		- 모든 노드가 다 들어간 것이 대문자 S이다.
		- 만약 Pub에 있다고 치면 2, 3, 4로 가는 경우가 존재한다.
			- 이 때 pub이 s가 되고, 2,3,4가 s'의 후보가 되는 것이다.
			- Transition probability?
				- Transition Probability는 강화학습에서 주어진 상태(State)와 행동(Action) 쌍이 다음 상태(State)로 전이될 확률을 의미합니다. 이는 환경의 동적 모델을 이해하는 데 핵심적인 요소입니다.

![[강화학습_2-2_(250311)_page-0013.jpg]]
- 계산하면 4.32인데 피겨에 표시된 4.3과 차이가 나는 이유는 
	- $v(s)$를 찾기 위해서 옆에 있는 state의 $v(s)$를 참조하는데, 이는 일종의 과거 $v(s)$를 참조한다는 뜻이다.
		- 다시 말해 내가 계산한 4.32가 현재의 state value이고, 피겨의 4.3은 한 타임스텝 이전, 즉 과거의 state value라서 다르다는 뜻이다.
	- (그럼 가장 처음 구해야하는 $v(s)$는 어떻게 찾아야하는가?) -> 임의의 값을 때려넣으면 된다.(어차피 Stationary Distribution에 의해 수렴한다.)


- 지금 피겨에 나와있는 값은 "과거의 값"이고, 계산해서 나온 4.32(오차라고 했던) 값은 업데이트 되는 현재 값이다.
![[강화학습_2-2_(250311)_page-0014.jpg]]
- 계산을 반복하면 $v(s)$가 계속 업데이트 될 것이다.
- stationary distribution에서 egen vecter를 찾는 로직을 그대로 집어넣어보자.
	- 계산을 계속 하다보면 어딘가로 수렴할 것이다.
	- 수렴한다는 말은 ![[Pasted image 20250317110451.png|150]]이 수식에서 좌변의 $v$와 우변의 $v$가 같다는 것이다.

- 리턴에 대한 기댓값이 얼마냐를 평가하는게 v이므로, 높을 수록 좋다.

- State value를 계산하는 것이 중요한 이유
	- ==???==

- 이 계산은 $O(n^3)$의 시간 복잡도를 요구한다.
	- state 개수가 100개정도 된다면 ![[Pasted image 20250317110927.png|150]]이 수식을 사용하면 된다.
	- 하지만 state 개수가 10000개 정도로 굉장히 많아진다면?
		- 이 솔루션은 나중에 나온다.
![[강화학습_2-2_(250311)_page-0015.jpg]]