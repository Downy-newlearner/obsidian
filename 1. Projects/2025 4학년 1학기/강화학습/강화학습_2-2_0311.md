![[강화학습_2-2_(250311)_page-0001.jpg]]

![[강화학습_2-2_(250311)_page-0002.jpg]]
- 포괄적인 표현을 위해 Expectation을 붙여놓은 것이다
- state s에서 다음 state로 넘어갈 때 얻는 reward가 $R_s$이다.

![[강화학습_2-2_(250311)_page-0003.jpg]]
- 여기선 Sleep이 Terminal state이다.
- C2 -> C3 이동에서 시점이 t -> t+1 이라고 한다면, Reward가 적용되는 시점은 t+1이다.
- Episode에 상관없이 모두 똑같이 Terminal state로 가며 종료되지만 Reward는 Episode에 따라 다르다.

![[강화학습_2-2_(250311)_page-0004.jpg]]
- Return == immediately reward ??
- 지금 당장의 보상이 중요하다면 discount factor인 $\gamma$를 $\gamma=0$에 가깝게, 미래의 보상도 중요하다면 $\gamma=1$에 가깝게 두면 된다.

![[강화학습_2-2_(250311)_page-0005.jpg]]

![[강화학습_2-2_(250311)_page-0006.jpg]]
- 기댓값을 쓰는 이유는 리턴값이 에피소드마다 달라지는데 그것들에 대한 평균값임을 표현하기 위해서이다.
- State value

![[강화학습_2-2_(250311)_page-0007.jpg]]
- 평균값이 state value이다.

![[강화학습_2-2_(250311)_page-0008.jpg]]
- $\gamma = 0$으로 하면 state value는 immediately reward가 된다.
![[강화학습_2-2_(250311)_page-0009.jpg]]
- 만약 $\gamma$가 0이 아니면 immediately reward와 다르다.
![[강화학습_2-2_(250311)_page-0010.jpg]]


## Bellman Equation
![[강화학습_2-2_(250311)_page-0011.jpg]]
- 결과 수식덕분에 이전에 balance equation을 연립할 일이 없어지고 간단해졌다.

![[강화학습_2-2_(250311)_page-0012.jpg]]
- 모든 s'에 대해 sumation
	- 모든 s'이란?
		- 모든 노드가 다 들어간 것이 대문자 S이다.
		- 만약 Pub에 있다고 치면 2, 3, 4로 가는 경우가 존재한다.
			- 이 때 pub이 s가 되고, 2,3,4가 s'의 후보가 되는 것이다.
			- Transition probability?
				- Transition Probability는 강화학습에서 주어진 상태(State)와 행동(Action) 쌍이 다음 상태(State)로 전이될 확률을 의미합니다. 이는 환경의 동적 모델을 이해하는 데 핵심적인 요소입니다.

![[강화학습_2-2_(250311)_page-0013.jpg]]
- 계산하면 4.32인데 이 오차는 
![[강화학습_2-2_(250311)_page-0014.jpg]]

![[강화학습_2-2_(250311)_page-0015.jpg]]