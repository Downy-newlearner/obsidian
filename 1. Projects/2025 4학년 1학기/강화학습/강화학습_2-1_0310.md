[[강화학습_2-1_0310#^60afd2|250311화]]


![[강화학습_2-1_(250310)_page-0001 1.jpg]]

## Machine Learning
![[강화학습_2-1_(250310)_page-0002.jpg]]
- Machine Learning이란 
	- for Performance
	- at Task
	- with Experience
	- P, T, E
![[강화학습_2-1_(250310)_page-0003.jpg]]

![[강화학습_2-1_(250310)_page-0004.jpg]]

![[강화학습_2-1_(250310)_page-0005.jpg]]

![[강화학습_2-1_(250310)_page-0006.jpg]]
- Anomaly
	- 일반적인 패턴이나 기대치에서 벗어난 특이한 데이터를 의미한다.
![[강화학습_2-1_(250310)_page-0007.jpg]]

![[강화학습_2-1_(250310)_page-0008.jpg]]

![[강화학습_2-1_(250310)_page-0009.jpg]]

![[강화학습_2-1_(250310)_page-0010.jpg]]

![[강화학습_2-1_(250310)_page-0011.jpg]]

![[강화학습_2-1_(250310)_page-0012.jpg]]

![[강화학습_2-1_(250310)_page-0013.jpg]]

![[강화학습_2-1_(250310)_page-0014.jpg]]

![[강화학습_2-1_(250310)_page-0015.jpg]]

![[강화학습_2-1_(250310)_page-0016.jpg]]

![[강화학습_2-1_(250310)_page-0017.jpg]]

![[강화학습_2-1_(250310)_page-0018.jpg]]

## Reinforcement Learning
![[강화학습_2-1_(250310)_page-0019.jpg]]

![[강화학습_2-1_(250310)_page-0020.jpg]]

![[강화학습_2-1_(250310)_page-0021.jpg]]
- 강화학습
	- 시도와 에러를 통해 발전하는 과정
	- 연속적인 결정 생성 문제에서 누적 보상을 최대화하기 위해 시도와 에러를 통해 행동을 교정해나가는 과정

![[강화학습_2-1_(250310)_page-0022.jpg]]
- 순차적인 액션 셋을 계속 실행한다.
- 강화학습은 일련의 decision making이다.

![[강화학습_2-1_(250310)_page-0023.jpg]]

![[강화학습_2-1_(250310)_page-0024.jpg]]
- Agent에게 state와 reward를 준다.

![[강화학습_2-1_(250310)_page-0025.jpg]]

![[강화학습_2-1_(250310)_page-0026.jpg]]

![[강화학습_2-1_(250310)_page-0027.jpg]]

![[강화학습_2-1_(250310)_page-0028.jpg]]

![[강화학습_2-1_(250310)_page-0029.jpg]]
- '어떻게 reward를 얻었는가'가 아닌 '얼마나 많이 얻었는가'이다.

![[강화학습_2-1_(250310)_page-0030.jpg]]

![[강화학습_2-1_(250310)_page-0031.jpg]]

![[강화학습_2-1_(250310)_page-0032.jpg]]

![[강화학습_2-1_(250310)_page-0033.jpg]]

![[강화학습_2-1_(250310)_page-0034.jpg]]


## Markov Process
![[강화학습_2-1_(250310)_page-0035.jpg]]
- 마르코프 프로세스는 강화학습의 토대이다.

![[강화학습_2-1_(250310)_page-0036.jpg]]
- Stochastic process
	- 랜덤 프로세스라고도 부른다.
	- Random variable에 시간의 개념이 추가된 것이다.
	- **확률 과정(Stochastic process 또는 Random process)** 는 시간에 따라 변하는 확률 변수들의 집합으로 정의됩니다. 이는 각 시간 포인트에서 확률 변수가 값을 취하는 방식으로, 이러한 값의 변화가 확률적으로 결정되는 과정입니다. 예를 들어, 주가의 시간에 따른 변화나 무작위하게 이동하는 입자의 경로 등이 확률 과정의 사례입니다.
	- 예를 들어 동전을 던져 앞이 나오면 앞으로 한 칸, 뒤가 나오면 뒤로 한 칸 간다고 할 때, 시간 t에 대한 위치를 $X(t)$라고 하자. 이것이 확률 과정이다.
		- $X(100)$를 구하는 것은 쉽지 않다.(계산할 것이 많아짐)

![[강화학습_2-1_(250310)_page-0037.jpg]]
- $X(100)$을 알기 위해서 $X(99)$만 알면 된다. -> 이 논리가 Markov Process의 핵심이다.
	- $X(99)$ 이전의 모든 것은 메모리에 둘 필요가 없다는 점에서 유용하다.
	- 세상의 모든 상황을 마르코프 프로세스를 통해 시뮬레이션할 수 있다.

- 마르코프 체인은 마르코프 프로세스를 시각화하려는 맥락이다.


![[강화학습_2-1_(250310)_page-0038.jpg]]

![[강화학습_2-1_(250310)_page-0039.jpg]]
- markov process는 {State, Transition, Probability}의 set이다.

![[강화학습_2-1_(250310)_page-0040.jpg]]
- memoryless property 덕분에 슬라이드같은 그림을 그릴 수 있는 것이다.
	- 현재 상태(Sun 또는 Rain)만 알면 미래 상태를 예측할 수 있으며, 이전의 상태들은 고려할 필요가 없습니다. 설명한 대로, 이전의 상태가 무엇이었는지 모르더라도 현재 상태로부터 다음 상태의 확률을 계산할 수 있기 때문에 이러한 과정이 가능합니다.
![[강화학습_2-1_(250310)_page-0041.jpg]]
- Stationary distribution
	- 마코프 체인의 상태가 시간이 지남에 따라 변해도 분포가 변하지 않는 확률 분포를 말합니다. 이 분포에서는 각 상태에 존재할 확률이 시간이 지나도 일정하게 유지됩니다. 즉, 마코프 체인이 충분히 오래 진행되었을 때, 상태들의 점유 분포가 이러한 해석적 분포에 도달하게 됩니다. 이는 확률 전이 행렬을 사용하여 구할 수 있으며, 안정된 상태로 시스템이 수렴했을 때의 상태 분포를 나타냅니다.



^250310
![[강화학습_2-1_(250310)_page-0042.jpg]] ^60afd2

![[강화학습_2-1_(250310)_page-0043.jpg]]

![[강화학습_2-1_(250310)_page-0044.jpg]]

![[강화학습_2-1_(250310)_page-0045.jpg]]

![[강화학습_2-1_(250310)_page-0046.jpg]]

![[강화학습_2-1_(250310)_page-0047.jpg]]
- 어떤 state가 유지될 확률은 '수렴'한다.
- Stationary distribution
	- Stationary probabilistic은 어떤 시스템을 불현듯 쳐다봤을 때 그 시스템이 '어떤 상황'일 확률이 '어떻게' 되는지에 대한 이야기이다.
	- sd는 saturate한 값을 가지는 가에 대한 이야기이다??
	- "정상 분포(stationary distribution)"는 확률 과정, 특히 마르코프 체인에서 자주 사용하는 개념입니다. 이는 시간에 따라 변화하지 않는 확률 분포를 의미합니다. 
		- 즉, 마르코프 체인이 충분히 오래 진행되면, 어떤 초기 상태에서 시작하더라도 시간이 지남에 따라 이 분포에 수렴하게 됩니다.
	- 여기서 "saturate"한 값을 가진다는 설명은, 시간이 무한대로 갈 때 이 분포가 수렴하는 일정한 상태를 가리킵니다. 즉, 체인이 시간이 지남에 따라 특정 분포에 도달하여 그 이후에는 변하지 않는 상태가 되는 것을 의미합니다. 이는 마르코프 체인의 성질에 따라 적절한 조건이 주어졌을 때 발생합니다.

- Saturate 
	- **Saturate의 사전적 의미**:
		- **포화시키다**: 어떤 물질이나 상태가 더 이상 흡수하거나 담을 수 없을 만큼 꽉 찬 상태를 의미합니다.
	
	- **이 문맥에서의 뉘앙스**:
		- **수렴 및 안정 상태**: "saturate"라는 용어가 사용되면, 시스템이 시간에 따라 특정한 값을 향해 수렴하고, 그 이후에는 더 이상 변하지 않는 상태가 된다는 의미입니다. 이는 마르코프 체인의 경우 불변 분포에 도달하여, 시간이 지나도 변화가 없는 안정된 상태가 되는 것을 강조하는 표현입니다. 시스템이 더 이상 변화하지 않고, 안정적인 상태에 도달했음을 나타냅니다.
![[강화학습_2-1_(250310)_page-0048.jpg]]

![[강화학습_2-1_(250310)_page-0049.jpg]]
- 이렇게 transition matrix를 극단적으로 만든다면 Diverge(발산)할 수 있다.
![[강화학습_2-1_(250310)_page-0050.jpg]]

![[강화학습_2-1_(250310)_page-0051.jpg]]
- 어떤 노드 'j'로부터 나가는 것(엣지)의 확률의 합은 들어오는 것의 확률의 합과 같다.
	- 이것이 Stationary distribution의 특징이다.
- 모든 Stationary distribution의 합은 1이다.
	- 시스템은 어떤 State던지간에 존재해야만 하기 때문이다.
![[강화학습_2-1_(250310)_page-0052.jpg]]
- 들어오는 것의 합과 나가는 것의 합이 같다 -> Balance equation


![[강화학습_2-1_(250310)_page-0053.jpg]]
- 이렇게 state의 개수가 많으면 Stationary matrix의 스스로 행렬곱이 매우 힘들다.
	- 이 때 outbound와 inbound가 같다는 것으로 계산을 할 수 있고 ... ???