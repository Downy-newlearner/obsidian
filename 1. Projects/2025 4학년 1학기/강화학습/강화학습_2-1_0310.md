![[강화학습_2-1_(250310)_page-0001 1.jpg]]

![[강화학습_2-1_(250310)_page-0002.jpg]]
- Machine Learning이란 
	- for Performance
	- at Task
	- with Experience
	- P, T, E
![[강화학습_2-1_(250310)_page-0003.jpg]]

![[강화학습_2-1_(250310)_page-0004.jpg]]

![[강화학습_2-1_(250310)_page-0005.jpg]]

![[강화학습_2-1_(250310)_page-0006.jpg]]
- Anomaly
	- 일반적인 패턴이나 기대치에서 벗어난 특이한 데이터를 의미한다.
![[강화학습_2-1_(250310)_page-0007.jpg]]

![[강화학습_2-1_(250310)_page-0008.jpg]]

![[강화학습_2-1_(250310)_page-0009.jpg]]

![[강화학습_2-1_(250310)_page-0010.jpg]]

![[강화학습_2-1_(250310)_page-0011.jpg]]

![[강화학습_2-1_(250310)_page-0012.jpg]]

![[강화학습_2-1_(250310)_page-0013.jpg]]

![[강화학습_2-1_(250310)_page-0014.jpg]]

![[강화학습_2-1_(250310)_page-0015.jpg]]

![[강화학습_2-1_(250310)_page-0016.jpg]]

![[강화학습_2-1_(250310)_page-0017.jpg]]

![[강화학습_2-1_(250310)_page-0018.jpg]]

![[강화학습_2-1_(250310)_page-0019.jpg]]

![[강화학습_2-1_(250310)_page-0020.jpg]]

![[강화학습_2-1_(250310)_page-0021.jpg]]
- 강화학습
	- 시도와 에러를 통해 발전하는 과정
	- 연속적인 결정 생성 문제에서 누적 보상을 최대화하기 위해 시도와 에러를 통해 행동을 교정해나가는 과정

![[강화학습_2-1_(250310)_page-0022.jpg]]
- 순차적인 액션 셋을 계속 실행한다.
- 강화학습은 일련의 decision making이다.

![[강화학습_2-1_(250310)_page-0023.jpg]]

![[강화학습_2-1_(250310)_page-0024.jpg]]
- Agent에게 state와 reward를 준다.

![[강화학습_2-1_(250310)_page-0025.jpg]]

![[강화학습_2-1_(250310)_page-0026.jpg]]

![[강화학습_2-1_(250310)_page-0027.jpg]]

![[강화학습_2-1_(250310)_page-0028.jpg]]

![[강화학습_2-1_(250310)_page-0029.jpg]]

![[강화학습_2-1_(250310)_page-0030.jpg]]

![[강화학습_2-1_(250310)_page-0031.jpg]]

![[강화학습_2-1_(250310)_page-0032.jpg]]

![[강화학습_2-1_(250310)_page-0033.jpg]]

![[강화학습_2-1_(250310)_page-0034.jpg]]


## Markov Process
![[강화학습_2-1_(250310)_page-0035.jpg]]
- 마르코프 프로세스는 강화학습의 토대이다.

![[강화학습_2-1_(250310)_page-0036.jpg]]
- Stochastic process
	- 랜덤 프로세스라고도 부른다.
	- Random variable에 시간의 개념이 추가된 것이다.
	- **확률 과정(Stochastic process 또는 Random process)** 는 시간에 따라 변하는 확률 변수들의 집합으로 정의됩니다. 이는 각 시간 포인트에서 확률 변수가 값을 취하는 방식으로, 이러한 값의 변화가 확률적으로 결정되는 과정입니다. 예를 들어, 주가의 시간에 따른 변화나 무작위하게 이동하는 입자의 경로 등이 확률 과정의 사례입니다.
	- 예를 들어 동전을 던져 앞이 나오면 앞으로 한 칸, 뒤가 나오면 뒤로 한 칸 간다고 할 때, 시간 t에 대한 위치를 $X(t)$라고 하자. 이것이 확률 과정이다.
		- $X(100)$를 구하는 것은 쉽지 않다.(계산할 것이 많아짐)

![[강화학습_2-1_(250310)_page-0037.jpg]]
- $X(100)$을 알기 위해서 $X(99)$만 알면 된다. -> 이 논리가 Markov Process의 핵심이다.
	- $X(99)$ 이전의 모든 것은 메모리에 둘 필요가 없다는 점에서 유용하다.
	- 세상의 모든 상황을 마르코프 프로세스를 통해 시뮬레이션할 수 있다.

- 마르코프 체인은 마르코프 프로세스를 시각화하려는 맥락이다.


![[강화학습_2-1_(250310)_page-0038.jpg]]

![[강화학습_2-1_(250310)_page-0039.jpg]]
- markov process는 {State, Transition, Probability}의 set이다.

![[강화학습_2-1_(250310)_page-0040.jpg]]
- memoryless property 덕분에 슬라이드같은 그림을 그릴 수 있는 것이다.
	- 현재 상태(Sun 또는 Rain)만 알면 미래 상태를 예측할 수 있으며, 이전의 상태들은 고려할 필요가 없습니다. 설명한 대로, 이전의 상태가 무엇이었는지 모르더라도 현재 상태로부터 다음 상태의 확률을 계산할 수 있기 때문에 이러한 과정이 가능합니다.
![[강화학습_2-1_(250310)_page-0041.jpg]]
- Stationary distribution
	- 마코프 체인의 상태가 시간이 지남에 따라 변해도 분포가 변하지 않는 확률 분포를 말합니다. 이 분포에서는 각 상태에 존재할 확률이 시간이 지나도 일정하게 유지됩니다. 즉, 마코프 체인이 충분히 오래 진행되었을 때, 상태들의 점유 분포가 이러한 해석적 분포에 도달하게 됩니다. 이는 확률 전이 행렬을 사용하여 구할 수 있으며, 안정된 상태로 시스템이 수렴했을 때의 상태 분포를 나타냅니다.


![[강화학습_2-1_(250310)_page-0042.jpg]]

![[강화학습_2-1_(250310)_page-0043.jpg]]

![[강화학습_2-1_(250310)_page-0044.jpg]]

![[강화학습_2-1_(250310)_page-0045.jpg]]

![[강화학습_2-1_(250310)_page-0046.jpg]]

![[강화학습_2-1_(250310)_page-0047.jpg]]
- 어떤 state가 유지될 확률은 '수렴'한다.
![[강화학습_2-1_(250310)_page-0048.jpg]]

![[강화학습_2-1_(250310)_page-0049.jpg]]

![[강화학습_2-1_(250310)_page-0050.jpg]]

![[강화학습_2-1_(250310)_page-0051.jpg]]

![[강화학습_2-1_(250310)_page-0052.jpg]]

![[강화학습_2-1_(250310)_page-0053.jpg]]
