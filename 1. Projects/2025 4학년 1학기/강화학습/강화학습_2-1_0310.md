![[강화학습_2-1_(250310)_page-0001 1.jpg]]

![[강화학습_2-1_(250310)_page-0002.jpg]]

![[강화학습_2-1_(250310)_page-0003.jpg]]

![[강화학습_2-1_(250310)_page-0004.jpg]]

![[강화학습_2-1_(250310)_page-0005.jpg]]

![[강화학습_2-1_(250310)_page-0006.jpg]]

![[강화학습_2-1_(250310)_page-0007.jpg]]

![[강화학습_2-1_(250310)_page-0008.jpg]]

![[강화학습_2-1_(250310)_page-0009.jpg]]

![[강화학습_2-1_(250310)_page-0010.jpg]]

![[강화학습_2-1_(250310)_page-0011.jpg]]

![[강화학습_2-1_(250310)_page-0012.jpg]]

![[강화학습_2-1_(250310)_page-0013.jpg]]

![[강화학습_2-1_(250310)_page-0014.jpg]]

![[강화학습_2-1_(250310)_page-0015.jpg]]

![[강화학습_2-1_(250310)_page-0016.jpg]]

![[강화학습_2-1_(250310)_page-0017.jpg]]

![[강화학습_2-1_(250310)_page-0018.jpg]]

![[강화학습_2-1_(250310)_page-0019.jpg]]

![[강화학습_2-1_(250310)_page-0020.jpg]]

![[강화학습_2-1_(250310)_page-0021.jpg]]

![[강화학습_2-1_(250310)_page-0022.jpg]]
- 순차적인 액션 셋을 계속 실행한다.
- 강화학습은 일련의 decision making이다.

![[강화학습_2-1_(250310)_page-0023.jpg]]

![[강화학습_2-1_(250310)_page-0024.jpg]]
- Agent에게 state와 reward를 준다.

![[강화학습_2-1_(250310)_page-0025.jpg]]

![[강화학습_2-1_(250310)_page-0026.jpg]]

![[강화학습_2-1_(250310)_page-0027.jpg]]

![[강화학습_2-1_(250310)_page-0028.jpg]]

![[강화학습_2-1_(250310)_page-0029.jpg]]

![[강화학습_2-1_(250310)_page-0030.jpg]]

![[강화학습_2-1_(250310)_page-0031.jpg]]

![[강화학습_2-1_(250310)_page-0032.jpg]]

![[강화학습_2-1_(250310)_page-0033.jpg]]

![[강화학습_2-1_(250310)_page-0034.jpg]]


## Markov Process
![[강화학습_2-1_(250310)_page-0035.jpg]]
- 마르코프 프로세스는 강화학습의 토대이다.

![[강화학습_2-1_(250310)_page-0036.jpg]]
- Stochastic process
	- 랜덤 프로세스라고도 부른다.
	- Random variable에 시간의 개념이 추가된 것이다.
	- **확률 과정(Stochastic process 또는 Random process)** 는 시간에 따라 변하는 확률 변수들의 집합으로 정의됩니다. 이는 각 시간 포인트에서 확률 변수가 값을 취하는 방식으로, 이러한 값의 변화가 확률적으로 결정되는 과정입니다. 예를 들어, 주가의 시간에 따른 변화나 무작위하게 이동하는 입자의 경로 등이 확률 과정의 사례입니다.
	- 예를 들어 동전을 던져 앞이 나오면 앞으로 한 칸, 뒤가 나오면 뒤로 한 칸 간다고 할 때, 시간 t에 대한 위치를 $X(t)$라고 하자. 이것이 확률 과정이다.
		- $X(100)$를 구하는 것은 쉽지 않다.(계산할 것이 많아짐)

![[강화학습_2-1_(250310)_page-0037.jpg]]
- $X(100)$을 알기 위해서 $X(99)$만 알면 된다. -> 이 논리가 Markov Process의 핵심이다.
	- $X(99)$ 이전의 모든 것은 메모리에 둘 필요가 없다는 점에서 유용하다.
	- 세상의 모든 상황을 마르코프 프로세스를 통해 시뮬레이션할 수 있다.

- 마르코프 체인은 마르코프 프로세스를 시각화하려는 맥락이다.


![[강화학습_2-1_(250310)_page-0038.jpg]]

![[강화학습_2-1_(250310)_page-0039.jpg]]
- markov process는 {State, Transition, Probability}의 set이다.

![[강화학습_2-1_(250310)_page-0040.jpg]]
- memoryless property 덕분에 슬라이드같은 그림을 그릴 수 있는 것이다.
	- 이전의 state가 rain인지, sun인지만 안다면 다음 state가 어떻게 될 것인지의 확률을 계산할 수 있기 때문이다.
![[강화학습_2-1_(250310)_page-0041.jpg]]

![[강화학습_2-1_(250310)_page-0042.jpg]]

![[강화학습_2-1_(250310)_page-0043.jpg]]

![[강화학습_2-1_(250310)_page-0044.jpg]]

![[강화학습_2-1_(250310)_page-0045.jpg]]

![[강화학습_2-1_(250310)_page-0046.jpg]]

![[강화학습_2-1_(250310)_page-0047.jpg]]

![[강화학습_2-1_(250310)_page-0048.jpg]]

![[강화학습_2-1_(250310)_page-0049.jpg]]

![[강화학습_2-1_(250310)_page-0050.jpg]]

![[강화학습_2-1_(250310)_page-0051.jpg]]

![[강화학습_2-1_(250310)_page-0052.jpg]]

![[강화학습_2-1_(250310)_page-0053.jpg]]
