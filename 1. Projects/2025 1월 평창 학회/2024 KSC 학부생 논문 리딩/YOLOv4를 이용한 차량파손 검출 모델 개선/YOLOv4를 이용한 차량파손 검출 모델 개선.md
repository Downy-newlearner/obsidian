**쟁점**: VGGNet을 vs U-Net vs Weproove.AI vs YOLOv4

**적용 주제**: 

**용어**: 

**논문의 허점**:

1. Weproove.AI
	WeProov.AI의 차량 파손 검출 모델[5]은 관련 연구들 중 차량 부위 및 손상 종류를 가장 세분화 한 모델에 속한다. 차량 부위는 44개의 부분으로 나누어 구분하고, 손상 종류는 유리깨짐, 스크래치, 덴트 등, 총 18개의 종류로 분류한다. 학습에 이용 한 데이터 또한 100만 장 이상이며, 24 가지의 모듈 을 통합한 자체적인 알고리즘을 제작하여 차량의 파손부위와 종류를 검출한다. 인식률이 매우 우수 하고 디테일한 판정결과를 출력하는 장점이 있으 나 검출하는데 소요되는 시간이 평균 2분 정도로 매우 느리다는 한계가 있다.

2. 이미지 분할 + U-Net(쏘카)
	이미지 분할과 U-Net[9]을 이용한 딥러닝 모델 [6]은 차량 대여 어플인 쏘카에서 2019년 진행되었 던 프로젝트이다. 이 모델은 입력 이미지에서 차량 이 존재하는 영역만을 선별하고 파손 존재 여부를 판단한다. 파손 존재 시, 이미지 분할 네트워크를 통하여 파손의 종류를 스크래치, 찌그러짐, 이격, 해 당없음 등으로 나누어 판정한다. 출력 데이터는 이 미지 단위 파손 존재 여부와 픽셀 단위 파손 종류에 대한 인덱스이다. 학습 데이터 수는 육안으로 파손 여부를 확실하게 판단할 수 있는 이미지 2,000장이 고 이미지 분석 시 이미지 분할 기법에 사용되는 연산량을 줄이기 위하여 U-Net를 사용한다. 파손 검출 성능은 96%의 정확도와 96%의 F1-score를 보인다. 쏘카 모델은 이미지 화소 단위로 확인하는 이미지 분할 기술을 활용하기 때문에 작은 파손도 감지할 수 있는 장점이 있지만 네트워크를 통과할 때 층이 깊고 연산량이 많다는 문제점을 보인다.

검출 시간을 낮추고 디테일한 파손 검출 정보를 제공하기 위해 YOLOv4를 사용함.
기존 YOLOv4에는 부위별 파손현황을 검출하지 못하는 한계가 있다.(바운딩 박스의 포함관계를 계산할 수 없음)
	어디가 앞문이고, 어디가 뒷문이고, 어디가 찌그러졌고 이렇게 각각은 알 수 있지만 앞문이 찌그러졌고, 뒷문이 긁혔고 이런 식으로 포함관계는 알 수 없다는 뜻이다.

그래서 부위별 파손현황을 도출하는 알고리즘을 구현하여 YOLOv4의 image.c 파일에 적용한다.
![[Pasted image 20241224163821.png]]
b1: 부위, 중심점은 $(x, y)$
b2: 파손, 중심점은 $(x_1, y_1)$
