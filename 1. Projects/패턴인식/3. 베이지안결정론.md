---
Lecture date: 2024-11-04
tags:
---
# 10월 23일 강의
Q. 베이지안 이론은 왜 필요한가?(어디에 사용하는가?)
A. 
- *알고있는 정보*(사전 확률, likelihood, normalization constant)를 이용하여 *알고싶은 정보*(후험 확률)를 구하기 위해서 필요하다.
- 다시 말해, 모든 클래스에 대해서 아래 정보들을 알고 있다면, 새로운 데이터가 들어왔을 때 어떤 클래스에 속할지 예측(합리적인 결정, optimal decision)하는 것이 가능하다.
	1. 랜덤 샘플링을 했을 때, 해당 클래스가 나올 확률(사전 확률)
	2. 해당 클래스에서 랜덤 샘플링을 했을 때 x가 나올 확률(likelihood)


Q. 베이지안 이론을 한 문장으로 정의하면 무엇인가?
A. 베이지안 이론은 사건이 일어날 확률을 알기 위해, 이미 알고 있는 정보(사전 확률)와 새로운 데이터를 결합해 확률을 갱신하는 방법이다.


![[베이지안결정론_page-0001.jpg]]

![[베이지안결정론_page-0002.jpg]]
데이터, 클래스, 그 안의 확률값들
이것을 데이터의 확률분포라고 한다.
PDF를 계산하는 것이 분석의 끝이다.
p(x)는 하나의 함수 형태인데 이것을 추정하기 위해서 우리가 잘 알고있는 함수를 이용하여 추정하는 것이다.(parametric method)
근데 근사할 수 있는 적당한 함수가 없는 경우에는 nonparametric method를 사용한다.

PDF를 구하면 확률이론에 의해서 분류기(classifier)를 만들 수 있다.
확률값을 기반으로 해서 분류기를 만든 것을 베이지안 classifier라고 한다.

*알고 있는 정보(학습에 이용하는 정보)*
1. 다이아몬드의 반짝임 정도 데이터들 $p(x|C_1)$
2. 큐빅의 반짝임 정도 데이터들 $p(x|C_2)$
3. 다이아몬드인지, 큐빅인지 모르는 무언가의 반짝임 정도 $x_{new}$

*알고 싶은 정보(인식할 정보)*
그 무언가가 다이아몬드일 확률 $p(C_1|x_{new})$
그 무언가가 큐빅일 확률 $p(C_2|x_{new})$
최종적으로 그 무언가가 무엇인지에 대한 인식 결과 $y(x_{new})$
	최종 인식 결과는 확률적으로 적절한 판단의 결과이다.(다이아와 큐빅일 확률 중 높은 것을 선택한다.)

![[베이지안결정론_page-0003.jpg]]
C1에서 x가 나올 확률, C2에서 x가 나올 확률, C3에서 x가 나올 확률을 학습한다.
새로운 $x_{new}$가 들어오면 $x_{new}$에서 C1, C2, C3가 나올 확률을 구할 수 있다.


## Bayesian Decision Theory란?
![[베이지안결정론_page-0004.jpg]]



![[베이지안결정론_page-0005.jpg]]
**베이지안 이론에서의 용어**
1. *사전 확률(Prior probability)*: $P(C_i)$

2. *Likelihood* : $P(x|C_i)$
	어떤 클래스 $C_i$에서 샘플을 수집할 때, 임의의 값 x가 나타날 확률 분포이다.

3. *후험 확률(Posterior probability): $P(C_i|x)$*
	x가 주어졌을 때 x가 Ci에 속할 확률
	베이지안 이론을 통해 얻고자 하는 '결과'

4. *정규화 상수(Normalization constant): $P(x)$*
	- $P(x)$는 결정(Decision)을 내리는데 영향을 끼치지 않는다.
		왜냐하면 결정($x$가 주어졌을 때, $x$가 어느 클래스에 속하는지에 대한 최선의 선택)은 ![[Pasted image 20241207140713.png]]
		의 상대적인 비교이고, 각 클래스마다 분모에 $P(x)$는 공통으로 가지고 있기 때문이다.
	- 정규화 상수라는 이름은 정확히 말하면 "확률을 만들기 위한 정규화 상수"이다.
		위 이유로 인해서, $x$가 어느 클래스에 속하는지 결정하기 위한 클래스간의 비교는 $P(C_i)P(x|C_i)$ 에서 발생하지만, $\Sigma P(C_i)P(x|C_i)$은 1이 아닌 $P(x)$이다.
		확률은 총 합이 1이 되어야만하므로, $P(C_i)P(x|C_i)$를 '정규화 상수'를 나누어 정규화를 해야만 우리가 집중하는 '후험 확률'이 확률의 조건을 비로소 만족할 수 있다.
	- 우도(likelihood)를 계산할 때 필요한 값이라는 관점도 괜찮다.


![[베이지안결정론_page-0006.jpg]]
[[Likelihood ratio R]]을 알아보자. ^43ddc1

**베이지안 결정**
클래스가 i, j, 두 개만 존재하는 경우에 Likelihood ratio R을 이용해 결정을 내릴 수 있다.
R이 1보다 크냐 작냐를 따지면 된다.

사실 말이 Likelihood ratio지, 후험 확률의 비이고  이는 곧 $P(C_i)P(x|C_i)$와 $P(C_j)P(x|C_j)$의 비이다.이다.

Likelihood ratio를 쓰는 경우는 클래스가 두 개인 경우이므로, 전제인 Total probability theory에 의해서 두 클래스에 대한 후험 확률의 합은 1이다.($\because$두 클래스를 합친 것이 전체이다.)
그러므로 R을 구하면 아래 두 식을 확보한 셈이다.
	1. $\frac{P(A|x)}{P(B|x)} = R$
	2. $P(A|x) + P(B|x) = 1$
두 식을 연립해서 두 후험 확률을 모두 구할 수 있다.
![[Pasted image 20241207142827.png]]

![[베이지안결정론_page-0007.jpg]]


## Continuous Densities
![[베이지안결정론_page-0008.jpg]]



### ACT 예제 1: a single feature
![[베이지안결정론_page-0009.jpg]]
학생의 성적을 보고 5년 안에 졸업을 할 수 있을지, 없을지 예측하는 것이 예제의 주제이다.

*주어진 정보*
과거 학생들 중 5년 안에 졸업한 학생들의 ACT 점수 분포
	가우시안 분포를 따름
	평균: 26
	분산: 2
$\rightarrow P(x|G) = N(26,2)$


과거 학생들 중 5년 안에 졸업하지 못한 학생들의 ACT 점수 분포
	가우시안 분포를 따름
	평균: 22
	분산: 3
$\rightarrow P(x|\bar{G}) = N(22,3)$

어떤 학생의 ACT 점수가 22점이라면 이 학생은 5년 안에 졸업할 수 있을까?

*알고싶은 정보*
$P(G|x=22)$

*주어진 정보를 통해 얻을 수 있는 정보*
$P(x=22|G)$
$P(x=22|\bar{G})$



![[베이지안결정론_page-0010.jpg]]



## Multiple Features
![[베이지안결정론_page-0011.jpg]]
멀티 피처가 되면서 싱글 피처와 달라지는 점은 x가 벡터가 되는 것이다.

### ACT 예제 2: multiple features(bivariate normal density)
![[베이지안결정론_page-0012.jpg]]

![[베이지안결정론_page-0013.jpg]]
중간고사 범위
***
# 11월 4일 강의
### 복습 북마크
여기 복습해야함
태블릿 필기 따라서 복습하기
또한, 저 시그마 행렬이 단일 변수 가우시안 분포에서 다중 변수 가우시안 분포가 될 때 생긴 것인데, 어떻게 생겨났고 작용하는지 알아내기

많은 경우에 피처가 늘어나면 성능이 늘어난다.
d개의 피처에 대한 노멀 density의 경우로 연습을 해보자.


- [x] 1차 학습: 2024-11-09
- [x] 1차 복습: 2024-12-07
- [ ] 2차 복습:
![[베이지안결정론_page-0014.jpg]]
[[d 차원 벡터]]는 d차원 위의 한 점을 가리킨다.
피처가 추가가되면 대부분의 경우의 분별력이 높아지는 경향이 있다.
피처가 d개까지 늘어나면 어떻게 되는지 확인한다.(Normal density의 예시로 확인한다.)

이 예제는 $\rho_{xy}$가 0이 아니므로, x와 y는 correlation이 있다.
만약 $\rho_{xy} = 0$이라면 x와 y는 correlation이 없는 것이고, 이 때, $\sum_G = \begin{bmatrix} \mu_x^2 & 0 \\ 0 & \mu_y^2 \end{bmatrix}$이 되고 이는 Diagonal Matrix라고 불린다.

Normal density에 가장 큰 연산은 [[det (행렬식, Determinant)]]와 역행렬 구하는 연산이다. 
Diagonal Matrix는 이를 쉽게 구할 수 있으므로, 계산량에서 매우 큰 이점이 존재한다.

참고로 x, y가 독립이면 서로 uncorrelated하다.
현실적으로 x, y가 독립인 경우가 거의 존재하지 않지만, correlation이 크지 않다면 독립이라고 가정해서 연산량에서 이득을 보는 쪽을 선택하곤 한다.

## Conditionally Independent Features
![[베이지안결정론_page-0015.jpg]]
![[Pasted image 20241109152440.png]]
d개의 피처가 존재하고, 각 피처가 가질 수 있는 값이 V개 그리고 k개의 클래스가 존재한다면 경우의 수는 $kV^d$이다.
하지만 각 피처가 독립이라는 전제를 세운다면 경우의 수는 $kVd$개로 대폭 감소한다.
이렇게 연산량의 차이가 존재한다.


![[베이지안결정론_page-0016.jpg]]
독립을 가정해서 연산하는 것의 단점은, 오류에 의한 Bias가 존재할 수 있다는 점이다.
그러나 계산상의 이득이 크다면 어느정도 bias는 감수할 수도 있다.

주의해야할 것은, Uncorrelated = independent가 아니라는 것이다.


![[베이지안결정론_page-0017.jpg]]
 *(a)*
 클래스 A와 B 안에서는 각각 x와 y가 서로 독립이다.
 하지만 전체로서는 x와 y는 독립이 아니다.(positively correlative)
	 $\because$전체에는 A와 B가 포함되어있기 때문이다.

*(b)*
전체로 봤을 때는 x, y는 서로 독립이지만, 클래스 안에서 봤을 때는 독립이 아니다
A안에서는 x가 커짐에 따라 y가 가질 수 있는 값의 범위가 작아진다.
반대로, B안에서는 x가 커짐에 따라 y가 가질 수 있는 값의 범위가 커진다.
하지만 전체(A, B 모두)를 봤을 때는 x, y는 서로 독립이다.


### ACT 예제 3: multiple features
예제 교훈: 피처가 늘어날 수록 더욱 정확한 예측(decision)이 가능하다.

![[베이지안결정론_page-0018.jpg]]
앞의 예제에서는 피처가 ACT 점수, 내신 점수 이렇게 두 개 였다.
이 예제에서는 에세이 점수라는 피처를 추가한다.([1~10]의 값을 가진다.)


![[베이지안결정론_page-0019.jpg]]
feature가 한 개 일 때는 졸업할 수 있는 확률을 44%로, 두 개 일 때는 10%, 이제 세 개 일 때 7%라고 예측했다.


### Detecting HIV with ELISA 예제
테스트를 2번 하는 경우이다.
각 테스트는 독립적이다.
![[베이지안결정론_page-0020.jpg]]
이전에는 한 번의 검사였고 이번에는 검사를 두 번 진행헀다.
이 각 두번의 검사는 서로 독립적이다.
두 검사가 서로 독립적이라서 이렇게 확률이 크게 뛸 수 있었다.


### x, y, z의 값에 따른 클래스 예측(decision) 예제
$\vec{X} = (0,1,4)$일 때 A, B, C 클래스 중 어느 클래스일지 예측하는 문제이다.
x, y, z는 서로 독립이므로 어렵지 않게 구할 수 있다.
x, y, z가 독립인 경우의 경우의 수와, 독립이 아닐 때 경우의 수를 계산하는 연습을 해야한다.
![[베이지안결정론_page-0021.jpg]]
샘플 19, 클래스 3개, 피처 3개
피처
	x는 0 or 1
	y는 -1 or 0 or 1
	z는 3 or 4
![[베이지안결정론_page-0022.jpg]]

## Decision Regions
![[베이지안결정론_page-0023.jpg]]
**번역**:
- **결정 영역**에 특정 특성 값 $x$를 가진 샘플이 포함되어 있으면, 해당 샘플을 그 영역에 할당된 클래스로 분류한다.
    - **결정 경계**: 결정 영역 사이의 경계
        - 최적의 결정 경계는 **특성 공간**을 결정 영역 $R_1, \ldots, R_k$로 분리한다.
        - 두 클래스 $A$와 $B$ 사이의 최적 결정 경계 계산:
            - $P(A|x) - P(B|x) \Rightarrow P(A)p(x|A) - P(B)p(x|B)$
            - x인데 A에 속해있을 확률 = A의 확률 \* A에서 x가 있을 확률
**설명**:
- **결정 영역**은 특정 클래스에 해당하는 특성 값의 범위로, 샘플의 특성 값이 어느 결정 영역에 속하는지에 따라 해당 클래스로 분류한다.
- **결정 경계**는 두 결정 영역 간의 경계를 나타내며, 클래스 간의 분리 기준 역할을 한다.
- **최적의 결정 경계**는 특성 공간을 여러 결정 영역으로 나누어, 샘플이 속할 클래스를 최적화된 방식으로 결정한다.
- **두 클래스 $A$와 $B$ 사이의 결정 경계**는 조건부 확률 $P(A|x)$와 $P(B|x)$를 통해 계산하며, 이는 주어진 $x$에 대해 $A$와 $B$ 중 어느 클래스에 속할 가능성이 높은지 판단하기 위해 사용된다.

**요약**:  
결정 경계는 특성 공간에서 각 클래스에 해당하는 영역을 분리하는 경계로, 샘플을 적절한 클래스로 분류하기 위해 사용된다.


Decision region은 피처 벨류 x를 가지고있는데 x를 가지고 있는 샘플이 decision region안에 있다면 이는 그 region안에 있다고 분류된다.
그리고 decision region을 결정하는 경계선을 decision boundary라고 한다.
sample space를 decision region들로 나누는데 이 때 오류가 가장 적게 하는 decision boundary를 optimal dicision boundary이다.

각 클래스에 속할 확률이 같게 되는 지점을 dicision boundary라고 한다.


### geiger 예측기를 통한 방사성 광물 예측 예제
![[베이지안결정론_page-0024.jpg]]
*문제*
geiger(가이거) 예측기에서 방사성 광물을 예측한다. (광물 A 또는 광물 B)
1초 동안 몇 번의 핵 분열을 하는지를 바탕으로 A 또는 B를 예측한다.
광물이 핵분열을 일으키는 상황은 푸아송 RV로 모델링할 수 있다.

A인지 B인지 모르는 광물을 가이거 예측기로 예측해봤더니 1초에 5번 핵분열을 일으킨다.
A에 대한 prior probability는 0.4로 알고있는 상황이다.

이 광물이 A일 확률은?

*풀이*
A의 Decision region이 전체 중 0.4, B는 0.6이므로 $P(A) = 0.4, P(B) = 0.6$이다.
$P(n=5|A), P(n=5|B)$는 푸아송RV의 모델로 표현할 수 있다. 

[[Likelihood Ratio R]]을 두 푸아송 RV로 표현한 확률의 비로 구한 후 $P(A|n), P(B|n)$을 구할 수 있다.
	R을 굳이 사용하는 이유는 비율인 R을 구하는 과정에서 연산이 쉽게 해결돼서인 것 같다.

*참고*
[[#^43ddc1|Likelihood Ratio R]]


![[베이지안결정론_page-0025.jpg]]
DB는 후행확률이 같은 지점인데, 이 슬라이드는 후행 확률의 한 인수인 $P(C_i)$가 같은  경우이다.
	그래서 figure를 보면 $P(x|C_i)$만 비교한다.

$P(C_i|x,y) = P(C_j|x,y)$를 만족시키는 라인을 그리면 Decision Boundary이다.



![[베이지안결정론_page-0026.jpg]]
위의 예시는 $P(C_1) = P(C_2)$인 경우였고 지금 예시는 다른 경우다.
그렇다면 $P(C_1)과 P(C_2)$에 가중치가 곱해진 꼴로 생각하면 된다.


![[베이지안결정론_page-0027.jpg]]



![[베이지안결정론_page-0028.jpg]]
노말 density의 decision boundary의 모양을 봤더니 규칙적인 현상을 발견했다.
	cov 매트릭스가 어떻게 생겼냐에 따라서 $P(x|C_i)$의 모양이 달라지는데 만약 피처들이 서로 독립이라면  cov 매트릭스는 Diagonal matrix 형태를 가진다.
	![[Pasted image 20241109172054.png]]
	파란 분포 경우에는 가로방향으로 퍼져있는 것과 세로 방향으로 퍼져있는 정도가 같다.
	이는 두 피처에 대한 분산이 같다는 것이다.
	$\begin{bmatrix}\sigma_1^2 & 0 \\ 0 & \sigma_2^2\end{bmatrix} = \begin{bmatrix}\sigma_1^2 & 0 \\ 0 & \sigma_1^2\end{bmatrix} = \sigma_1\begin{bmatrix}1 & 0 \\ 0 & 1\end{bmatrix}$이다.
	
그래서 이 경우에는 decision boundary가 어떻게?
모든 클래스에 대한 cov가 동일하다면?
cov가 특수한 경우가 아닌 각각 독립적인 형태를 띈다면 어떻게 될까?
위 경우들을 다음 시간에 배운다.

# 11월 11일 강의
*복습*
디시전 바운더리를 얘기하면서 각 클래스들의 PDF가 노멀 density 형태일 때, 이를 규정하는 파라미터는 민과 covariance니까. *covariance에 따라서 두 개의 노멀 덴시티의 계형 유형이 나뉠 수 있다.*

1. 하나는 모든 클래스의 cov matrix가 identity matrix의 상수배와 동일하다면 디시전 바운더리가 어떤 특성을 보인다.

2. 또 하나는 identity matrix의 상수배의 형태는 아니지만 모든 클래스에 대한 cov matrix가 동일한 경우(이는 상관관계가 비슷하다는 거지 피쳐 값 자체가 동일하다는 의미는 아니다.) 어떤 디시전 바운더리가 특정 양상을 띈다.

3. 세 번는 조건 없이 일반적인 경우에는(cov matrix가 서로 다른 경우) 어떤지 오늘 알아볼 것이다.

위 경우들은 모두 노멀 덴시티의 경우이다.(다루기 쉬움)
노멀 덴시티를 가지는 분포들이 상당히 많이 있기 때문에 적용이 쉽게 가능하다.
노멀 덴시티를 띄지 않더라도, 노멀 덴시티 여러개를 가지고 불특정한 분포를 모델링할 수 있다.(가우시안 믹스쳐 분포)


## 가우시안 분포로 DB 구하기
이 예제는 사전확률이 같은 경우이다.
![[베이지안결정론_page-0027.jpg]]


#### 1. 모든 분포의 Covariance matrix가 동일하면서 Identity matrix의 상수배(공분산이 모두 diagonal하다.)인 경우(또한 서로 Independent)
mean과 가까운 클래스로 데이터가 decision된다.
![[베이지안결정론_page-0028.jpg]]
디시전 바운더리는 C1과 C2가 같아지는 지점을 이어 만든 구역이다.
	피겨의 두 분포를 각각 C1, C2라고 함(Class1, 2)

*이 경우에서 보장되는 것*
	$\Sigma_1 = \Sigma_2$
	$\Sigma_1^{-1} = \Sigma_2^{-1}$

*헷갈리면 안되는 것*
	Covariance matrix가 서로 같다는 것은 $C_1(x,y)$에서의 $x$와 $y$의 분포가 $C_2(x,y)$의  $x$와 $y$의 분포와 같다는 의미일 뿐, $C_1 = C_2$인 것은 아니다.

#### 2. 모든 클래스의 Covariance matrix가 같은 경우
![[베이지안결정론_page-0029.jpg]]
앞선 첫 번째 경우는 서로 독립을 가정한 경우라서 분포가 기울어지지 않았지만 이 경우는 Covariance를 가지는 경우이므로 분포가 기울어졌다.
	다만 같은 Covariance를 가진다는 가정이므로 같은 기울기로 기울었다.

*이 경우에서 보장되는 것*
	$\Sigma_1 = \Sigma_2$

*헷갈리면 안되는 것*
	$\Sigma_1^{-1} \neq \Sigma_2^{-1}$ 
		두 Covariance matrix($\Sigma$)는 Identify matrix의 상수배가 아니므로 공분산의 역행렬은 서로 다를 수 있다.

데이터 포인트가 속하는 Region을 판별할 때, 단순히 $\mu$와의 거리로 비교하면 오류가 발생할 수 있다.
	$\therefore$ "다른 measure의 거리"를 사용해야한다.

우리가 보통 사용하는 '거리'는 유클리드 거리인데 이 유클리드 거리만 '거리'인 것이 아니다.
예를 들어 맨해튼 거리가 존재한다.
	![[Pasted image 20241129163835.png|150]]

현재 경우에는 [[Mahalanobis distance]]를 사용한다.


#### 3. Covariance matrix에 어떠한 제한이 없는 경우(일반적인 경우)
![[베이지안결정론_page-0030.jpg]]
위의 첫 번째, 두 번째 경우와는 다르게 $l_i(x)$에서 소거되는 부분이 없으므로 Decision boundary 구하면 이차식으로 정리된다.
	즉 곡선 형태의 Decision boundary가 발생한다.



### 가우시안 분포 DB 구하는 예제
bivariate normal classes
각 분포에서 두 변수 x, y는 서로 독립이다.
![[베이지안결정론_page-0031.jpg]]

![[베이지안결정론_page-0032.jpg]]

![[베이지안결정론_page-0033.jpg]]
한 번 D를 만들어놓으면 D의 부호만 보고 classification이 가능하므로 편리하다


### ACT 예제
![[베이지안결정론_page-0034.jpg]]

![[베이지안결정론_page-0035.jpg]]



### ACT 예제 2: the same covariance matrix
두 클래스의 covariance matrix가 같을 때와 다를 때 contour가 어떻게 형성되는지 확인하기 위한 예제이다.
![[베이지안결정론_page-0036.jpg]]
figure 1은 앞의 예제고, figure 2가 지금 상황이다.
cov matrix가 같아서 DB 식 정리중에 이차식은 모두 소거되어 직선 DB가 나온다.

### Normal density class 한 쌍에 대한 DB 형태의 경우의 수 총정리
![[베이지안결정론_page-0037.jpg]]
DB는 맥시멈으로 나올 수 있는 항이 x^2, y^2, xy, x, y, 상수항




## d-dimensional Decision Boundaries in Matrix Notation
![[베이지안결정론_page-0038.jpg]]

![[베이지안결정론_page-0039.jpg]]

![[베이지안결정론_page-0040.jpg]]



### 같은 cov matrices를 갖는 경우 DB 구하는 예제
이전 슬라이드에서의의 A가 소거되므로 초평면으로 DB가 형성된다.

![[베이지안결정론_page-0041.jpg]]

## Unequal Costs of Errors

![[베이지안결정론_page-0042.jpg]]
 

![[베이지안결정론_page-0043.jpg]]

![[베이지안결정론_page-0044.jpg]]

![[베이지안결정론_page-0045.jpg]]

![[베이지안결정론_page-0046.jpg]]

![[베이지안결정론_page-0047.jpg]]

![[베이지안결정론_page-0048.jpg]]
### Estimation of Error Rates

![[베이지안결정론_page-0049.jpg]]

![[베이지안결정론_page-0050.jpg]]

![[베이지안결정론_page-0051.jpg]]

![[베이지안결정론_page-0052.jpg]]

교수님이 수업 끝나기 3분 전부터 총 정리를 하심
다시 듣고 정리하기
