---
Lecture date: 2024-11-18
tags:
---
![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0001.jpg]]
데이터가 *특정 분포를 따른다고 가정*하고, 그 분포를 설명하는 파라미터(평균, 분산 등)를 추정하는 방식이다.

- 장점
	- 계산 효율 좋음
	- 데이터가 모델을 잘 따르면, 모델 성능이 우수함
- 단점
	- 데이터가 가정한 분포와 맞지 않으면 성능이 떨어질 수 있음
	- 유연성이 부족하며, 복잡한 데이터 구조를 모델링하는 데 한계가 있다.
	
# 논파라메트릭 모델링
![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0002.jpg]]
데이터의 분포에 대한 가정을 하지 않고, 파라미터가 없거나 매우 적은 모델로 데이터를 자유롭게 설명하는 방식이다.

- 장점
	- 데이터의 사전 가정 없이 복잡한 분포도 잘 모델링 가능
	- 복잡한 데이터면 더 좋은 성능을 보일 수 있다.
- 단점
	- 데이터가 많아지면 계산량이 급격히 증가할 수 있음
	- 학습 느림, 계산 비용 큼
	- 오버피팅 발생 가능
## 히스토그램 접근
히스토그램은 확률의 approximation
![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0003.jpg]]
*단점*
bin을 어떻게 하냐에 따라 성능 차이가 나고, 이를 세우는 것에 정해진 기준이 없다.

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0004.jpg]]
bin을 정하는 나름의 규칙을 사람들이 세웠다.
	엄지룰
	cube root

차원의 저주가 존재한다.

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0005.jpg]]

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0006.jpg]]

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0007.jpg]]


### 히스토그램 예제 1
![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0008.jpg]]

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0009.jpg]]


### 히스토그램 예제 2
![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0010.jpg]]

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0011.jpg]]

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0012.jpg]]

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0013.jpg]]

## 커널과 윈도우 추정(Kernel and Window Estimators)
![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0014.jpg]]
샘플이 많이 발생한 지점의 확률이 높을 것이라는 추론에서 시작.
샘플이 발생하면 그 지역에 커널을 씌운다.(아래 figure들 참고)
	삼각형 또는 가우시안 커널을 가장 많이 사용한다.

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0015.jpg]]


### 커널 예시 1
그냥 그렇다~
![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0016.jpg]]

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0017.jpg]]

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0018.jpg]]
샘플 충분해야함
계산량 많음
윈도우 사이즈에 art 요소가 있다




## 가까운 이웃 분류(Nearest Neighbor Classification)
![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0019.jpg]]
pdf를 추정하지 말고, sample만 갖고 바로 추정해보자.

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0020.jpg]]
사용하는 distance가 다양하다.


### 유클리드 거리를 이용한 예제
![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0021.jpg]]
거리를 이용해 DB를 그려 Decision region을 만든 것이다.

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0022.jpg]]




## 적응형 DB 모델링(Adaptive Decision Boundaries Modeling)
![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0023.jpg]]
DB가 데이터에 맞게 조정하는 방법이다.
scikit learn에서 linear는 선형 DB를 파라메터(w) 조절로 추정하는데, 지금 방법은 선형을 넘어 비선형 DB도 w 조절을 통해 만들어낸다.

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0024.jpg]]

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0025.jpg]]

### feature가 1개인 경우
![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0026.jpg]]

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0027.jpg]]

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0028.jpg]]

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0029.jpg]]

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0030.jpg]]

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0031.jpg]]


### feature가 2개인 경우
![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0032.jpg]]

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0033.jpg]]




## 적응형 판별 함수(Adaptive Discriminant Functions)
입력 데이터를 $D_1 ... D_N$까지 모든 판별식에 넣어보고, 값이 제일 크게 나오는 클래스로 decision한다.
![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0034.jpg]]
클래스가 n개라면 n개의 decision function을 모두 정의한다.
그 후 샘플이 하나 들어오면, 그 샘플을 모든 decision function에 집어넣어, 가장 값이 큰 쪽으로 decision을 내리는 방법이다.

이 방법을 사용했는데 틀린다면, w들을 서로 조절한다.
조절하는 방식은 다음과 같다.
	원래 답이 i클래스인데 j클래스로 분류가 됐다면, j클래스의 w는 줄여주고, i클래스의 w는 늘려준다.
	
![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0035.jpg]]

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0036.jpg]]



### 예제 1
수업시간에 설명 안함 그냥 읽어보기
![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0037.jpg]]

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0038.jpg]]

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0039.jpg]]

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0040.jpg]]

## Minimum Squared Error Discriminant Functions
![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0041.jpg]]
한 개의 discriminant 모델을 정한다.
트레인 샘플이 v개가 있다고 가정하면, 이 v개의 샘플들을 D에 집어넣어 값을 얻어낸다.
desired value와 얻은 값의 차이를 squared 시킨다.(오차를 제곱하여 더할 때 오차가 서로 상쇄되지 않도록 한다.)

w들을 미분해서 w모두 0이 되도록하는 값을 찾아가는 것이다.
![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0042.jpg]]

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0043.jpg]]

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0044.jpg]]

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0045.jpg]]



### 예제
![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0046.jpg]]

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0047.jpg]]
미분 후 연립해서 $w_0, w_1, w_2$를 구해 Decision boundary를 구한다.

![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0048.jpg]]



###
![[[강의노트]Ch4_NonparametricDecisionMaking(수정) (1)_page-0049.jpg]]

*요약*
classification 방법은 통계적 방법을 이용했을 때, 베이지안 클래시파이어를 이용한다.
이 때의 확률 값은 
	1. 잘 알려진 RV function을 이용하는 방법과, 
	2. 샘플들 자체를 이용하는 방법(윈도우를 씌워 모델링), 
	3. 또는 이런 확률 추정을 하지 않고 nearest neighbor 샘플만 이용하는 방법도 있고, 
	4. iteration 방법을 통해서 decision boundary를 찾아가는 adaptive discriptive function 방법
	5. 전체 평균 에러를 최소화 하는 방법으로 Decision boundary를 찾는 min squared error 방식이 있다.