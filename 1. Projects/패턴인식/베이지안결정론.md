![[베이지안결정론_page-0001.jpg]]

![[베이지안결정론_page-0002.jpg]]

![[베이지안결정론_page-0003.jpg]]

![[베이지안결정론_page-0004.jpg]]

![[베이지안결정론_page-0005.jpg]]

![[베이지안결정론_page-0006.jpg]]
[[Likelihood ratio R]]을 알아보자. ^43ddc1

![[베이지안결정론_page-0007.jpg]]

![[베이지안결정론_page-0008.jpg]]

![[베이지안결정론_page-0009.jpg]]

![[베이지안결정론_page-0010.jpg]]

![[베이지안결정론_page-0011.jpg]]

![[베이지안결정론_page-0012.jpg]]

![[베이지안결정론_page-0013.jpg]]
중간고사 범위
***
## 11월 4일 강의
### 복습 북마크
여기 복습해야함
태블릿 필기 따라서 복습하기
또한, 저 시그마 행렬이 단일 변수 가우시안 분포에서 다중 변수 가우시안 분포가 될 때 생긴 것인데, 어떻게 생겨났고 작용하는지 알아내기
- [x] 1차 학습: 2024-11-09
- [ ] 1차 복습:
- [ ] 2차 복습:
![[베이지안결정론_page-0014.jpg]]
[[d 차원 벡터]]는 d차원 위의 한 점을 가리킨다.
피처가 추가가되면 대부분의 경우의 분별력이 높아지는 경향이 있다.
피처가 d개까지 늘어나면 어떻게 되는지 확인한다.(Normal density의 예시로 확인한다.)

이 예제는 $\rho_{xy}$가 0이 아니므로, x와 y는 correlation이 있다.
만약 $\rho_{xy} = 0$이라면 x와 y는 correlation이 없는 것이고, 이 때, $\sum_G = \begin{bmatrix} \mu_x^2 & 0 \\ 0 & \mu_y^2 \end{bmatrix}$이 되고 이는 Diagonal Matrix라고 불린다.

Normal density에 가장 큰 연산은 [[det (행렬식, Determinant)]]와 역행렬 구하는 연산이다. 
Diagonal Matrix는 이를 쉽게 구할 수 있으므로, 계산량에서 매우 큰 이점이 존재한다.

참고로 x, y가 독립이면 서로 uncorrelated하다.
현실적으로 x, y가 독립인 경우가 거의 존재하지 않지만, correlation이 크지 않다면 독립이라고 가정해서 연산량에서 이득을 보는 쪽을 선택하곤 한다.


![[베이지안결정론_page-0015.jpg]]
![[Pasted image 20241109152440.png]]
d개의 피처가 존재하고, 각 피처가 가질 수 있는 값이 V개 그리고 k개의 클래스가 존재한다면 경우의 수는 $kV^d$이다.
하지만 각 피처가 독립이라는 전제를 세운다면 경우의 수는 $kVd$개로 대폭 감소한다.
이렇게 연산량의 차이가 존재한다.
![[베이지안결정론_page-0016.jpg]]
독립을 가정해서 연산하는 것의 단점은, 오류에 의한 Bias가 존재할 수 있다는 점이다.
주의해야할 것은, Uncorrelated = independent가 아니라는 것이다.
![[베이지안결정론_page-0017.jpg]]
 *(a)*
 클래스 A와 B 안에서는 각각 x와 y가 서로 독립이다.
 하지만 전체로서는 x와 y는 독립이 아니다.(positively correlative)
	 $\because$전체에는 A와 B가 포함되어있기 때문이다.

*(b)*
전체로 봤을 때는 x, y는 서로 독립이지만, 클래스 안에서 봤을 때는 독립이 아니다
A안에서는 x가 커짐에 따라 y가 가질 수 있는 값의 범위가 작아진다.
반대로, B안에서는 x가 커짐에 따라 y가 가질 수 있는 값의 범위가 커진다.
하지만 전체(A, B 모두)를 봤을 때는 x, y는 서로 독립이다.

![[베이지안결정론_page-0018.jpg]]
앞의 예제에서는 피처가 ACT 점수, 내신 점수 이렇게 두 개 였다.
이 예제에서는 에세이 점수라는 피처를 추가한다.([1~10]의 값을 가진다.)


![[베이지안결정론_page-0019.jpg]]
feature가 한 개 일 때는 졸업할 수 있는 확률을 44%로, 두 개 일 때는 10%, 이제 세 개 일 때 7%라고 예측했다.


![[베이지안결정론_page-0020.jpg]]
이전에는 한 번의 검사였고 이번에는 검사를 두 번 진행헀다.
이 각 두번의 검사는 서로 독립적이다.
두 검사가 서로 독립적이라서 이렇게 확률이 크게 뛸 수 있었다.

![[베이지안결정론_page-0021.jpg]]
샘플 19, 클래스 3개, 피처 3개
피처
	x는 0 or 1
	y는 -1 or 0 or 1
	z는 3 or 4
![[베이지안결정론_page-0022.jpg]]

### Decision Regions
![[베이지안결정론_page-0023.jpg]]
**번역**:
- **결정 영역**에 특정 특성 값 $x$를 가진 샘플이 포함되어 있으면, 해당 샘플을 그 영역에 할당된 클래스로 분류한다.
    - **결정 경계**: 결정 영역 사이의 경계
        - 최적의 결정 경계는 **특성 공간**을 결정 영역 $R_1, \ldots, R_k$로 분리한다.
        - 두 클래스 $A$와 $B$ 사이의 최적 결정 경계 계산:
            - $P(A|x) - P(B|x) \Rightarrow P(A)p(x|A) - P(B)p(x|B)$
            - x인데 A에 속해있을 확률 = A의 확률 \* A에서 x가 있을 확률
**설명**:
- **결정 영역**은 특정 클래스에 해당하는 특성 값의 범위로, 샘플의 특성 값이 어느 결정 영역에 속하는지에 따라 해당 클래스로 분류한다.
- **결정 경계**는 두 결정 영역 간의 경계를 나타내며, 클래스 간의 분리 기준 역할을 한다.
- **최적의 결정 경계**는 특성 공간을 여러 결정 영역으로 나누어, 샘플이 속할 클래스를 최적화된 방식으로 결정한다.
- **두 클래스 $A$와 $B$ 사이의 결정 경계**는 조건부 확률 $P(A|x)$와 $P(B|x)$를 통해 계산하며, 이는 주어진 $x$에 대해 $A$와 $B$ 중 어느 클래스에 속할 가능성이 높은지 판단하기 위해 사용된다.

**요약**:  
결정 경계는 특성 공간에서 각 클래스에 해당하는 영역을 분리하는 경계로, 샘플을 적절한 클래스로 분류하기 위해 사용된다.


Decision region은 피처 벨류 x를 가지고있는데 x를 가지고 있는 샘플이 decision region안에 있다면 이는 그 region안에 있다고 분류된다.
그리고 decision region을 결정하는 경계선을 decision boundary라고 한다.
sample space를 decision region들로 나누는데 이 때 오류가 가장 적게 하는 decision boundary를 optimal dicision boundary이다.

각 클래스에 속할 확률이 같게 되는 지점을 dicision boundary라고 한다.


![[베이지안결정론_page-0024.jpg]]
*문제*
geiger(가이거) 예측기에서 방사성 광물을 예측한다. (광물 A 또는 광물 B)
1초 동안 몇 번의 핵 분열을 하는지를 바탕으로 A 또는 B를 예측한다.
광물이 핵분열을 일으키는 상황은 푸아송 RV로 모델링할 수 있다.

A인지 B인지 모르는 광물을 가이거 예측기로 예측해봤더니 1초에 5번 핵분열을 일으킨다.
A에 대한 prior probability는 0.4로 알고있는 상황이다.

이 광물이 A일 확률은?

*풀이*
A의 Decision region이 전체 중 0.4, B는 0.6이므로 $P(A) = 0.4, P(B) = 0.6$이다.
$P(n=5|A), P(n=5|B)$는 푸아송RV의 모델로 표현할 수 있다. 

[[Likelihood Ratio R]]을 두 푸아송 RV로 표현한 확률의 비로 구한 후 $P(A|n), P(B|n)$을 구할 수 있다.
	R을 굳이 사용하는 이유는 비율인 R을 구하는 과정에서 연산이 쉽게 해결돼서인 것 같다.

*참고*
[[#^43ddc1|Likelihood Ratio R]]
![[베이지안결정론_page-0025.jpg]]
$P(C_i|x,y) = P(C_j|x,y)$를 만족시키는 라인을 그리면 Decision Boundary이다.



![[베이지안결정론_page-0026.jpg]]
위의 예시는 $P(C_1) = P(C_2)$인 경우였고 지금 예시는 다른 경우다.
그렇다면 $P(C_1)과 P(C_2)$에 가중치가 곱해진 꼴로 생각하면 된다.

![[베이지안결정론_page-0027.jpg]]
이제 가우시안 분포를 예시로 확인해보자

![[베이지안결정론_page-0028.jpg]]
### 1. 모든 분포의 Covariance matrix가 
노말 density의 decision boundary의 모양을 봤더니 규칙적인 현상을 발견했다.
	cov 매트릭스가 어떻게 생겼냐에 따라서 $P(x|C_i)$의 모양이 달라지는데 만약 피처들이 서로 독립이라면  cov 매트릭스는 Diagonal matrix 형태를 가진다.
	![[Pasted image 20241109172054.png]]
	파란 분포 경우에는 가로방향으로 퍼져있는 것과 세로 방향으로 퍼져있는 정도가 같다.
	이는 두 피처에 대한 분산이 같다는 것이다.
	$\begin{bmatrix}\sigma_1^2 & 0 \\ 0 & \sigma_2^2\end{bmatrix} = \begin{bmatrix}\sigma_1^2 & 0 \\ 0 & \sigma_1^2\end{bmatrix} = \sigma_1\begin{bmatrix}1 & 0 \\ 0 & 1\end{bmatrix}$이다.
	
그래서 이 경우에는 decision boundary가 어떻게?
모든 클래스에 대한 cov가 동일하다면?
cov가 특수한 경우가 아닌 각각 독립적인 형태를 띈다면 어떻게 될까?
위 경우들을 다음 시간에 배운다.

## 11월 11일 강의
*복습*
디시전 바운더리를 얘끼하면서 각 클래스들의 PDF가 노멀 density 형태일 때, 이를 규정하는 파라미터는 민과 covariance니까. covariance에 따라서 두 개의 노멀 덴시티의 계형 유형이 나뉠 수 있다.
하나는 모든 클래스의 cov matrix가 identity matrix의 상수배와 동일하다면 디시전 바운더리가 어떤 특성을 보인다.
또 하나는 identity matrix의 상수배의 형태는 아니지만 모든 클래스에 대한 cov matrix가 동일한 경우(이는 상관관계가 비슷하다는 거지 피쳐 값 자체가 동일하다는 의미는 아니다.) 어떤 디시전 바운더리가 특정 양상을 띈다.
세 번쨰는 조건 없이 일반적인 경우에는(cov matrix가 서로 다른 경우) 어떤지 오늘 알아볼 것이다.
위 경우들은 모두 노멀 덴시티의 경우이다.(다루기 쉬움)
노멀 덴시티를 가지는 분포들이 상당히 많이 있기 때문에 적용이 쉽게 가능하다.
노멀 덴시티를 띄지 않더라도, 노멀 덴시티 여러개를 가지고 불특정한 분포를 모델링할 수 있다.(가우시안 믹스쳐 분포)
![[베이지안결정론_page-0029.jpg]]

![[베이지안결정론_page-0030.jpg]]

![[베이지안결정론_page-0031.jpg]]

![[베이지안결정론_page-0032.jpg]]

![[베이지안결정론_page-0033.jpg]]

![[베이지안결정론_page-0034.jpg]]

![[베이지안결정론_page-0035.jpg]]

![[베이지안결정론_page-0036.jpg]]

![[베이지안결정론_page-0037.jpg]]

![[베이지안결정론_page-0038.jpg]]

![[베이지안결정론_page-0039.jpg]]

![[베이지안결정론_page-0040.jpg]]

![[베이지안결정론_page-0041.jpg]]

![[베이지안결정론_page-0042.jpg]]

![[베이지안결정론_page-0043.jpg]]

![[베이지안결정론_page-0044.jpg]]

![[베이지안결정론_page-0045.jpg]]

![[베이지안결정론_page-0046.jpg]]

![[베이지안결정론_page-0047.jpg]]

![[베이지안결정론_page-0048.jpg]]

![[베이지안결정론_page-0049.jpg]]

![[베이지안결정론_page-0050.jpg]]

![[베이지안결정론_page-0051.jpg]]

![[베이지안결정론_page-0052.jpg]]