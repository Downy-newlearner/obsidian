## 1.1 LLM이란?
### 1.1.1 LLM 정의
- LLM과 트랜스포머가 해결하고있는 작업은 **언어 모델링 작업**이다.
	1. 자동 인코딩
	2. 자기회귀 언어 모델

![[Pasted image 20250112235952.png|400]]

- 자동 인코딩은 문장의 누락된 부분을 채우도록 모델에 요청한다.
- 자기회귀 언어 모델은 주어진 문장의 바로 다음에 가장 가능성 있는 토큰을 생성하도록 모델에 요청한다. (예시: BERT)

### 1.1.2 LLM 주요 특징
- 기존의 트랜스포머 아키텍처는 2017년에 고안된 **시퀀스-투-시퀀스**sequence-to-sequence (seq2 seq) 모델이었으며, 이는 두 가지 구성 요소를 주로 가지고 있었다.
	- ![[Pasted image 20250112235750.png|300]]
		- 인코더는 원시 텍스트를 받아들여 핵심 구성 요소로 분리하고, 해당 구성 요소를 벡터로 반환하는 업부를 담당한다. 또한 어텐션을 사용하여 텍스트의 맥락을 이해한다.
		- 디코더는 수정된 형식의 어텐션을 사용하여 다음에 올 토큰을 예측한다.

- LLM 예시
![[Pasted image 20250113000036.png]]
35


### 1.1.3 LLM 작동 원리

- 토큰화
	- 특별한 토큰이 존재한다.
		- CLS 토큰은 Classification이라는 뜻으로 모델이 주어진 입력에 대해 클래스 예측을 할 때 사용된다.
			- 이 토큰은 문장 앞에서 문장의 전체적인 의미를 이해하도록 도와주는 역할을 한다.
			- 모델은 입력 문장에 대해 CLS 토큰을 기준으로 문장의 전체적인 의미를 벡터로
