---
created: 2025-01-13
tags:
  - Terminology
aliases: 
reference:
---
![[Pasted image 20250113003045.png]]

- **트랜스포머의 인코더만 사용**하고 디코더를 무시한다.
- 한 번에 하나의 토큰을 생성하는 데 중점을 둔 더 느린 다른 LLM에 비해서, 엄청나게 많은 양의 텍스트를 매우 빠르게 처리/이해할 수 있다.

- 따라서 BERT 기반의 아키텍처는 우리가 *자유로운 텍스트를 작성할 필요가 없을 때 대량의 말뭉치를 빠르게 작업하고 분석하는 데 가장 적합*합니다. 

- BERT 자체는 텍스트를 분류하거나 문서를 요약하지 않지만, 하위 NLP 작업을 위한 사전 훈련된 모델로 자주 사용됩니다.

- BERT는 NLP 커뮤니티에서 널리 사용되고 높이 평가받는 LLM 이 되었으며, 더 발전된 고급 언어 모델의 개발을 위한 길을 만들어왔습니다.