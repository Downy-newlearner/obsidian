---
Lecture date: 
tags:
  - 한권으로끝내는실전LLM파인튜닝
Part of book:
  - 2. GPT
reference:
---
- max_iteration만큼 반복을 수행하는 for 루프를 통해 모델 학습을 진행합니다. 

- 각 반복에서 step 변수는 현재 반복의 번호를 나타냅니다. step % eval_interval \==0 조건문은 현 재 반복 번호가 eval_interval로 정확히 나눠떨어질 때, 즉 지정된 평가 간격마다 참이 됩니다. 이때compute_loss_metrics() 함수를 호출해 현재 모델의 학습 손실과 검증 손실을 계산합니다. 이 함수는 학습 데이터와 검증 데이터에 대해 모델을 평가하고 각 평균 손실값을 계산해 반환합니다. 

- losses 딕셔너리에는 train과 eval 키를 통해 접근할 수 있는 학습 손실과 검증 손실 값을; 합니다. 이후 print 함수로 현재 단계(step), 학습 손실(losses ["train"]). 그리고 검증 손실 (losses["eval"])을 출력합니다. 이를 통해 학습 과정 진행 상황을 모니터링하고 모델이 학습 데이터와 검증 데이터에 대해 얼마나 잘 작동하는지 평가할 수 있습니다. 

- batch_function 함수를 사용해 학습 데이터에서 미니배치를 추출합니다. 모델에 입력 데이터(example x)와 정답 데이터(example_y)를 전달해 예측값과 손실값을 계산합니다. 그다음 역전파를 수행하고 옵티 마이저를 사용해 모델의 파라미터를 업데이트합니다. 

- 학습이 완료된 후, 모델을 사용해 새로운 텍스트를 생성합니다. 입력으로 0으로 채워진 텐서를 사용하 고 모델의 generate 메서드를 호출해 최대 100개의 새로운 토큰을 생성합니다. 생성된 토큰은 token. decode 함수를 사용해 텍스트로 변환되어 출력됩니다. 

- 이 코드를 모두 종합해 optimizer를 적용한 전체 코드입니다. 오류가 발생하거나 중간에 문제가 생겼을 때 는 세션을 초기화하고 코드를 다시 실행하면 일괄적으로 작동하도록 구성했습니다.