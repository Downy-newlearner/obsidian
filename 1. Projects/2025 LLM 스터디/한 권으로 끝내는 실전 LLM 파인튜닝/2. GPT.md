---
Lecture date: 2025-01-13
tags:
  - 한권으로끝내는실전LLM파인튜닝
Part of book:
  - 2. GPT
reference:
---
## 2.1 Data

- 허깅페이스의 datasets 라이브러리의 load_dataset 함수를 활용해서 데이터를 data 변수에 싣는다.
	 ![[Pasted image 20250121011150.png]]

- ko_text 변수에 data 리스트의 모든 뉴스들을 하나의 요소로 합친다.
	```
	ko_text = "".join(data["train"]["document"]) # 모든 문서를 하나의 문자열로 합친다.
	```

- token_encode, token_decode 함수를 생성한다.
	```
	# 딕셔너리 컴프리헨션을 이용함

	character_to_ids = {char:i for i, char in enumerate(ko_chars)} # 딕셔너리를 생성하는 코드. 문자를 고유한 숫자 ID에 매핑한다.

	ids_to_character = {i:char for i, char in enumerate(ko_chars)}

	token_encode = lambda s:[character_to_ids[c] for c in s]
	
	token_decode = lambda l: "".join([ids_to_character[i] for i in l])
	
	print(token_encode("안녕하세요 함께 인공지능을 공부하게 되어 반가워요."))
	
	print(token_decode(token_encode("안녕하세요 함께 인공지능을 공부하게 되어 반가워요.")))
	```

- ko_text를 인코딩한 후 텐서로 변환한다.
	```
	import torch

	tokenized_data = torch.tensor(token_encode(ko_text), dtype=torch.long) # 데이터를 텐서로 변환하고 데이터 타입을 long으로 지정하는 과정이다.	  
	
	print(tokenized_data.shape, tokenized_data.dtype)
		
	print(tokenized_data[:100])
	```
	- Data type을 long으로 지정하는 이유:
		- 텍스트 데이터를 숫자로 인코딩할 때 각 단어나 토큰에 해당하는 정수 값이 큰 범위를 가질 수 있다.
		- 파이토치의 많은 함수들이 기본적으로 long 타입의 인덱스를 기대하기 때문에 추후 호환성을 보장할 수 있다.

- 데이터를 train, test로 분리한다.(순서대로 분리되는 것이 아니라 block_size의 크기만큼 청크 단위로 무작위 샘플링한다.)
	- 여기서 block_size는 한 번에 모델이 처리할 수 있는 글자의 수이다.
		- '컨텍스트 길이'라고도 부른다.

- ![[Pasted image 20250121164414.png]]
	- 만약 첫 블록으로만 학습을 진행했다면 1928이 나오면 다음은 2315로 예측하고, 0을 예측할 때는 1928, 2315를 함께 사용해 예측한다

- ![[Pasted image 20250121164609.png|400]]
	- 위 예시처럼 학습이 진행되지만 실제로는 '배치(batch)' 단위로 학습한다.


- 배치 함수
	```
	# 무작위로 배치를 설정함 -> 입력 텐서 x와 목표 텐서 y를 반환함

	def batch_function(mode):
	
	dataset = train_dataset if mode == "train" else test_dataset
	
	idx = torch.randint(len(dataset) - block_size, (batch_size,)) # randint는 random int의 줄임말이다.
	
		# len(dataset) - block_size는 데이터셋의 마지막 블록 범위에서 시작 인덱스를 선택할 수 있도록 보장.
		
		# batch_size 길이의 1차원 텐서를 생성.
	
	x = torch.stack([dataset[index:index+block_size] for index in idx])
	
	y = torch.stack([dataset[index+1:index+block_size+1] for index in idx])
	
	return x, y
	```
	- 이 함수는 모드에 따라 적절한 데이터셋에서 미니 배치를 만든다.
	- 데이터셋 선택 -> 랜덤 인덱스 생성 -> 입력 시퀀스 & 목표 시퀀스 생성


## 2.3 언어 모델 만들기
- semiGPT 클래스
	- 객체 지향 프로그래밍의 기본 원칙을 따른다.
	- 메서드
		1. \_\_init\_\_ 메서드
			- 클래스의 초기화를 담당한다
		2. forward 메서드
			- 모델의 실제 연산을 