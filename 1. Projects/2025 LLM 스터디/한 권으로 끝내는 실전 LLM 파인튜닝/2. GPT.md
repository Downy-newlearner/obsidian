---
Lecture date: 2025-01-13
tags:
  - 한권으로끝내는실전LLM파인튜닝
Part of book:
  - 2. GPT
reference:
---
## 2.1 Data

- 허깅페이스의 datasets 라이브러리의 load_dataset 함수를 활용해서 데이터를 data 변수에 싣는다.
	 ![[Pasted image 20250121011150.png]]

- ko_text 변수에 data 리스트의 모든 뉴스들을 하나의 요소로 합친다.
	```
	ko_text = "".join(data["train"]["document"]) # 모든 문서를 하나의 문자열로 합친다.
	```

- token_encode, token_decode 함수를 생성한다.
	```
	# 딕셔너리 컴프리헨션을 이용함

	character_to_ids = {char:i for i, char in enumerate(ko_chars)} # 딕셔너리를 생성하는 코드. 문자를 고유한 숫자 ID에 매핑한다.

	ids_to_character = {i:char for i, char in enumerate(ko_chars)}

	token_encode = lambda s:[character_to_ids[c] for c in s]
	
	token_decode = lambda l: "".join([ids_to_character[i] for i in l])
	
	print(token_encode("안녕하세요 함께 인공지능을 공부하게 되어 반가워요."))
	
	print(token_decode(token_encode("안녕하세요 함께 인공지능을 공부하게 되어 반가워요.")))
	```

- ko_text를 인코딩한 후 텐서로 변환한다.
	```
	import torch

	tokenized_data = torch.tensor(token_encode(ko_text), dtype=torch.long) # 데이터를 텐서로 변환하고 데이터 타입을 long으로 지정하는 과정이다.	  
	
	print(tokenized_data.shape, tokenized_data.dtype)
		
	print(tokenized_data[:100])
	```
	- Data type을 long으로 지정하는 이유:
		- 텍스트 데이터를 숫자로 인코딩할 때 각 단어나 토큰에 해당하는 정수 값이 큰 범위를 가질 수 있다.
		- 파이토치의 많은 함수들이 기본적으로 long 타입의 인덱스를 기대하기 때문에 추후 호환성을 보장할 수 있다.

- 데이터를 train, test로 분리한다.(순서대로 분리되는 것이 아니라 block_size의 크기만큼 청크 단위로 무작위 샘플링한다.)
	- 여기서 block_size는 한 번에 모델이 처리할 수 있는 글자의 수이다.
		- '컨텍스트 길이'라고도 부른다.

- ![[Pasted image 20250121164414.png]]
	- 만약 첫 블록으로만 학습을 진행했다면 1928이 나오면 다음은 2315로 예측하고, 0을 예측할 때는 1928, 2315를 함께 사용해 예측한다

- 