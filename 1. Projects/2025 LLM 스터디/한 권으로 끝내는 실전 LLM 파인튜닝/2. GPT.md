---
Lecture date: 2025-01-13
tags:
  - 한권으로끝내는실전LLM파인튜닝
Part of book:
  - 2. GPT
reference:
---
## 2.1 Data

- 허깅페이스의 datasets 라이브러리의 load_dataset 함수를 활용해서 데이터를 data 변수에 싣는다.
	 ![[Pasted image 20250121011150.png]]

- ko_text 변수에 data 리스트의 모든 뉴스들을 하나의 요소로 합친다.
	```
	ko_text = "".join(data["train"]["document"]) # 모든 문서를 하나의 문자열로 합친다.
	```

- token_encode, token_decode 함수를 생성한다.
	```
	# 딕셔너리 컴프리헨션을 이용함

	character_to_ids = {char:i for i, char in enumerate(ko_chars)} # 딕셔너리를 생성하는 코드. 문자를 고유한 숫자 ID에 매핑한다.

	ids_to_character = {i:char for i, char in enumerate(ko_chars)}

	token_encode = lambda s:[character_to_ids[c] for c in s]
	
	token_decode = lambda l: "".join([ids_to_character[i] for i in l])
	
	print(token_encode("안녕하세요 함께 인공지능을 공부하게 되어 반가워요."))
	
	print(token_decode(token_encode("안녕하세요 함께 인공지능을 공부하게 되어 반가워요.")))
	```

- ko_text를 인코딩한 후 텐서로 변환한다.
	```
	import torch

	tokenized_data = torch.tensor(token_encode(ko_text), dtype=torch.long) # 데이터를 텐서로 변환하고 데이터 타입을 long으로 지정하는 과정이다.	  
	
	print(tokenized_data.shape, tokenized_data.dtype)
		
	print(tokenized_data[:100])
	```
	- Data type을 long으로 지정하는 이유:
		- 텍스트 데이터를 숫자로 인코딩할 때 각 단어나 토큰에 해당하는 정수 값이 큰 범위를 가질 수 있다.
		- 파이토치의 많은 함수들이 기본적으로 long 타입의 인덱스를 기대하기 때문에 추후 호환성을 보장할 수 있다.

- 데이터를 train, test로 분리한다.(순서대로 분리되는 것이 아니라 block_size의 크기만큼 청크 단위로 무작위 샘플링한다.)
	- 여기서 block_size는 한 번에 모델이 처리할 수 있는 글자의 수이다.
		- '컨텍스트 길이'라고도 부른다.

- ![[Pasted image 20250121164414.png]]
	- 만약 첫 블록으로만 학습을 진행했다면 1928이 나오면 다음은 2315로 예측하고, 0을 예측할 때는 1928, 2315를 함께 사용해 예측한다

- ![[Pasted image 20250121164609.png|400]]
	- 위 예시처럼 학습이 진행되지만 실제로는 '배치(batch)' 단위로 학습한다.


- 배치 함수
	```
	# 무작위로 배치를 설정함 -> 입력 텐서 x와 목표 텐서 y를 반환함

	def batch_function(mode):
	
	dataset = train_dataset if mode == "train" else test_dataset
	
	idx = torch.randint(len(dataset) - block_size, (batch_size,)) # randint는 random int의 줄임말이다.
	
		# len(dataset) - block_size는 데이터셋의 마지막 블록 범위에서 시작 인덱스를 선택할 수 있도록 보장.
		
		# batch_size 길이의 1차원 텐서를 생성.
	
	x = torch.stack([dataset[index:index+block_size] for index in idx])
	
	y = torch.stack([dataset[index+1:index+block_size+1] for index in idx])
	
	return x, y
	```
	- 이 함수는 모드에 따라 적절한 데이터셋에서 미니 배치를 만든다.
	- 데이터셋 선택 -> 랜덤 인덱스 생성 -> 입력 시퀀스 & 목표 시퀀스 생성


## 2.3 언어 모델 만들기
- semiGPT 클래스
	- 객체 지향 프로그래밍의 기본 원칙을 따른다.
	- 메서드
		1. \_\_init\_\_ 메서드
			- 클래스의 초기화를 담당한다
			- 모델의 초기 파라미터 설정
		2. forward 메서드
			- 모델의 실제 연산을 수행한다.
			- 입력 데이터를 받아 모델을 통과시켜 출력을 생성한다.

- ![[Pasted image 20250121170648.png]]
	- nn.Embedding은 단어를 벡터로 변환하는 테이블을 만든다.
		- 첫 파라미터 vocab_length는 총 단어의 수를 의미한다.
		- 두번째 파라미터 vocab_length는 각 단어를 표현할 벡터의 크기를 나타낸다.
	- 임베딩 과정이 중요한 이유
		1. 텍스트 -> 숫자 변환으로 컴퓨터가 이해할 수 있도록 한다.(컴퓨터는 숫자만 이해 가능)
		2. 벡터 표현은 단어 간의 의미적 관계를 수학적으로 표현할 수 있게 해서 컴퓨터가 단어간의 유사성을 계산하고 이해할 수 있다.
		3. 수학적 연산이 가능해져서 복잡한 언어 모델링과 자연어 처리 작업에 유용하다.
		4. 텍스트 -> 고차원 벡터 변환은 단어의 다양한 특성을 표현할 수 있다.
	- 최종 출력은 [[logit]]으로 이는 각 단어가 다음에 올 확률을 나타내는 점수가 된다.


- [[nn.Embedding]]은 임베딩 층을 만드는 함수이다.
	- 임베딩 층은 룩업 테이블이며 역전파 과정에서 학습된다.


[[torch.nn]]
[[torch.nn.functional]]

- Loss를 사용할 때 2가지 중요한 가정
	1. 전체 손실은 개별 샘플 손실의 합과 같다.
	2. 각 샘플의 손실을 계산할 때 신경망의 최종 출력값과 입력값만 사용한다.

- Shape이 맞지 않는 오류 해결법
	- ![[Pasted image 20250121174053.png|400]]
	- ![[![[Pasted image 20250121174634.png]]Pasted image 20250121174634.png]]
	- \[4, 2701]은 \[batch_size, vocab_length] 였는데 이것을 \[32, 2701], 즉 \[batch_size * block_size, vocab_length]로 바꾼다.
	- \[4, 8]은 \[batch_size, sequence_length]였는데 이것을 \[32], 즉 \[batch_size * block_size]로 바꾼다.
	- 참고로 \[32, 2701]이라는 것은 32 by 2701의 2차원 행렬이다.

- 위와 같이 shape를 변경하면 각 토큰에 대한 예측과 실제 값을 일대일로 비교할 수 있게 된다.

