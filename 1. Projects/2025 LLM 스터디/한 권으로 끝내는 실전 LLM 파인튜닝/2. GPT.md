---
Lecture date: 2025-01-13
tags:
  - 한권으로끝내는실전LLM파인튜닝
Part of book:
  - 2. GPT
reference:
---
## 2.1 Data

- 허깅페이스의 datasets 라이브러리의 load_dataset 함수를 활용해서 데이터를 data 변수에 싣는다.
	 ![[Pasted image 20250121011150.png]]

- ko_text 변수에 data 리스트의 모든 뉴스들을 하나의 요소로 합친다.
	```
	ko_text = "".join(data["train"]["document"]) # 모든 문서를 하나의 문자열로 합친다.
	```

- token_encode, token_decode 함수를 생성한다.
	```
	# 딕셔너리 컴프리헨션을 이용함

character_to_ids = {char:i for i, char in enumerate(ko_chars)} # 딕셔너리를 생성하는 코드. 문자를 고유한 숫자 ID에 매핑한다.

ids_to_character = {i:char for i, char in enumerate(ko_chars)}

token_encode = lambda s:[character_to_ids[c] for c in s]

token_decode = lambda l: "".join([ids_to_character[i] for i in l])

print(token_encode("안녕하세요 함께 인공지능을 공부하게 되어 반가워요."))

print(token_decode(token_encode("안녕하세요 함께 인공지능을 공부하게 되어 반가워요.")))
```