---
Lecture date: 2025-01-16
tags:
  - 한권으로끝내는실전LLM파인튜닝
Part of book:
  - 3. 전체 파인튜닝
reference:
---
## 3.1 전체 파인튜닝 데이터 준비

### 3.1.1 전체 파인튜닝의 원리와 종류
#### - 파인튜닝이란
- LLM을 특정 작업에 맞게 추가로 학습하는 것
- LLM은 많은 데이터로 이미 학습이 되어있지만 세상의 모든 정보를 알고 있는 것은 아니라, *환각*을 일으키곤 한다.
	- 특히 전문적이거나 특수한 분야에서 더욱 많이 발생함.
- 그래서 원하는 분야의 데이터를 추가로 학습시키는데 이것을 파인튜닝이라고 한다.

#### 처음부터 원하는 데이터로 학습시키지 않고 파인튜닝을 하는 이유
	1. 과적합
		- 데이터가 적어서 그렇다.
	2. 자연스러운 언어 생성 능력 부족
		- 이것 또한 데이터가 적어서 그렇다.

#### 파인튜닝 종류
- Full Fine-Tuning
	- 모델이 이전에 학습한 내용을 잊을 수 있으므로 *천천히 진행해야한다*. 
- PEFT(Parameter-Efficient Fine-Tuning)
	1. 어댑터 튜닝
	2. 프롬프트 튜닝
	3. LoRA

#### 파인튜닝할 때 주의점
1. 과적합
2. 재앙적 망각 현상
	- 모델을 학습한다 = 가중치를 변화시킨다.
	- 이전에 *학습한 정보를 표현하던 가중치 패턴*이 크게 변경되어 이전 지식 또는 능력이 손실 될 수 있다.
3. 모델 사이즈에 따른 부담
4. 데이터의 품질과 양
5. 데이터에 편향(인종차별, 성차별 등)이 없는지 확인해야함


### 3.1.2 다양한 태스크와 데이터셋
- LLM과 생성형 AI의 성능을 높이기 위해서는 다음 요소들이 중요하다.
	1. 다양한 태스크
	2. 적절한 데이터셋
	3. 효과적인 전처리 전략

#### 대화형 태스크
- 인공지능이 사람과 자연스럽게 대화를 나눌 수 있도록 하는 작업이다.
- 데이터 구축 방법 두 가지
	1. 앞서 수집한 다양한 데이터를 대화 형식으로 변환
	2. 실제 사람들 사이의 대화를 직접 수집

- 이 과정에서 **'[[페르소나]]'** 개념의 도입이 중요하다.

#### 질의응답(질문 답변 태스크)
- 주어진 질문에 대해 정확한 답변을 제공하는 작업이다.
- 질문과 답변 쌍으로 구성된다.
- 답변 스타일을 자연스럽고 친근한 대화형으로 하는 경향때문에 대화형 태스크와 통합되는 경향이 있다.
	- 사과는 무슨 색이야? 
		- -> 빨간색 (X)
		- -> 사과는 보통 빨간색입니다. (O)

#### 요약 태스크
- 원본 텍스트와 그에 대한 요약문이 쌍으로 이뤄진 데이터셋을 사용한다.
- 중요한 문장을 뽑아내는 '추출적 요약'과 새로운 문장으로 내용을 정리하는 '추상적 요약' 두가지 방식을 모두 포함하는 것이 좋다.

#### 기계 번역

#### 바꿔 쓰기

#### 코드 생성 및 이해

#### 검색 증강 생성

### 3.1.3 데이터 전처리
- 데이터 클렌징으로 전처리를 시작한다
	- 중복 데이터 없애기
	- 빠진 정보 채우기
	- 이상한 데이터 처리하기

- 데이터 클렌징 이후에 본격적인 전처리 단계로 넘어 간다

- 전처리의 첫 단계는 **토큰화**이다

	- 이 단계에서는 텍스트를 의미있는 작은 단위로 나눈다.
	- 예전에는 단어의 기본 형태를 찾아내는 어간 추출과 표제어 추출 과정이 중요했다
	- 하지만 최근 인공지능 기술이 발전 하면서 어간 추출이나 불용어 제거 같은 과정을 거치지 않아도 모델이 문맥을 이해하고 적절히 처리 할 수 있게 됐다
	
	- 더 나아가 불용어 라고 얘기했던 단어에도 실제로 중한 의미가 담겨 있다는 것이 밝혀졌다

- **허깅페이스 토크나이저**가 크게 주목 받고 있다
	- 주요 기능으로는 
		- 텍스트를 더 작은 단위로 나누는 것 
		- 문장이 시작과 끝을 알리는 특별한 표시를 추가 하는 것 
		- 길이가 서로 다른 문장을 같은 길이로 맞추는 것 
		- 단어를 숫자로 바꾸는 것 
		- 잘못된 띄어쓰기를 자동으로 고치는 것 
		- 긴 단어나 생소한 단어를 더 잘은 의미 단어 등으로 나누는 등이 있다