## 동적 프로그래밍을 사용해야하는 이유
- 강화학습의 최종 목표인 '최적 정책 찾기'를 위해서는 기존 방법인 벨만 최적 방정식을 만족하는 연립방정식을 풀어야했다.
- 하지만 이 방법은 계산량이 너무 많다.(상태의 크기를 S, 행동의 크기를 A라고 한다면 $A^S$만큼의 계산이 필요함.)
- ==벨만 방정식을 이용하여 식정리를 한 다음 동적프로그램 적용의 실마리를 찾는 흐름 다시 잡기==
- '4.2절'까지는 정책을 평가했다. -> '반복적 정책 평가'라는 알고리즘이었다.
- 평가를 할 수 있게 되었으니 정책을 살짝 수정하여 더 나아지는지 '비교'하며 '개선'할 수 있다. -> '4.3절'에서 정책을 개선하는 방법을 알아본다.

## 질문
- '상태 (s)에서의 최적 정책'과 '상태 (s')에서의 최적 정책'이 다를 수 있어? 한 environment에는 한 개의 최적 정책만 존재하는게 아니야?
	- 최적 정책은 특정 환경에서 누적 보상을 극대화하는 행동 선택 방법을 말한다.
	- 그러나 상태 ( s )와 상태 ( s' )에서의 최적 정책이 서로 다를 수 있다는 것은 오해의 소지가 있습니다.
	- **환경 내에서 전체적으로 정의된 최적 정책**은 환경 내의 모든 상태에 대해 최적의 행동을 정의한다.
	- 각 상태에서의 최적 행동은 정책의 일부분이므로, 특정 상태(s와 s')에서 다른 행동을 선택하는 것이 최적일 수 있다.

- 그렇다면 '상태 s에서의 최적 정책'이라는 표현은 환경 내에서 전체적으로 정의된 최적 정책이 정의한 s에서의 transmission probability를 얘기하는 게 맞아?
	- 네, 맞아요. "상태 s에서의 최적 정책"이라는 표현은 그 상태에서 선택할 행동에 대한 최적의 확률 분포를 의미합니다. 이 확률 분포는 주어진 환경에서 장기적으로 최대의 보상을 받을 수 있도록 정의된 전체 최적 정책에 의해 결정됩니다. 최적 정책은 각 상태에서 최선의 행동을 결정함으로써 환경에 대한 행동 선택 과정을 안내합니다.

- '가치함수를 평가한다'라는게 무엇인가?


## 4.1.2 반복적 정책 평가_첫 번째 구현
**전제**
- 상태 전이는 deterministic하다.
	- 다음 상태 $s'$가 함수 $f(s,a)$에 의해 고유하게 결정된다.

![[Pasted image 20250323213333.png|400]]


## GridWorld
![[Pasted image 20250323215046.png|400]]

## 4.3 정책 반복법
![[Pasted image 20250324002851.png|400]]
- 식 4.5를 통해 최적 가치 함수 $v_k$를 알면 최적 정책 $\mu_k$를 구할 수 있다. 하지만 $v_k$를 알기 위해 $\mu_k$가 필요하다. 마치 닭과 달걀 문제같다.

### 탐욕화
![[Pasted image 20250324004209.png|400]]
![[Pasted image 20250324004216.png|400]]
- 식4.6 혹은 식 4.7에 의한 정책 갱신을 '탐욕화'라고 부르자.

**탐욕화된 정책 $\mu'(s)$의 특징**
1. $\mu(s) = \mu'(s)$라면 $\mu(s)$는 최적 정책이다.
2. 1번의 경우와 달리 정책이 갱신되는 경우, 항상 더 나아진다(개선된다).
	- 정책 개선 정리

## 4.3.2 평가와 개선 반복
- 최적 정책을 찾는 방법의 핵심 두 가지
	- 4.2절에서 구현한 상태 가치 함수를 평가하는 알고리즘
	- 탐욕화

![[Pasted image 20250324005144.png]]
- 이렇게 평가와 개선을 반복하는 알고리즘을 정책 반복법(policy iteration)이라고 한다.



## 4.4 정책 반복법 구현


## 4.5 가치 반복법
### 이전 챕터에서 학습한 정책 반복법의 결론
![[Pasted image 20250324011747.png|400]]
![[Pasted image 20250324011805.png]]

