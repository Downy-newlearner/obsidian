## 동적 프로그래밍을 사용해야하는 이유
- 강화학습의 최종 목표인 '최적 정책 찾기'를 위해서는 기존 방법인 벨만 최적 방정식을 만족하는 연립방정식을 풀어야했다.
- 하지만 이 방법은 계산량이 너무 많다.(상태의 크기를 S, 행동의 크기를 A라고 한다면 $A^S$만큼의 계산이 필요함.)
- ==벨만 방정식을 이용하여 식정리를 한 다음 동적프로그램 적용의 실마리를 찾는 흐름 다시 잡기==
- '4.2절'까지는 정책을 평가했다. -> '반복적 정책 평가'라는 알고리즘이었다.
- 평가를 할 수 있게 되었으니 정책을 살짝 수정하여 더 나아지는지 '비교'하며 '개선'할 수 있다. -> '4.3절'에서 정책을 개선하는 방법을 알아본다.


## 4.1.2 반복적 정책 평가_첫 번째 구현
**전제**
- 상태 전이는 deterministic하다.
	- 다음 상태 $s'$가 함수 $f(s,a)$에 의해 고유하게 결정된다.

![[Pasted image 20250323213333.png]]


## GridWorld
![[Pasted image 20250323215046.png]]

## 4.3 정책 반복법
