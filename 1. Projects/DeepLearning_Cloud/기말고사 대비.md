## Chap01
### 1. Deep learning
- 딥러닝은 인공 신경망에 기반을 둔다.
	- Perceptron -> Multi layer perceptron -> DNN(Deep neural network)
	- DNN은 MLP뿐만 아니라 CNN, RNN 등의 개념을 포괄하는 개념이다.
### 2. Cloud
- 데이터를 인터넷과 연결된 중앙컴퓨터에 저장해서 인터넷에 접속하기만 하면 언제 어디서든 데이터를 이용할 수 있는 환경
	- 데이터 뿐만 아니라 SW, 컴퓨팅 자원도 클라우드를 통해 이용가능하다.
- IaaS, Paas, Saas
![[Pasted image 20241216232221.png|400]]
	- 순서대로 Infrastructure, Platform, SW as a Service
	- IaaS: AWS
	- Paas: 앱 개발 및 배포 관련 지원. Google App Engine, Oracle Cloud Platform
	- SaaS: 서비스 제공처에서 인프라와 SW 모두 제공. 웹 메일, 구글 드라이브


## Chap03
### 1. AI scopes

### 2. Machine learning
- 머신러닝은 과거의 경험을 *미래의 결정에 활용(예측)* 하는 소프트웨어를 디자인하고 연구하는 분야이다.
	- 사용 예시: 주가 예측, 질병 진단, 이미지 분류, 번역 등
- 반응변수(y)와 설명변수(X) 간의 관계를 찾는 것이 *훈련*이다.

### 3. Machine learning areas
- 데이터 형태에 따른 분류: 지도학습, 비지도학습, 강화학습
- 타겟 형태에 따른 분류: Classification, Regression, Clustring

### 4. Development process of learning model
![[Pasted image 20241216233928.png|400]]

## Chap04_Regression
### 1. Simple linear regression
![[Pasted image 20241216234355.png|200]]
- 종속변수(y)와 독립변수(X) 사이의 선형 관계를 파악하고 이를 예측에 활용하는 방법
- 두 변수가 선형 관계에 있는지 알아보는 방법: 산점도, 상관계수
- 모델 평가
	- MSE
		![[Pasted image 20241216234632.png|200]]
		값이 작을 수록 정확한 모델이다.
	- [[R squre score]]($R^2$ Score)


### 2. Multiple linear regression
![[Pasted image 20241216235147.png|300]]
- 독립변수가 2개 이상인 경우이다.

### 3. Logistic regression
- 일반적인 회귀 문제에서 종속변수는 수치형 데이터지만, 로지스틱 회귀에서 종속변수는 범주형 데이터이다.
- 독립변수는 수치형이어야하므로 범주형 독립변수의 경우 인코딩하여 수치형 데이터로 변환한 후 훈련시켜야한다.(Scikit-learn에서는 자동으로 해줌)

## Chap05_DT, RF, SVM
### 1. Concept of decision tree
- 큰 문제를 작은 문제들의 조각으로 나누어 해결하는 기법이다.
- 이슈
	1. 트리의 노드를 선택할 때, 데이터셋에서 어떤 속성부터 선택할 것인가
		- Feature evaluation이 필요하다.(불순도([[impurity]])를 계산해야함)
		- ![[Pasted image 20241217001737.png|200]]
			- 몸무게의 불순도가 가장 낮다. 그러므로 몸무게를 기준으로 먼저 나누어야한다.
	2. 트리 split을 언제 중단할 것인가?
- 장점
	- 모든 문제에 적합하다.
	- 여러 속성들 중 중요한 속성들만을 사용하여 예측한다.
	- 단순한 이론적 근거에 비해 높은 효율성
- 단점
	- 모델이 쉽게 과대적합 및 과소적합됨
	- 훈련 데이터에 대한 약간의 변경이 결정 논리에 큰 변화를 준다.

### 3. Support Vector Machine(SVM)
- 기본적으로 선형 분류 방식이다.
- 기존의 분류 방법들은 '분류 오류율을 최소화'하려는 목적으로 설계되었지만, SVM은 *두 부류 사이에 존재하는 '여백을 최대화'* 하려는 목적으로 설계되었다.
- 장점
	- 범주형, 수치형 데이터 모두에 사용 가능하다.
	- 노이즈 데이터에 큰 영향을 받지 않고, overfitting이 잘 일어나지 않는다.
	- 높은 정확도를 보인다.
- 단점
	- 훈련 시간이 많이 소요된다.
	- 모델 해석이 어렵다.

### 4. Ensemble
- '다수 협의에 의한 결정'이라는 원리를 예측 문제 해결에 적용한 것이다.
- 배깅
	- ![[Pasted image 20241217003147.png|300]]
	- Bootstrap Aggregation의 약자이다.
	- RF가 배깅 방식을 사용한다.
- 부스팅
	- ![[Pasted image 20241217003250.png|350]]
	- 예측모델 1의 결과를 보고 오답에 가중치 부여해서 다음 예측모델에서 그 오답 부분을 집중적으로 학습하여 오답을 낮춘다. 이 과정을 반복
	- 배깅에 비해 성능이 더 좋으나 과적합이 더 쉽게 발생한다.
### 5. Random Forest

### 6. XGBoost
- Gradient Boosting framework 기반으로 구현됐다.
- 병렬 트리 부스팅을 제공한다.
	- 많은 데이터 사이언스 문제들을 빠르고 정확하게 푸는 방법으로 사용되고 있는 기법이다.

## Chap06_Cross Validation
### 1. Bias-Variance trade off
![[Pasted image 20241217005034.png]]
- Bias
	- 데이터 내의 *모든 정보를 고려하지 않음으로 인해*, 지속적으로 잘못된 것들을 학습하는 경향
	- 과소적합 유발
- Variance
	- 데이터의 *너무 세세한 부분까지 학습하여* 새로운 데이터가 추가되면 모델이 쉽게 바뀜
	- 과적합 유발
		- 이를 방지하기 위한 기능
			- tree 기반 알고리즘: 가지치기
			- regression, SVM: 정규화
			- neural network: dropout

### 2. K-fold Cross Validation
- 데이터셋은 랜덤 샘플링하여 train&validation&test data로 나뉜다. 
- 그런데 이렇게 '랜덤' 샘플링된 데이터셋으로 훈련 및 평가 사이클을 한 번만 실행하는 것이 충분한가?
	- 샘플링된 데이터셋의 구성에 따라 성능이 다르게 나올 수도 있다.

![[Pasted image 20241217005410.png|500]]
- 모델의 정확도는 각 fold의 정확도 평균으로 계산한다.
- sklearn의 cross_val_score 메서드를 사용하면 자동으로 폴드를 나누고 cv번의 훈련을 진행한 후 각 폴드의 성능을 검사하여 리턴한다.
- CV는 하이퍼 파라미터 튜닝에 사용한다.
- 또한 전처리시 feature selection에 사용한다.

### 3. Hyper parameter tuning
- 대부분의 분류 알고리즘은 모델 성능에 영향을 끼치는 하이퍼파라미터를 가지고있다.
- 모델 빌딩 과정
	1. 피처 셀렉션 등 전처리 된 데이터셋 준비
	2. cross validation을 이용하여 하이퍼파라미터 최적화
	3. 최적의 하이퍼파라미터로 튜닝된 알고리즘(모델)에 데이터를 학습시킨다.
	4. 테스트 데이터로 모델을 평가한다.
- Grid Search, Random Search

### 4. Model comparison
- 모든 데이터셋에 항상 최고인 분류기는 없다.
- 그러므로 '모델 비교'가 필요하다.

### 5. Performance metric
- 이진 분류 메트릭
![[Pasted image 20241217010518.png]]
- 정확도 
![[Pasted image 20241217011203.png|300]]

- 민감도(sensitivity, recall)
![[Pasted image 20241217011213.png|300]]
	-> 정답이 P인 것들 중 맞춘 비율(환자를 환자라고 예측한 비율)

- 특이도(specificity)
![[Pasted image 20241217011221.png|300]]
	-> 정답이 N인 것들 중 맞춘 비율(정상인을 정상인이라고 예측한 비율)

- 정밀도(Precision)
 ![[Pasted image 20241217011235.png|300]]
	 -> P라고 예측한 것들 중 맞춘 비율(환자라고 예측한 것 중에서 실제 환자의 비율)

- F1 score
- ![[Pasted image 20241217011607.png]]
	- 정밀도와 민감도의 조화평균
	- precision과 recall의 불균형에 대해 감점이 이루어진다.

- ROC-AUC
	- Accuracy는 불균형 데이터셋에 대해 올바로 평가하지 못한다.
		- 99%가 Positive인 데이터셋에 대해 전부 다 Positive라고 하면 장땡임
	- AUC는 이런 불균형 데이터셋 평가에 효과적이다.


## Chap07_Feature Selection_Stacking
### 1. Feature selection
![[Pasted image 20241217021543.png|400]]
- 성능에 더 많이 기여하는 중요한 피처를 찾는 것이 중요하다.
	- 명확한 클래스 바운더리를 갖는 피처가 좋은 피처다.
- 필터 방식
	- 데이터의 통계적 특성만을 이용하여 각 피처를 평가하고 최고의 피처 n개를 찾는다.
	- 피처간의 상호관계를 고려하지 않는다는 단점이 있다.
	- sklearn의 SelectKBest 메서드를 사용할 수 있다.
- 래퍼 방식
	- 모델을 학습하면서 피처 조합의 성능을 평가한다.
	- 모델 학습을 여러번 반복한다.
	- Forward search, Backward elimination, Sequential Forward Search(SFS) 방식이 있다.
![[Pasted image 20241217022411.png|400]]
- 피처 셀렉션, 하이퍼파라미터 튜닝, 예측기 선택 모두 cross validation과 함께 진행해야한다.

### 3. Bagging meta-estimator
![[Pasted image 20241217023215.png|400]]
- 동일한 데이터셋에 대해 여러번 샘플링을 하여 다수의 데이터셋을 구성한 뒤 단일 알고리즘으로 모델을 생성한다.
- 각 모델의 결과를 종합하여 최종 의사결정을 한다.
- sklearn의 BaggingClassifier 분류기를 사용한다.

### 4. Model stacking
![[Pasted image 20241217022927.png|400]]
- 앙상블 방법 중의 하나로, 여러 모델의 예측값을 input으로 해서 새로운 모델을 학습한다.
- 과적합 방지를 위해 사용된다.
- Level 0의 모델들은 다양한 예측값을 내놓게 하도록 각기 다른 알고리즘을 사용한다.
- sklearn의 StackingClassifier 분류기를 사용한다.

![[Pasted image 20241217023309.png|400]]


## Chap08_인공신경망개요
### 1. History
- Warren McCulloch & Walter Pitts가 1943년에 인공 신경망을 처음 제안함.
### 3. Perceptron
![[Pasted image 20241217025840.png|300]]
- 가장 단순한 형태의 신경망으로 Frank Rosenblatt가 1957년에 제안했다.
- 편향 값에 따라 y값이 0 또는 1에 치우친 값이 나오므로 편향이라고 한다.
	- 편향은 뉴런이 얼마나 쉽게 활성화 되는지를 조절한다.
- Weight는 각각의 입력신호가 얼마나 쉽게 활성화 되는지를 조절한다.

## Chap09_신경망학습
### 1. Learning in Neural Network
- 활성화 함수$\phi()$
	- 여러 함수가 사용될 수 있으며 시그모이드 함수가 대표적이다.
	- 가중합 v의 값을 0과 1 사이의 값으로 변환한다.
	1. Sigmoid function
	- ![[Pasted image 20241217030629.png|200]]
	2. Softmax function
		- 시그모이드 함수는 자신의 노드로 들어오는 가중합만 고려하여 출력값을 조절하지만, 소프트맥스 함수는 출력 노드가 여러개일 때, *자신의 노드 뿐만 아니라 다른 노드로 들어오는 신호의 가중합도 고려*한다.
- 델타 룰
	- ![[Pasted image 20241217031425.png|300]]
	- 신경망의 출력값과 정답사이의 오차를 통해 w를 조정하는 방법 중 하나이다.
	- 어떤 입력노드가 출력노드의 오차에 기여했다면, 두 노드의 연결 가중치는 해당 입력 노드의 입력값($x_j$)과 출력 노드의 오차($e_j)$에 비례하여 조절한다.

### 2. Multi-output perceptron
- 태블릿 예제 다시 확인하기

### 3. Cost function
- loss function과 같은 용어로 에러를 계산하는 방법이다. 
- 앞선 예제에서는 loss function을 $d-y$로 단순하게 뒀지만 보통 MSE, Cross entropy 등으로 사용한다.
- MSE
	- ![[Pasted image 20241217034418.png|300]]
- Cross entropy
	- ![[Pasted image 20241217034452.png|400]]
	- d가 1일 때, 첫 번째 텀이 살아남고 이 때 y가 1에 가까울 수록(정답에 가까울 수록) 로스가 낮다.
	- 반면 d가 0일 때는 두 번째 텀이 살아남고 y가 0에 가까울 수록 로스가 낮다.

### 4. Update weight matrix
- 모든 w를 갱신하는 것은 매우 오래걸리므로 효율적인 w 갱신 방법이 필요하다.
1. Stochastic 경사하강법
	- ![[Pasted image 20241217035527.png|400]]
	- 하나의 학습 데이터마다 오차를 계산하여 신경망의 가중치를 업데이트한다.
2. 배치
	- ![[Pasted image 20241217035604.png|400]]
	- 모든 학습 데이터에 대한 가중치 갱신값을 계산한 다음 이들의 평균값으로 가중치를 한번에 업데이트한다
	- 학습데이터가 많으면 계산 시간도 많이 걸린다.
3. 미니 배치
	- ![[Pasted image 20241217035646.png|400]]
	- SGD와 배치 방식의 중간이다.
	- 전체 학습 데이터에서 일부 데이터만 골라 배치 방식으로 학습한다.

![[Pasted image 20241217035905.png|400]]

## Chap10_terms of dnn
### 2. ReLU
- ![[Pasted image 20241217043203.png|200]]
	- 시그모이드보다 계산 속도가 빠르고 더 빠르게 수렴
	- 하지만 음수들을 모두 0으로 처리하기 때문에 한 번 음수가 나오면 더이상 그 노드들은 학습되지 않는다.
		- 이를 보완한 것이 Leaky ReLU이다.
![[Pasted image 20241217043229.png|200]]


### 3. Momentum
![[Pasted image 20241217044037.png|300]]
- 훈련 속도를 높이고 진동의 리스크를 줄이기 위해 사용한다.
- ![[Pasted image 20241217044139.png|200]]
- $\bar{m}$은 이전 모멘텀이다.
- 현재 모멘텀은 과거 모멘텀 값이 계속 추가됨으로써 가중치 갱신값이 커지고 이에 따라 학습속도가 향상된다.

### 5. Initialize W
- 초기 가중치는 인공 신경망 학습 퍼포먼스에 큰 영향을 끼치므로 랜덤 초깃값 세팅은 좋은 아이디어가 아니다.
- ReLU 활성화 함수를 사용하는 경우
	- He initialization
- sigmoid, tanh 활성화 함수를 사용하는 경우
	- Xavier initialization


## Chap12_keras_CNN
### 2. CNN concepts
- Stride
	- ![[Pasted image 20241217050703.png|300]]
	- 이미지에 필터를 적용할 때 몇 칸씩 옮겨가며 적용할 것인가?
- Padding
	- ![[Pasted image 20241217050748.png|300]]
	- Convolution 연산을 적용하면 그 특성상 원본 이미지의 크기가 줄어든다
	- 원본 이미지의 크기를 유지하기 위한 장치가 padding이다.
- Pooling
	- ![[Pasted image 20241217050856.png|300]]
	- 출력 데이터의 크기를 줄이거나 특정 데이터를 강조하는 용도로 사용한다.
	- Max, Average, Min pooling이 있다.

### 3. CNN builidng
- Convolution 레이어 출력 데이터 크기 계산
	- ![[Pasted image 20241217051349.png|400]]
- Pooling layer 크기 계산
	- ![[Pasted image 20241217051422.png|400]]

## Chap13_keras_CNN_augmentation
### 1. Image augmentation
![[Pasted image 20241217052154.png|500]]

## Chap14_keras_transfer_learning
### 1. Summary
- 전이학습
	- 잘 구축된 모델을 다른 작업에 활용하는 것을 말한다.
	- 아키텍처만 활용하는 방법과 Weight 값까지 활용하는 방법이 있다.

## Chap15_keras_GAN
### 1. Object Detection
- 학습 시 이미지데이터와 object 좌표(xywh) with 레이블이 필요하다.
- 예측 결과는 object 좌표(xywh) with 레이블이다.

### 2. [[GAN]]
![[Pasted image 20241217053537.png|400]]
- 가짜 이미지를 만들어내도록 학습되는 생성자와 가짜와 진짜 이미지를 구별하는 감별자를 경쟁시켜 학습하면, 생성자가 점점 진짜와 같은 이미지를 만들게 되는 원리이다.
- 장점
	- 자연스러운 이미지 생성 가능
	- 정답과 단순히 비교하는 것이 아니라, 감별자를 이용해 학습하므로 좋은 성능을 낼 수 있다.
- 단점
	- 학습이 올바르게 이루어지기 어렵고 필요한 데이터 수가 많아야 한다.
	- 사용하는 특징의 개수에 매우 민감하기 때문에 좋은 성능을 내는 특징의 개수를 정하기 어렵다.
- 생성자 구조
	- ![[Pasted image 20241217054451.png|300]]
	- 배치 정규화를 사용해 모델의 학습 속도를 높이고, 오버피팅을 방지한다.

### 3. Applications of GAN


## Chap17_NLP이론(2)
